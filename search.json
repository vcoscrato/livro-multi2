[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Multivariada 2",
    "section": "",
    "text": "Prefácio\nSeja Bem-vindo! Este livro aborda uma variedade de técnicas de análise multivariada, essenciais para a compreensão de dados complexos em diversas áreas do conhecimento.\nEste material foi elaborado especialmente para estudantes tendo um primeiro contato com técnicas estatísticas multivariadas. São cobertos os métodos a seguir:\n\nAnálise de Componentes Principais\nAnálise Fatorial\nAnálise de Agrupamento\nAnálise Discriminante\nAnálise de Correlação Canônica\nAnálise de Correspondência\n\nO objetivo é fornecer uma base sólida e prática para a aplicação dessas técnicas. antes, temos uma breve introdução dos conceitos fundamentais que norteiam a análise multivariada e algumas definições e resultados com vetores e matrizes que são importantes para o acompanhamento do livro.\nEspero que este livro seja um recurso valioso em sua jornada de aprendizado.\nVictor Coscrato",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html",
    "href": "src/01_intro/intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Motivação: Dados de Saúde\nEm diversas áreas do conhecimento, a coleta de dados frequentemente envolve a medição de múltiplas características para cada unidade de observação. Por exemplo, ao estudar o desempenho de alunos, podemos registrar suas notas em diferentes disciplinas (matemática, português, história), o tempo dedicado aos estudos e o número de horas de sono. Cada aluno, nesse contexto, é representado por um conjunto de valores que formam um vetor de variáveis. A análise multivariada surge da necessidade de compreender as interconexões e padrões dentro desses vetores complexos, indo além da análise isolada de cada característica.\nComo o nome sugere, dados multivariados são conjuntos de dados que contêm múltiplas variáveis (ou características) medidas para cada observação (ou unidade experimental, amostra). Diferente da análise univariada, que foca em uma única variável por vez, ou da bivariada, que explora a relação entre duas variáveis, a análise multivariada examina simultaneamente as relações entre três ou mais variáveis.\nA estrutura de um conjunto de dados multivariados é tipicamente representada por uma matriz, onde as linhas correspondem às observações e as colunas às variáveis.\n\\[\n\\mathbf{X} = \\begin{pmatrix}\n\\text{Observação 1} & \\rightarrow & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n\\text{Observação 2} & \\rightarrow & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Observação n} & \\rightarrow & x_{n1} & x_{n2} & \\cdots & x_{np} \\\\\n& & \\uparrow & \\uparrow & & \\uparrow \\\\\n& & \\text{Variável 1} & \\text{Variável 2} & \\cdots & \\text{Variável p}\n\\end{pmatrix}\n\\]\nNesta matriz \\(\\mathbf{X}\\) de dimensão \\(n \\times p\\):\nDo ponto de vista probabilístico, consideramos um vetor aleatório \\(\\mathbf{x} = [X_1, X_2, \\dots, X_p]^T\\). Ou seja, cada variável de estudo é uma variável aleatória \\(X_j\\), que juntas compoem esse vetor, e a análise multivariada busca entender a distribuição conjunta dessas variáveis, suas interdependências e como elas se relacionam com outras variáveis ou grupos.\nPara ilustrar, imagine que estamos coletando dados de saúde de 5 pacientes em um estudo. Para cada paciente, medimos a Altura (em cm), o Peso (em kg) e a Idade (em anos). Esses dados podem ser organizados na seguinte matriz \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} =\n\\begin{pmatrix}\n\\overbrace{175}^{\\text{Altura}} & \\overbrace{70}^{\\text{Peso}} & \\overbrace{25}^{\\text{Idade}} \\\\\n160 & 55 & 32 \\\\\n180 & 85 & 41 \\\\\n170 & 68 & 28 \\\\\n165 & 60 & 35\n\\end{pmatrix}\n\\begin{matrix}\n\\\\\n\\rightarrow \\text{Paciente 1} \\\\\n\\rightarrow \\text{Paciente 2} \\\\\n\\rightarrow \\text{Paciente 3} \\\\\n\\rightarrow \\text{Paciente 4} \\\\\n\\rightarrow \\text{Paciente 5}\n\\end{matrix}\n\\]\nNesta matriz:\nEste vetor captura todas as informações sobre um único indivíduo.\nEste vetor contém os valores de uma única variável para todos os indivíduos.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html#motivação-dados-de-saúde",
    "href": "src/01_intro/intro.html#motivação-dados-de-saúde",
    "title": "1  Introdução",
    "section": "",
    "text": "Vetor-Linha (Observação): Cada linha representa um paciente e seu conjunto completo de medidas. Por exemplo, o primeiro paciente é representado pelo vetor-linha \\[\n\\mathbf{x}_{1 \\bullet} = \\begin{pmatrix} 175 & 70 & 25 \\end{pmatrix}\n\\]\n\n\n\nVetor-Coluna (Variável): Cada coluna representa uma característica medida para todos os pacientes. Por exemplo, a variável Altura é representada pelo vetor-coluna \\[\n\\mathbf{x}_{\\bullet 1} = \\begin{pmatrix} 175 \\\\ 160 \\\\ 180 \\\\ 170 \\\\ 165 \\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\nNota\n\n\n\nO símbolo \\(\\bullet\\) é utilizado como um índice coringa, para se referir a toda uma linha ou coluna na matriz \\(X\\).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html#por-que-usar-análise-multivariada",
    "href": "src/01_intro/intro.html#por-que-usar-análise-multivariada",
    "title": "1  Introdução",
    "section": "1.2 Por que usar Análise Multivariada?",
    "text": "1.2 Por que usar Análise Multivariada?\nA análise multivariada é motivada pela necessidade de extrair informações significativas de conjuntos de dados complexos. Ao invés de analisar variáveis de forma isolada, essas técnicas permitem uma compreensão mais profunda e realista dos dados. Os principais objetivos são:\n\nSimplificação Estrutural: Reduzir a dimensionalidade dos dados, identificando as principais fontes de variação e eliminando redundâncias. Isso facilita a visualização e a interpretação de dados complexos, revelando a estrutura subjacente de forma mais clara.\nAgrupamento e Classificação: Organizar as observações em grupos homogêneos (agrupamento) ou atribuir observações a categorias predefinidas (classificação). O objetivo é identificar padrões que permitam segmentar os dados de maneira significativa.\nInvestigação de Estruturas de Dependência: Explorar e quantificar as relações entre variáveis. Isso inclui desde a análise de correlações simples até a modelagem de interações complexas entre múltiplos conjuntos de variáveis.\nPredição: Construir modelos para prever o valor de uma ou mais variáveis com base em outras.\nInferência: Realizar testes de hipóteses e inferências estatísticas sobre as relações em um contexto multivariado.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html#visão-geral-das-técnicas-multivariadas",
    "href": "src/01_intro/intro.html#visão-geral-das-técnicas-multivariadas",
    "title": "1  Introdução",
    "section": "1.3 Visão Geral das Técnicas Multivariadas",
    "text": "1.3 Visão Geral das Técnicas Multivariadas\nAs técnicas de análise multivariada podem ser classificadas com base em seus objetivos e na natureza das relações entre as variáveis. Uma distinção fundamental é entre técnicas de dependência, que analisam a relação entre variáveis dependentes e independentes, e técnicas de interdependência, que exploram as relações em um único conjunto de variáveis.\n\nTécnicas de Dependência: Analisam a relação entre uma ou mais variáveis dependentes e um conjunto de variáveis independentes. O objetivo é prever ou explicar o valor das variáveis dependentes.\nTécnicas de Interdependência: Exploram as relações entre todas as variáveis de um conjunto, sem fazer distinção entre dependentes e independentes. O foco é entender a estrutura geral dos dados.\n\nAlém disso a escolha de uma determinada técnica depende também dos tipos de variáveis em questão.\n\nVariáveis Categóricas (Qualitativas): Representam categorias ou grupos (e.g., gênero, tipo de produto).\nVariáveis Métricas (Quantitativas): Representam quantidades numéricas (e.g., idade, altura, renda, temperatura).\n\nCom o objetivo de classificar os métodos a serem apresentados nesse livro e posteriormente auxiliar na escolha da técnica mais adequada para o tratamento de um conjunto de dados, apresentamos a seguir uma tabela com algumas características de cada método e na sequência um fluxograma de decisão.\n\n\n\nTabela 1.1: Principais técnicas abordadas neste livro.\n\n\n\n\n\n\n\n\n\n\n\nTécnica\nObjetivo Principal\nTipo de Variável\nTipo de Análise\n\n\n\n\nComponentes Principais (PCA)\nRedução de dimensionalidade\nQuantitativas\nInterdependência\n\n\nAnálise Fatorial (FA)\nIdentificação de fatores latentes\nQuantitativas\nInterdependência\n\n\nAnálise de Agrupamento\nFormação de grupos homogêneos\nQuantitativas/Qualitativas\nInterdependência\n\n\nAnálise Discriminante\nClassificação de observações\nMista (Quali/Quanti)\nDependência\n\n\nCorrelação Canônica\nRelação entre conjuntos de variáveis\nQuantitativas\nDependência\n\n\nAnálise de Correspondência\nRelação entre variáveis categóricas\nQualitativas\nInterdependência\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\ncluster_blue_leaves\n\n\n\n\ncanonical_corr\n\nCorrelação Canônica\n\n\n\ndiscr_analysis\n\nAnálise Discriminante\n\n\n\nfactor_analysis\n\nAnálise Fatorial\n\n\n\npca\n\nComponentes Principais\n\n\n\ncluster_analysis\n\nAnálise de Agrupamento\n\n\n\ncorrespondence_analysis\n\nAnálise de Correspondência\n\n\n\nmulti_regression\n\nRegressão Múltipla\n\n\n\nmanova\n\nMANOVA\n\n\n\nlogistic_regression\n\nRegressão Logística\n\n\n\nrelation_type\n\nTipo de relação estudada\n\n\n\ndep_vs_indep\n\nDependência\n\n\n\nrelation_type-&gt;dep_vs_indep\n\n\n\n\n\ninterdep\n\nInterdependência\n\n\n\nrelation_type-&gt;interdep\n\n\n\n\n\ndep_var_type\n\nVariável Dependente?\n\n\n\ndep_vs_indep-&gt;dep_var_type\n\n\n\n\n\ngoal_interdep\n\nObjetivo?\n\n\n\ninterdep-&gt;goal_interdep\n\n\n\n\n\nnum_dependents\n\nQuantas Dependentes?\n\n\n\ndep_var_type-&gt;num_dependents\n\n\nMétrica\n\n\n\ngoal_cat_dep\n\nObjetivo?\n\n\n\ndep_var_type-&gt;goal_cat_dep\n\n\nCategórica\n\n\n\nnum_dependents-&gt;multi_regression\n\n\nUma\n\n\n\ngoal_multi_dep\n\nObjetivo?\n\n\n\nnum_dependents-&gt;goal_multi_dep\n\n\nMúltiplas\n\n\n\ngoal_multi_dep-&gt;canonical_corr\n\n\nRelacionar Conjuntos\n\n\n\ngoal_multi_dep-&gt;manova\n\n\nComparar Grupos\n\n\n\ngoal_cat_dep-&gt;discr_analysis\n\n\nClassificar\n\n\n\ngoal_cat_dep-&gt;logistic_regression\n\n\nPrever Probabilidade\n\n\n\ngoal_interdep-&gt;cluster_analysis\n\n\nAgrupar\n\n\n\nlatent_factors\n\nFatores Latentes?\n\n\n\ngoal_interdep-&gt;latent_factors\n\n\nReduzir Dimensão\n\n\n\ncategorical_vars\n\nVariáveis Categóricas?\n\n\n\ngoal_interdep-&gt;categorical_vars\n\n\nAssociar\n\n\n\nlatent_factors-&gt;factor_analysis\n\n\nSim\n\n\n\nlatent_factors-&gt;pca\n\n\nNão\n\n\n\ncategorical_vars-&gt;correspondence_analysis\n\n\nSim\n\n\n\n\n\n\nFigura 1.1: Diagrama de decisão para escolha de técnica de Análise Multivariada. Nós enfatizados fundo azul escuro indicam as técnicas de análise multivariada abordadas neste livro. Importante: Este diagrama é um guia simplificado para auxiliar na escolha da técnica mais adequada com base nas características dos dados e nos objetivos da análise. Ele não é exaustivo e serve apenas para posicionar as técnicas discutidas neste livro. A escolha final da técnica deve sempre considerar o contexto específico do problema e as características detalhadas dos dados.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/vet_mat.html",
    "href": "src/01_intro/vet_mat.html",
    "title": "2  Fundamentos Matemáticos",
    "section": "",
    "text": "2.1 Medidas descritivas\nCom os conceitos e objetivos da análise multivariada estabelecidos no capítulo anterior, este capítulo foca nos fundamentos matemáticos que sustentam as técnicas. A representação de dados multivariados através de vetores e matrizes é o ponto de partida para a formalização dos métodos.\nA estrutura central de um conjunto de dados multivariados é a matriz de dados, \\(\\mathbf{X}\\), de dimensão \\(n \\times p\\):\n\\[\n\\mathbf{X} = \\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{pmatrix}\n\\tag{2.1}\\]\nNesta matriz, as \\(n\\) linhas correspondem às observações (ou amostras) e as \\(p\\) colunas às variáveis. O elemento \\(x_{ij}\\) representa o valor da \\(j\\)-ésima variável para a \\(i\\)-ésima observação. A partir desta estrutura, desenvolveremos as medidas descritivas multivariadas e as operações matriciais que formam a base das análises subsequentes.\nPara cada uma das \\(p\\) variáveis, podemos calcular a média amostral:\n\\[\n\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}\n\\tag{2.2}\\]\nAssim como a média resume a tendência central de uma única variável, o vetor de médias resume a tendência central de um conjunto de dados multivariados. Ele simplesmente contém a média de cada variável.\n\\[\n\\bar{\\mathbf{x}} = \\begin{pmatrix}\n\\bar{x}_1 \\\\\n\\bar{x}_2 \\\\\n\\vdots \\\\\n\\bar{x}_p\n\\end{pmatrix}\n\\]\nDa mesma forma, a variância amostral, para cada variável, mede a dispersão dos dados em torno de sua média e é calculada como:\n\\[\ns_j^2 = s_{jj} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2\n\\]\nA covariância amostral entre duas variáveis, \\(j\\) e \\(k\\), mede como elas variam juntas:\n\\[\ns_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)\n\\]\nGeneralizando essas ideias, a dispersão e a inter-relação de todas as variáveis são capturadas pela matriz de variâncias e covariâncias amostral, denotada por \\(\\mathbf{S}\\). Esta é uma matriz \\(p \\times p\\) cuja entrada \\((j, k)\\) é a covariância entre a \\(j\\)-ésima e a \\(k\\)-ésima variável.\n\\[\n\\mathbf{S} = \\begin{pmatrix}\ns_{11} & s_{12} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\cdots & s_{pp}\n\\end{pmatrix}\n\\]\nNote que a diagonal principal contém as variâncias de cada variável, pois a covariância de uma variável com ela mesma é a sua própria variância.\nA matriz de covariâncias \\(\\mathbf{S}\\) é sempre simétrica, isto é, \\(s_{jk} = s_{kj}\\). Isso é uma consequencia direta da comutatividade do produto:\n\\[\ns_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k) = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ik} - \\bar{x}_k)(x_{ij} - \\bar{x}_j) = s_{kj}\n\\]\nÉ possível calcular a matriz de covariâncias de forma mais compacta usando operações matriciais. Primeiro, criamos a matriz de dados centralizados, \\(\\mathbf{X}_c\\), subtraindo o vetor de médias de cada linha da matriz de dados original.\n\\[\n\\mathbf{X}_c = \\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^T\n\\]\nOnde \\(\\mathbf{1}\\) é um vetor-coluna de uns de tamanho \\(n\\). A matriz de covariâncias é então calculada como:\n\\[\n\\mathbf{S} = \\frac{1}{n-1} \\mathbf{X}_c^T \\mathbf{X}_c\n\\]\nA covariância é uma medida útil, mas sua magnitude depende da escala das variáveis. Por exemplo, a covariância entre altura (em metros) e peso (em quilogramas) será muito diferente da covariância entre altura (em centímetros) e peso (em gramas), mesmo que a relação subjacente seja a mesma.\nPara obter uma medida de associação que seja independente de escala, usamos a correlação. A correlação de Pearson, \\(r_{jk}\\), padroniza a covariância, dividindo-a pelo produto dos desvios padrão das duas variáveis:\n\\[\nr_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}} = \\frac{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum_{i=1}^n (x_{ik} - \\bar{x}_k)^2}}\n\\]\nA correlação varia entre -1 e 1, onde 1 indica uma relação linear positiva perfeita, -1 indica uma relação linear negativa perfeita, e 0 indica ausência de relação linear.\nAssim como a matriz de covariâncias, podemos organizar as correlações em uma matriz de correlações, \\(\\mathbf{R}\\):\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nA diagonal principal da matriz de correlações é sempre 1, pois a correlação de uma variável com ela mesma é perfeita. Assim como a matriz de covariâncias, a matriz de correlações também é simétrica (\\(r_{jk} = r_{kj}\\)).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "src/01_intro/vet_mat.html#medidas-descritivas",
    "href": "src/01_intro/vet_mat.html#medidas-descritivas",
    "title": "2  Fundamentos Matemáticos",
    "section": "",
    "text": "Nota Conceitual: População × Amostra\n\n\n\nNote que até agora trabalhamos apenas com conceitos amostrais, como médias, variâncias e covariâncias obtidas a partir dos dados observados. No entanto, é importante frisar que existe uma base probabilística subjacente, que dá sentido a essas medidas.\nDo ponto de vista probabilístico, consideramos um vetor aleatório definido sobre um espaço de probabilidade. \\[\n\\mathbf{x} = (X_1, X_2, \\ldots, X_p)^T,\n\\]\n\n\n\n\n\n\nCuidado\n\n\n\nAs notações nesse ponto podem se confundir. Uma letra minúscula em negrito (por exemplo, \\(\\mathbf{x}\\)) é utilizada para o vetor aleatório, enquanto uma letra maíscula em negrito (por exemplo, \\(\\mathbf{X}\\)) é utilizada para a matriz de dados. Finalmente, uma letra maíscula comum (por exemplo, X) é utilizada para denotar uma variável aleatória.\n\n\nEsse vetor possui parâmetros teóricos fundamentais:\n\nMédia (vetor de expectativas):\n\\[\n\\mu_j = E[X_j], \\quad \\boldsymbol{\\mu} = E[\\mathbf{X}]\n\\]\nMatriz de covariâncias populacional:\n\\[\n\\Sigma = (\\sigma_{jk}), \\quad \\sigma_{jk} = Cov(X_j, X_k) = E[(X_j - \\mu_j)(X_k - \\mu_k)]\n\\]\n\nAs observações \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) são realizações independentes do vetor aleatório \\(\\mathbf{X}\\). A matriz de dados que manipulamos é composta exatamente por essas realizações.\nNa prática, entretanto, não temos acesso direto a \\(\\boldsymbol{\\mu}\\) e \\(\\Sigma\\), mas sim às suas estimativas amostrais:\n\nVetor de médias amostrais:\n\\[\n\\bar{\\mathbf{x}} = (\\bar{x}_1, \\ldots, \\bar{x}_p)^T, \\quad\n\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij}\n\\]\nMatriz de covariâncias amostral:\n\\[\n\\mathbf{S} = (s_{jk}), \\quad\ns_{jk} = \\frac{1}{n-1}\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)\n\\]\n\n\nEm resumo:\n- No nível populacional, trabalhamos com o vetor aleatório e seus parâmetros teóricos \\((\\boldsymbol{\\mu}, \\Sigma)\\).\n- No nível amostral, trabalhamos com as observações e estatísticas que estimam esses parâmetros \\((\\bar{\\mathbf{x}}, \\mathbf{S})\\).\nEste curso tem caráter prático, e por isso utilizaremos predominantemente as medidas amostrais. Ainda assim, é essencial manter clara a distinção entre parâmetros populacionais e suas estimativas, pois toda análise multivariada repousa nessa relação.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "src/01_intro/vet_mat.html#sec-espectral",
    "href": "src/01_intro/vet_mat.html#sec-espectral",
    "title": "2  Fundamentos Matemáticos",
    "section": "2.2 Decomposição Espectral",
    "text": "2.2 Decomposição Espectral\nA decomposição espectral, também conhecida como decomposição de autovalores, é uma das fatorações de matrizes mais importantes da álgebra linear, com vastas aplicações em estatística e aprendizado de máquina. Ela nos permite decompor uma matriz em suas partes fundamentais: seus autovalores e autovetores. Esta decomposição se aplica a matrizes simétricas, que é o caso, por exemplo, das matrizes de covariância e correlação.\n\nAutovetores São vetores especiais associados a uma matriz. A característica principal de um autovetor é que, quando multiplicado pela matriz, sua direção não muda. O vetor pode ser esticado, comprimido ou ter seu sentido invertido, mas ele permanecerá na mesma linha.\nAutovalores: Para cada autovetor, existe um autovalor correspondente. O autovalor é um escalar (um número) que nos diz o fator pelo qual o autovetor foi esticado ou comprimido.\n\nMatematicamente, se \\(\\mathbf{v}\\) é um autovetor da matriz \\(\\mathbf{A}\\) e \\(\\lambda\\) é seu autovalor correspondente, a relação é definida como:\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nEsta equação nos diz que o resultado da multiplicação da matriz \\(\\mathbf{A}\\) pelo seu autovetor \\(\\mathbf{v}\\) é o mesmo que simplesmente multiplicar o autovetor \\(\\mathbf{v}\\) pelo escalar \\(\\lambda\\).\nA decomposição espectral afirma que qualquer matriz simétrica \\(\\mathbf{A}\\) pode ser reescrita da seguinte forma:\n\\[\n\\mathbf{A} = \\mathbf{P}\\Lambda\\mathbf{P}^T\n\\]\nOnde:\n\n\\(\\mathbf{A}\\): É a matriz simétrica original que queremos decompor.\n\\(\\mathbf{P}\\): É uma matriz ortogonal cujas colunas são os autovetores de \\(\\mathbf{A}\\). Uma matriz ortogonal tem a propriedade especial de que sua transposta é também sua inversa (\\(\\mathbf{P}^T = \\mathbf{P}^{-1}\\)), o que a torna muito conveniente para cálculos.\n\\(\\Lambda\\): É uma matriz diagonal cujos elementos da diagonal principal são os autovalores de \\(\\mathbf{A}\\), na mesma ordem dos autovetores correspondentes em \\(\\mathbf{P}\\). Todos os outros elementos de \\(\\Lambda\\) são zero.\n\n\n2.2.1 Exemplo\nVamos decompor a seguinte matriz simétrica \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n\\]\n\nCalcular Autovalores: Encontramos os autovalores resolvendo a equação característica det(A - λI) = 0. \\[\n\\det \\begin{pmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{pmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = 0\n\\] As raízes são \\(\\lambda_1 = 3\\) e \\(\\lambda_2 = 1\\).\nCalcular Autovetores:\n\nPara \\(\\lambda_1 = 3\\): Resolvemos (A - 3I)v = 0, que nos dá o autovetor \\(\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\).\nPara \\(\\lambda_2 = 1\\): Resolvemos (A - 1I)v = 0, que nos dá o autovetor \\(\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\n\nConstruir as Matrizes P e D:\n\nNormalizamos os autovetores (dividindo por sua norma) para que tenham comprimento 1: \\(\\mathbf{v}'_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}\\) e \\(\\mathbf{v}'_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}\\).\nA matriz \\(\\mathbf{P}\\) é formada por esses autovetores normalizados como colunas: \\[\n\\mathbf{P} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\\n1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix}\n\\]\nA matriz \\(\\Lambda\\) contém os autovalores na diagonal: \\[\n\\Lambda = \\begin{pmatrix} 3 & 0 \\\\\n0 & 1 \\end{pmatrix}\n\\]\n\n\nA decomposição espectral de \\(\\mathbf{A}\\) é, portanto, \\(\\mathbf{A} = \\mathbf{P}\\Lambda\\mathbf{P}^T\\).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html",
    "href": "src/02_tec_mult/03_acp.html",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "",
    "text": "3.1 A Intuição Geométrica\nA Análise de Componentes Principais (ACP) é uma técnica estatística multivariada que transforma um conjunto de variáveis possivelmente correlacionadas em um novo conjunto de variáveis não correlacionadas, chamadas de componentes principais. O objetivo primário da ACP é a redução de dimensionalidade: representar a variabilidade presente nos dados originais com um número menor de variáveis, minimizando a perda de informação.\nCada componente principal é uma combinação linear das variáveis originais. O primeiro componente principal é construído para capturar a maior variabilidade possível nos dados. O segundo componente principal, ortogonal ao primeiro, captura a maior parte da variabilidade restante, e assim por diante. Ao final, o número de componentes principais é igual ao número de variáveis originais, mas a expectativa é que os primeiros componentes concentrem a maior parte da informação relevante.\nPara construir a intuição, vamos começar com um exemplo simples. Suponha que coletamos dados de duas variáveis, Peso (em kg) e Altura (em cm), de um grupo de 10 pessoas.\nTabela de dados com Peso (kg) e Altura (cm).\n\n\nPeso\nAltura\n\n\n\n\n65\n170\n\n\n72\n182\n\n\n58\n165\n\n\n81\n190\n\n\n75\n178\n\n\n60\n168\n\n\n68\n175\n\n\n70\n172\n\n\n78\n185\n\n\n62\n169\nAo plotarmos esses dados (já centralizados), obtemos a nuvem de pontos abaixo. O sistema de eixos em preto (Peso, Altura) é a nossa perspectiva padrão.\nFigura 3.1: Diagrama de dispersão para os dados de Peso e Altura.\nObservando o gráfico, notamos que a nuvem de pontos forma uma elipse inclinada, o que indica uma correlação entre Peso e Altura. Descrever os dados usando os eixos originais é perfeitamente válido, mas talvez não seja a forma mais eficiente. A maior parte da variabilidade ocorre ao longo de uma diagonal.\nA ACP propõe uma rotação dos eixos para que eles se alinhem melhor com a estrutura dos dados. O resultado é um novo sistema de eixos, os Componentes Principais (\\(CP_1\\) e \\(CP_2\\)), como mostrado abaixo.\nFigura 3.2: O sistema de eixos rotacionado (vermelho) se alinha com a máxima dispersão (variância) dos dados.\nO primeiro componente, \\(CP_1\\), agora aponta na direção de maior “alongamento” da nuvem de pontos. O segundo, \\(CP_2\\), é perpendicular ao primeiro e aponta na direção de maior variabilidade restante. Encontramos uma nova perspectiva que descreve a estrutura dos dados de forma mais natural e eficiente.\nEm casos com mais de duas variáveis (p &gt; 2), a lógica se estende: O i-ésimo componente principal (\\(CP_i\\)) aponta para a direção de maior variabilidade, sob a restrição de ser ortogonal (não correlacionado) a todos os componentes anteriores, \\(Cov(CP_j, CP_i) = 0 \\, \\forall \\, j &lt; i\\).\nUma intuição geométrica para o problema é buscar o ângulo \\(\\theta\\) de rotação do eixo das variáveis tal que a variância dos componentes seja máxima. Essa rotação é simples de ser observada nesse exemplo bidimensional (veja Figura Figura 3.2).",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#variância-como-medida-de-informação",
    "href": "src/02_tec_mult/03_acp.html#variância-como-medida-de-informação",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "3.2 Variância como Medida de Informação",
    "text": "3.2 Variância como Medida de Informação\nA essa altura, você deve estar se perguntando: por que a direção do “maior alongamento” é a mais importante? Em estatística, a variância é frequentemente usada como uma medida de informação. Uma variável com alta variância indica que seus valores são bem espalhados, o que nos ajuda a diferenciar as observações. Se a variância fosse zero, todos os pontos seriam idênticos, não nos fornecendo nenhuma informação sobre suas diferenças.\nA ACP utiliza essa ideia para encontrar os eixos mais informativos. Ao rotacionar o sistema de coordenadas, ela não altera a variabilidade total dos dados, mas a redistribui de forma inteligente.\nNo nosso exemplo, as variâncias das variáveis originais são:\n\nVariância do Peso: 59.88\nVariância da Altura: 66.71\nVariância Total Original: 126.59\n\nApós a rotação, as variâncias ao longo dos novos eixos (os componentes principais) são:\n\nVariância de \\(CP_1\\): 123.55\nVariância de \\(CP_2\\): 3.04\nVariância Total dos Componentes: 126.59\n\nDois fatos cruciais se destacam:\n\nA variância total é conservada. A soma das variâncias é a mesma nos dois sistemas de eixos. Nenhuma informação foi perdida; o ponto de vista foi apenas alterado.\nA variância foi eficientemente redistribuída. O primeiro componente, \\(CP_1\\), agora concentra 97.60% da variância total. Isso significa que, se quiséssemos reduzir nossos dados de 2D para 1D, poderíamos manter apenas o \\(CP_1\\) e ainda reter a maior parte da informação original. Essa é a essência da redução de dimensionalidade com ACP.",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#a-formalização-matemática",
    "href": "src/02_tec_mult/03_acp.html#a-formalização-matemática",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "3.3 A Formalização Matemática",
    "text": "3.3 A Formalização Matemática\nCom a intuição geométrica estabelecida, podemos formalizar a Análise de Componentes Principais. O objetivo é transformar um conjunto de variáveis correlacionadas \\(\\mathbf{x} = (X_1, \\dots, X_p)^T\\) em um novo conjunto de variáveis não correlacionadas, os componentes principais \\(\\mathbf{y} = (Y_1, \\dots, Y_p)^T\\). Cada componente é uma combinação linear das variáveis originais:\n\\[\n\\begin{aligned}\nY_1 &= e_{11}X_1 + e_{12}X_2 + \\dots + e_{1p}X_p = \\mathbf{e}_1^T \\mathbf{x} \\\\\nY_2 &= e_{21}X_1 + e_{22}X_2 + \\dots + e_{2p}X_p = \\mathbf{e}_2^T \\mathbf{x} \\\\\n&\\vdots \\\\\nY_p &= e_{p1}X_1 + e_{p2}X_2 + \\dots + e_{pp}X_p = \\mathbf{e}_p^T \\mathbf{x}\n\\end{aligned}\n\\]\nEm notação matricial, a transformação pode ser escrita de forma compacta:\n\\[\n\\mathbf{Y} = \\mathbf{P}^T \\mathbf{X}\n\\tag{3.1}\\]\nOnde \\(\\mathbf{Y}\\) é o vetor \\(p \\times 1\\) de autovalores e \\(\\mathbf{P}\\) é a matriz \\(p \\times p\\) cujas colunas são os vetores de coeficientes \\(\\mathbf{e}_k\\).\nEsses componentes são construídos para satisfazer duas condições fundamentais:\n\nVariâncias Ordenadas: A variância do primeiro componente é a maior possível, a do segundo é a maior possível entre as direções não correlacionadas com o primeiro, e assim por diante. Ou seja, \\(Var(Y_1) \\ge Var(Y_2) \\ge \\dots \\ge Var(Y_p)\\).\nNão Correlacionados: Os componentes são ortogonais entre si, o que significa que \\(Cov(Y_i, Y_k) = 0\\) para todo \\(i \\neq k\\).\n\n\n3.3.1 O Problema de Maximização\nO primeiro componente principal, \\(Y_1 = \\mathbf{e}_1^T\\mathbf{x}\\), é a combinação linear com variância máxima. A variância de \\(Y_1\\) é dada por:\n\\[\nVar(Y_1) = Var(\\mathbf{e}_1^T\\mathbf{x}) = \\mathbf{e}_1^T Var(\\mathbf{x}) \\mathbf{e}_1 = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1\n\\]\nOnde \\(\\mathbf{\\Sigma}\\) é a matriz de covariâncias de \\(\\mathbf{x}\\). Para evitar que a variância seja aumentada simplesmente inflando os coeficientes em \\(\\mathbf{e}_1\\), impomos a restrição de que seu comprimento seja unitário, \\(\\mathbf{e}_1^T\\mathbf{e}_1 = 1\\). Formalmente, o problema de maximização para o primeiro componente principal se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_1} \\quad & \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1 \\\\\n    \\text{sujeito a} \\quad & \\mathbf{e}_1^T \\mathbf{e}_1 = 1\n\\end{aligned}\n\\]\nPara maximizar a variância sujeita à restrição, utilizamos o método dos multiplicadores de Lagrange. A função a ser maximizada é:\n\\[\nL(\\mathbf{e}_1, \\lambda_1) = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1 - \\lambda_1 (\\mathbf{e}_1^T \\mathbf{e}_1 - 1)\n\\]\nDerivando em relação a \\(\\mathbf{e}_1\\) e igualando a zero, obtemos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_1} = 2 \\mathbf{\\Sigma} \\mathbf{e}_1 - 2 \\lambda_1 \\mathbf{e}_1 = 0\n\\]\nO que nos leva à equação fundamental de autovalores e autovetores:\n\\[\n\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\n\\]\nEsta equação mostra que o vetor de coeficientes \\(\\mathbf{e}_1\\) deve ser um autovetor da matriz de covariâncias \\(\\mathbf{\\Sigma}\\). Para encontrar a variância, pré-multiplicamos a equação por \\(\\mathbf{e}_1^T\\):\n\\[\n\\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1^T \\mathbf{e}_1\n\\]\nComo \\(Var(Y_1) = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1\\) e a restrição é \\(\\mathbf{e}_1^T \\mathbf{e}_1 = 1\\), temos:\n\\[\nVar(Y_1) = \\lambda_1\n\\]\nPara maximizar a variância de \\(Y_1\\), devemos escolher o maior autovalor possível. Portanto, \\(\\lambda_1\\) é o maior autovalor de \\(\\mathbf{\\Sigma}\\), e \\(\\mathbf{e}_1\\) é o autovetor correspondente.\n\n\n\n\n\n\nNota\n\n\n\nA matriz de covariâncias \\(\\mathbf{\\Sigma}\\) é, por construção, uma matriz simétrica e positiva semi-definida. Conforme discutido em Seção 2.2, o Teorema Espectral garante que os autovalores de tal matriz são reais e não-negativos, e que seus autovetores correspondentes a autovalores distintos são ortogonais. Esta propriedade é fundamental para a existência e unicidade dos componentes principais.\n\n\n\n\n\n\n\n\nNota\n\n\n\nA demonstração acima, utilizando multiplicadores de Lagrange, é uma maneira moderna e elegante de conduzir a derivação do problema de máximização. Uma abordagem clássica restringe a norma de \\(e\\) através do quociente,\n\\[\nVar(Y_1) = \\max_{e_1} \\frac{\\mathbf{e}_1^T \\Sigma \\mathbf{e}_1}{\\mathbf{e}_1^T \\mathbf{e_1}}\n\\]\nEste é um problema clássico na álgebra linear. Um teorema fundamental afirma que para qualquer matriz simétrica \\(A\\), o máximo da forma quadrática \\(\\mathbf{x}^T A \\mathbf{x}\\), sujeito à restrição \\(\\mathbf{x}^T \\mathbf{x} = 1\\), é o maior autovalor de \\(A\\). O vetor \\(\\mathbf{x}\\) que atinge esse máximo é o autovetor correspondente. Como a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) é simétrica, este teorema se aplica diretamente ao nosso problema.\n\n\n\n\n3.3.2 Componentes Subsequentes\nUma vez encontrada a primeira direção de máxima variância, o segundo componente principal, \\(Y_2 = \\mathbf{e}_2^T\\mathbf{x}\\), busca capturar o máximo da variabilidade restante, sob a condição de ser não correlacionado com \\(Y_1\\). A condição de componentes não correlacionados garante que a informação presente no segundo componente principal não é redundante com relação aquela já presente no primeiro. Formalmente, temos:\n\\[\nCov(Y_1, Y_2) = Cov(\\mathbf{e}^T_1 \\mathbf{x}, \\mathbf{e}^T_2 \\mathbf{x}) = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_2\n\\]\nComo \\(\\mathbf{e}_1\\) é o primeiro autovetor, temos \\(\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\\), Assim:\n\\[\n\\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_2 = (\\lambda_1 \\mathbf{e}_1)^T \\mathbf{e}_2 = \\lambda_1 \\mathbf{e}_1^T \\mathbf{e}_2\n\\]\nLogo:\n\\[\nCov(Y_1, Y_2) = 0 \\iff \\mathbf{e}_1^T \\mathbf{e}_2 = 0\n\\]\nCom essa condição bem definida, o problema para o segundo componente se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_2} \\quad & \\mathbf{e}_2^T \\mathbf{\\Sigma} \\mathbf{e}_2 \\\\\n    \\text{sujeito a} \\quad & \\begin{cases}\n        \\mathbf{e}_2^T \\mathbf{e}_2 = 1 \\\\\n        \\mathbf{e}_1^T \\mathbf{e}_2 = 0\n    \\end{cases}\n\\end{aligned}\n\\]\nA função Lagrangiana agora inclui dois multiplicadores, \\(\\lambda_2\\) e \\(\\phi\\):\n\\[\nL(\\mathbf{e}_2, \\lambda_2, \\phi) = \\mathbf{e}_2^T \\mathbf{\\Sigma} \\mathbf{e}_2 - \\lambda_2(\\mathbf{e}_2^T \\mathbf{e}_2 - 1) - \\phi(\\mathbf{e}_1^T \\mathbf{e}_2 - 0)\n\\]\nDerivando em relação a \\(\\mathbf{e}_2\\) e igualando a zero, temos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_2} = 2\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_2 - \\phi\\mathbf{e}_1 = \\mathbf{0}\n\\]\nPré-multiplicando por \\(\\mathbf{e}_1^T\\):\n\\[\n2\\mathbf{e}_1^T\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_1^T\\mathbf{e}_2 - \\phi\\mathbf{e}_1^T\\mathbf{e}_1 = 0\n\\]\nSabendo que:\n\n\\(\\mathbf{e}_1^T\\mathbf{\\Sigma} = \\lambda_1\\mathbf{e}_1^T\\)\n\\(\\mathbf{e}_1^T\\mathbf{e}_2 = 0\\)\n\\(\\mathbf{e}_1^T\\mathbf{e}_1 = 1\\)\n\nA equação se simplifica a \\(\\phi = 0\\). Substituindo \\(\\phi=0\\) de volta na derivada, a equação se torna:\n\\[\n\\mathbf{\\Sigma}\\mathbf{e}_2 = \\lambda_2\\mathbf{e}_2\n\\]\nAssim, \\(\\mathbf{e}_2\\) é o autovetor de \\(\\mathbf{\\Sigma}\\) correspondente ao autovalor \\(\\lambda_2\\). Como \\(\\lambda_1\\) foi o maior autovalor, para maximizar a variância de \\(Y_2\\), \\(\\lambda_2\\) deve ser o segundo maior autovalor. Este processo se generaliza para os componentes subsequentes.\nEste processo continua: o \\(k\\)-ésimo componente principal (\\(Y_k\\)) é definido pelo autovetor \\(\\mathbf{e}_k\\) associado ao \\(k\\)-ésimo maior autovalor \\(\\lambda_k\\), garantindo que \\(Var(Y_k) = \\lambda_k\\) e que todos os componentes sejam mutuamente não correlacionados.\nÉ neste ponto que a conexão com a Seção 2.2 se torna explícita. A matriz \\(\\mathbf{P}\\) (Equação 3.1), cujas colunas são os autovetores da matriz de covariâncias \\(\\mathbf{\\Sigma}\\), é exatamente a mesma matriz \\(\\mathbf{P}\\) da decomposição espectral \\(\\mathbf{\\Sigma} = \\mathbf{P}\\mathbf{D}\\mathbf{P}^T\\). Além disso, \\(\\mathbf{D}\\) é uma matriz diagonal contendo a variância de cada componente principal. Logo, podemos obter todos os componentes principais de maneira prática e simultânea através da decomposição espectral.",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#a-importância-do-pré-processamento-dos-dados",
    "href": "src/02_tec_mult/03_acp.html#a-importância-do-pré-processamento-dos-dados",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "3.4 A Importância do Pré-processamento dos Dados",
    "text": "3.4 A Importância do Pré-processamento dos Dados\nA Análise de Componentes Principais é, em sua essência, uma análise de variabilidade. A forma como medimos essa variabilidade impacta diretamente o resultado. Dois pré-processamentos são cruciais: a centralização e o escalonamento.\n\n3.4.1 Centralização\nNa Análise de Componentes Principais (ACP), a centralização dos dados — ou seja, a subtração da média de cada variável — é uma etapa fundamental não apenas para o cálculo da matriz de covariâncias, mas também para a projeção dos dados nos componentes principais.\nAo projetar os dados em um componente \\(\\mathbf{e}_i\\), é imprescindível que a projeção seja feita a partir dos dados centralizados, ou seja:\n\\[\nY_i = \\mathbf{e}_i^𝑇(\\mathbf{x} − \\bar{\\mathbf{{x}}})\n\\]\nEsse detalhe é essencial porque os autovetores da ACP são obtidos com base na matriz de covariâncias, a qual descreve a dispersão dos dados em torno da média, e não em torno da origem. Se aplicarmos a projeção diretamente sobre \\(\\mathbf{x}\\), sem subtrair a média, os componentes resultantes não representarão adequadamente as direções de maior variabilidade — e sim uma combinação da dispersão com a posição média dos dados.\nPortanto, para que os componentes principais preservem a interpretação correta como combinações lineares que explicam a variância dos dados em torno do centro da nuvem de pontos, é indispensável que tanto o cálculo da matriz de covariâncias quanto a projeção dos dados utilizem os dados centralizados.\n\n\n\n\n\n\nImportante\n\n\n\nNo contexto de ACP, é comum e prático denotar por \\(\\mathbf{x}\\) o vetor de variáveis já centralizado. Utilizamos esse abuso de notação durante esse capítulo para simplificação do texto sem perda de generalidade.\n\n\n\n\n3.4.2 Por que Escalonar? O Dilema da Covariância vs. Correlação\nA Análise de Componentes Principais (ACP) é sensível à escala das variáveis. Se uma variável tiver uma variância numericamente muito maior que as outras — mesmo que apenas por causa da sua unidade de medida — ela poderá dominar os primeiros componentes principais.\nImagine incluir uma terceira variável no conjunto Altura/Peso: a renda mensal, medida em Reais. As variâncias poderiam ser aproximadamente:\n\nAltura: 80 cm²\n\nPeso: 60 kg²\n\nRenda: 4.000.000 (R$)²\n\nNesse cenário, a variância da Renda é milhares de vezes maior que a das outras variáveis. Se aplicarmos a ACP diretamente na matriz de covariâncias, o primeiro componente principal será fortemente direcionado pela Renda, mesmo que sua correlação com as demais variáveis seja baixa. Isso ocorre porque a ACP estará apenas “seguindo” a direção da variável com maior variância — não necessariamente a mais informativa.\nPara evitar esse viés, escalonamos as variáveis: cada uma é dividida por seu desvio padrão. Isso padroniza todas para variância igual a 1. Ao fazer isso, estamos na prática realizando a ACP sobre a matriz de correlação \\((\\mathbf{R})\\) em vez da matriz de covariâncias \\((\\mathbf{\\Sigma})\\).\nVantagem do escalonamento:\nUsar a matriz de correlação “democratiza” a análise. Todas as variáveis começam com a mesma importância inicial (variância 1), e a ACP passa a capturar a estrutura de correlações, ao invés de ser enviesada pelas diferenças de escala.\nQuando usar a matriz de covariâncias?\nSomente quando todas as variáveis estão na mesma unidade de medida e possuem uma interpretação comparável. Por exemplo, comparar a temperatura em Celsius em diferentes regiões pode fazer sentido sem escalonamento. Fora isso, a matriz de correlação é geralmente a escolha mais robusta e segura.",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "href": "src/02_tec_mult/03_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "3.5 Componentes Principais Populacionais vs. Amostrais",
    "text": "3.5 Componentes Principais Populacionais vs. Amostrais\nAté este ponto, discutimos os componentes principais em um contexto populacional, onde a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) (ou correlação \\(\\mathbf{R}\\)) e seus autovalores \\(\\lambda_k\\) e autovetores \\(\\mathbf{e}_k\\) são conhecidos. Na prática, quase sempre trabalhamos com uma amostra de dados. Nesse caso, não conhecemos os verdadeiros parâmetros populacionais e devemos estimá-los.\nOs componentes principais amostrais são obtidos da mesma maneira, mas usando a matriz de covariâncias amostral \\(\\mathbf{S}\\) (ou a matriz de correlação amostral \\(\\mathbf{R}\\)). As quantidades resultantes são estimativas dos seus análogos populacionais:\n\nO \\(k\\)-ésimo autovalor amostral, \\(\\hat{\\lambda}_k\\), é uma estimativa de \\(\\lambda_k\\).\nO \\(k\\)-ésimo autovetor amostral, \\(\\hat{\\mathbf{e}}_k\\), é uma estimativa de \\(\\mathbf{e}_k\\).\nO \\(k\\)-ésimo componente principal amostral, \\(\\hat{Y}_k = \\hat{\\mathbf{e}}_k^T \\mathbf{x}\\), é uma estimativa de \\(Y_k\\).\n\nA teoria e a interpretação permanecem as mesmas. Para simplificar a notação, ao longo deste capítulo, omitimos o acento circunflexo (\\(\\hat{\\phantom{a}}\\)), mas é importante lembrar que, na aplicação prática, estamos sempre lidando com estimativas amostrais.",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#correlação-entre-componentes-e-variáveis-originais",
    "href": "src/02_tec_mult/03_acp.html#correlação-entre-componentes-e-variáveis-originais",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "3.6 Correlação entre Componentes e Variáveis Originais",
    "text": "3.6 Correlação entre Componentes e Variáveis Originais\nOs coeficientes \\(e_{kj}\\) do autovetor \\(\\mathbf{e}_k\\) são chamados de cargas (loadings) e representam o peso da variável original \\(X_j\\) na formação do componente \\(Y_k\\). Embora as cargas sejam importantes, a sua interpretação pode ser complicada, pois sua magnitude depende das unidades das variáveis originais.\nUma medida mais interpretável é a correlação entre os componentes principais e as variáveis originais, \\(Cor(Y_k, X_j)\\). Ela nos diz o quão “alinhado” um componente está com cada variável original, numa escala padronizada de -1 a 1. A fórmula para essa correlação é:\n\\[\nCor(Y_k, X_j) = \\frac{e_{kj} \\sqrt{\\lambda_k}}{\\sqrt{s_{jj}}}\n\\]\nOnde:\n\n\\(e_{kj}\\) é a carga da variável \\(j\\) no componente \\(k\\).\n\\(\\lambda_k\\) é o autovalor (variância) do componente \\(k\\).\n\\(s_{jj}\\) é a variância da variável original \\(j\\).\n\nQuando a ACP é realizada sobre a matriz de correlação (ou seja, com dados padronizados), as variâncias \\(s_{jj}\\) são todas iguais a 1. Nesse caso, a fórmula simplifica para \\(Cor(Y_k, X_j) = e_{kj} \\sqrt{\\lambda_k}\\). As correlações se tornam proporcionais às cargas, facilitando a interpretação.",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#escolhendo-o-número-de-componentes",
    "href": "src/02_tec_mult/03_acp.html#escolhendo-o-número-de-componentes",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "3.7 Escolhendo o Número de Componentes",
    "text": "3.7 Escolhendo o Número de Componentes\nA principal vantagem da ACP é a redução de dimensionalidade. Mas como decidimos quantos componentes (\\(q &lt; p\\)) reter? A escolha de \\(q\\) envolve um trade-off entre a simplicidade (poucos componentes) e a fidelidade aos dados originais (muitos componentes). Não existe uma regra única, mas sim um conjunto de critérios que devem ser avaliados em conjunto.\n\n3.7.1 Critério da Variância Explicada Acumulada\nEste é o critério mais comum. Calculamos a proporção da variância total explicada por cada componente e acumulamos essa proporção.\n\\[\n\\text{Proporção da Variância por } CP_k = \\frac{\\lambda_k}{\\sum_{j=1}^{p} \\lambda_j}\n\\]\nEm seguida, escolhemos o menor número de componentes \\(q\\) cuja variância explicada acumulada atinja um limiar satisfatório, geralmente entre 70% e 90%. A escolha do limiar depende do contexto da análise.\n\n\n3.7.2 Critério do Autovalor (Critério de Kaiser)\nProposto por Henry Kaiser, este critério sugere reter apenas os componentes cujos autovalores (\\(\\lambda_k\\)) são maiores que 1. A intuição por trás dessa regra é mais clara quando a ACP é aplicada sobre a matriz de correlação. Nesse caso, as variáveis originais são padronizadas para ter variância 1. Um componente com autovalor (variância) menor que 1 está, portanto, explicando menos variabilidade do que uma única variável original. Reter tal componente não traria uma “economia” de informação”, tornando-o um candidato à exclusão.\n\n\n3.7.3 Scree Plot (Gráfico de Cotovelo)\nO Scree Plot, proposto por Raymond Cattell, é uma ferramenta visual que nos ajuda a identificar o número ideal de componentes. Ele é um gráfico de linha dos autovalores (variâncias dos componentes) em ordem decrescente.\nTipicamente, o gráfico mostra uma queda acentuada nos primeiros autovalores, seguida por um nivelamento gradual para os autovalores restantes. O ponto onde a curva “dobra” ou forma um “cotovelo” (elbow) é considerado o ponto de corte. A ideia é reter os componentes que aparecem antes do cotovelo, pois eles são os que contribuem mais significativamente para a variância total. Os componentes após o cotovelo formam o “cascalho” (scree) na base de uma montanha e são considerados “ruído”.\n\n\n\n\n\n\n\n\nFigura 3.3: Exemplo de um Scree Plot. O ‘cotovelo’ em k=3 sugere a retenção de 3 componentes.",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#interpretando-os-componentes-principais",
    "href": "src/02_tec_mult/03_acp.html#interpretando-os-componentes-principais",
    "title": "3  Análise de Componentes Principais (ACP)",
    "section": "3.8 Interpretando os Componentes Principais",
    "text": "3.8 Interpretando os Componentes Principais\nUma vez que selecionamos o número de componentes a reter, o passo final é a interpretação. O que esses novos eixos, que são combinações de nossas variáveis originais, realmente significam? A interpretação é uma arte que se baseia na análise das cargas (loadings) ou, mais diretamente, das correlações entre os componentes e as variáveis originais.\nQuando a ACP é realizada sobre a matriz de correlações, as variáveis são padronizadas. Nesse caso, uma opção comum e direta é avaliar os próprios loadings (os autovetores da matriz de correlação) para entender a contribuição de cada variável. Um loading alto (próximo de 1 ou -1) indica que a variável tem uma forte influência na construção daquele componente.\nA etapa mais crucial da ACP é transformar os eixos matemáticos (os componentes) em descobertas práticos. A ferramenta visual mais adequada para essa tarefa é o biplot. O termo “biplot” significa “dois plots” (plot duplo), pois ele sobrepõe duas informações em um único gráfico:\n\nOs scores: As coordenadas das observações no novo espaço dos componentes principais.\nOs loadings: As contribuições das variáveis originais para a criação desses componentes.\n\nO resultado é um mapa rico que mostra não apenas como as observações se agrupam, mas por que elas se agrupam daquela maneira. A interpretação de um biplot segue uma lógica visual. Vamos quebrar em partes:\n\nEixos (Componentes Principais): O eixo horizontal é o CP1 e o vertical é o CP2. Eles são as “réguas” do nosso novo mapa e representam as direções de maior variabilidade nos dados. A porcentagem de variância que cada um explica é mostrada nos seus rótulos.\nPontos (Observações): Cada ponto no gráfico é uma observação.\n\nProximidade: Pontos próximos uns dos outros representam observações com perfis semelhantes (conforme capturado pelos dois primeiros CPs).\nAgrupamentos: Grupos de pontos (clusters) indicam subpopulações nos dados.\n\nVetores (Variáveis Originais): Cada seta (vetor) representa uma das variáveis originais.\n\nDireção: A direção da seta indica como a variável contribui para os dois componentes. Uma seta que aponta para a direita indica uma forte contribuição positiva para o CP1. Uma que aponta para cima, uma forte contribuição positiva para o CP2.\nComprimento: O comprimento da seta é proporcional a quão bem a variável é representada no espaço 2D do biplot. Setas mais longas significam que a variável tem uma forte influência nos componentes mostrados e é bem representada no gráfico. Setas curtas são menos importantes para os dois primeiros CPs ou sua variabilidade está melhor explicada em outros componentes (CP3, CP4, etc.).\nRelações entre Variáveis: O ângulo entre os vetores nos informa sobre a correlação entre as variáveis originais.\n\nÂngulo pequeno (&lt; 90°): As variáveis são positivamente correlacionadas.\nÂngulo de ~90°: As variáveis não são correlacionadas.\nÂngulo obtuso (&gt; 90°): As variáveis são negativamente correlacionadas.\n\n\nRelação entre Pontos e Vetores: Para entender o perfil de um ponto (ou grupo de pontos), projete-o ortogonalmente sobre os vetores das variáveis. Se a projeção de um ponto cai na direção de um vetor, aquela observação tem um valor alto para aquela variável. Se cai na direção oposta, tem um valor baixo.\n\nCom essas regras em mente, vamos analisar um biplot genérico.\n\n\n\n\n\n\n\n\nFigura 3.4: Exemplo de um biplot genérico para ilustrar a interpretação dos seus elementos. Os pontos representam as observações e as setas, as variáveis originais.",
    "crumbs": [
      "Parte II: Técnicas Multivariadas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análise de Componentes Principais (ACP)</span>"
    ]
  }
]