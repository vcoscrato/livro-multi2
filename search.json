[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estat√≠stica Multivariada 2",
    "section": "",
    "text": "Pref√°cio\nSeja Bem-vindo! Este livro aborda uma variedade de t√©cnicas de an√°lise multivariada, essenciais para a compreens√£o de dados complexos em diversas √°reas do conhecimento.\nEste material foi elaborado especialmente para estudantes tendo um primeiro contato com t√©cnicas estat√≠sticas multivariadas. S√£o cobertos os m√©todos a seguir:\n\nAn√°lise de Componentes Principais\nAn√°lise Fatorial\nAn√°lise de Agrupamento\nAn√°lise Discriminante\nAn√°lise de Correla√ß√£o Can√¥nica\nAn√°lise de Correspond√™ncia\n\nO objetivo √© fornecer uma base s√≥lida e pr√°tica para a aplica√ß√£o dessas t√©cnicas. antes, temos uma breve introdu√ß√£o dos conceitos fundamentais que norteiam a an√°lise multivariada e algumas defini√ß√µes e resultados com vetores e matrizes que s√£o importantes para o acompanhamento do livro.\nEspero que este livro seja um recurso valioso em sua jornada de aprendizado.\nVictor Coscrato",
    "crumbs": [
      "Pref√°cio"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html",
    "href": "src/01_intro/intro.html",
    "title": "1¬† Introdu√ß√£o",
    "section": "",
    "text": "1.1 Motiva√ß√£o: Dados de Sa√∫de\nEm diversas √°reas do conhecimento, a coleta de dados frequentemente envolve a medi√ß√£o de m√∫ltiplas caracter√≠sticas para cada unidade de observa√ß√£o. Por exemplo, ao estudar o desempenho de alunos, podemos registrar suas notas em diferentes disciplinas (matem√°tica, portugu√™s, hist√≥ria), o tempo dedicado aos estudos e o n√∫mero de horas de sono. Cada aluno, nesse contexto, √© representado por um conjunto de valores que formam um vetor de vari√°veis. A an√°lise multivariada surge da necessidade de compreender as interconex√µes e padr√µes dentro desses vetores complexos, indo al√©m da an√°lise isolada de cada caracter√≠stica.\nComo o nome sugere, dados multivariados s√£o conjuntos de dados que cont√™m m√∫ltiplas vari√°veis (ou caracter√≠sticas) medidas para cada observa√ß√£o (ou unidade experimental, amostra). Diferente da an√°lise univariada, que foca em uma √∫nica vari√°vel por vez, ou da bivariada, que explora a rela√ß√£o entre duas vari√°veis, a an√°lise multivariada examina simultaneamente as rela√ß√µes entre tr√™s ou mais vari√°veis.\nA estrutura de um conjunto de dados multivariados √© tipicamente representada por uma matriz, onde as linhas correspondem √†s observa√ß√µes e as colunas √†s vari√°veis.\n\\[\n\\mathbf{X} = \\begin{pmatrix}\n\\text{Observa√ß√£o 1} & \\rightarrow & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n\\text{Observa√ß√£o 2} & \\rightarrow & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Observa√ß√£o n} & \\rightarrow & x_{n1} & x_{n2} & \\cdots & x_{np} \\\\\n& & \\uparrow & \\uparrow & & \\uparrow \\\\\n& & \\text{Vari√°vel 1} & \\text{Vari√°vel 2} & \\cdots & \\text{Vari√°vel p}\n\\end{pmatrix}\n\\]\nNesta matriz \\(\\mathbf{X}\\) de dimens√£o \\(n \\times p\\):\nDo ponto de vista probabil√≠stico, consideramos um vetor aleat√≥rio \\(\\mathbf{x} = [X_1, X_2, \\dots, X_p]^T\\). Ou seja, cada vari√°vel de estudo √© uma vari√°vel aleat√≥ria \\(X_j\\), que juntas compoem esse vetor, e a an√°lise multivariada busca entender a distribui√ß√£o conjunta dessas vari√°veis, suas interdepend√™ncias e como elas se relacionam com outras vari√°veis ou grupos.\nPara ilustrar, imagine que estamos coletando dados de sa√∫de de 5 pacientes em um estudo. Para cada paciente, medimos a Altura (em cm), o Peso (em kg) e a Idade (em anos). Esses dados podem ser organizados na seguinte matriz \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} =\n\\begin{pmatrix}\n\\overbrace{175}^{\\text{Altura}} & \\overbrace{70}^{\\text{Peso}} & \\overbrace{25}^{\\text{Idade}} \\\\\n160 & 55 & 32 \\\\\n180 & 85 & 41 \\\\\n170 & 68 & 28 \\\\\n165 & 60 & 35\n\\end{pmatrix}\n\\begin{matrix}\n\\\\\n\\rightarrow \\text{Paciente 1} \\\\\n\\rightarrow \\text{Paciente 2} \\\\\n\\rightarrow \\text{Paciente 3} \\\\\n\\rightarrow \\text{Paciente 4} \\\\\n\\rightarrow \\text{Paciente 5}\n\\end{matrix}\n\\]\nNesta matriz:\nEste vetor captura todas as informa√ß√µes sobre um √∫nico indiv√≠duo.\nEste vetor cont√©m os valores de uma √∫nica vari√°vel para todos os indiv√≠duos.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html#motiva√ß√£o-dados-de-sa√∫de",
    "href": "src/01_intro/intro.html#motiva√ß√£o-dados-de-sa√∫de",
    "title": "1¬† Introdu√ß√£o",
    "section": "",
    "text": "Vetor-Linha (Observa√ß√£o): Cada linha representa um paciente e seu conjunto completo de medidas. Por exemplo, o primeiro paciente √© representado pelo vetor-linha \\[\n\\mathbf{x}_{1 \\bullet} = \\begin{pmatrix} 175 & 70 & 25 \\end{pmatrix}\n\\]\n\n\n\nVetor-Coluna (Vari√°vel): Cada coluna representa uma caracter√≠stica medida para todos os pacientes. Por exemplo, a vari√°vel Altura √© representada pelo vetor-coluna \\[\n\\mathbf{x}_{\\bullet 1} = \\begin{pmatrix} 175 \\\\ 160 \\\\ 180 \\\\ 170 \\\\ 165 \\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\nNota\n\n\n\nO s√≠mbolo \\(\\bullet\\) √© utilizado como um √≠ndice coringa, para se referir a toda uma linha ou coluna na matriz \\(X\\).",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html#por-que-usar-an√°lise-multivariada",
    "href": "src/01_intro/intro.html#por-que-usar-an√°lise-multivariada",
    "title": "1¬† Introdu√ß√£o",
    "section": "1.2 Por que usar An√°lise Multivariada?",
    "text": "1.2 Por que usar An√°lise Multivariada?\nA an√°lise multivariada √© motivada pela necessidade de extrair informa√ß√µes significativas de conjuntos de dados complexos. Ao inv√©s de analisar vari√°veis de forma isolada, essas t√©cnicas permitem uma compreens√£o mais profunda e realista dos dados. Os principais objetivos s√£o:\n\nSimplifica√ß√£o Estrutural: Reduzir a dimensionalidade dos dados, identificando as principais fontes de varia√ß√£o e eliminando redund√¢ncias. Isso facilita a visualiza√ß√£o e a interpreta√ß√£o de dados complexos, revelando a estrutura subjacente de forma mais clara.\nAgrupamento e Classifica√ß√£o: Organizar as observa√ß√µes em grupos homog√™neos (agrupamento) ou atribuir observa√ß√µes a categorias predefinidas (classifica√ß√£o). O objetivo √© identificar padr√µes que permitam segmentar os dados de maneira significativa.\nInvestiga√ß√£o de Estruturas de Depend√™ncia: Explorar e quantificar as rela√ß√µes entre vari√°veis. Isso inclui desde a an√°lise de correla√ß√µes simples at√© a modelagem de intera√ß√µes complexas entre m√∫ltiplos conjuntos de vari√°veis.\nPredi√ß√£o: Construir modelos para prever o valor de uma ou mais vari√°veis com base em outras.\nInfer√™ncia: Realizar testes de hip√≥teses e infer√™ncias estat√≠sticas sobre as rela√ß√µes em um contexto multivariado.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "src/01_intro/intro.html#vis√£o-geral-das-t√©cnicas-multivariadas",
    "href": "src/01_intro/intro.html#vis√£o-geral-das-t√©cnicas-multivariadas",
    "title": "1¬† Introdu√ß√£o",
    "section": "1.3 Vis√£o Geral das T√©cnicas Multivariadas",
    "text": "1.3 Vis√£o Geral das T√©cnicas Multivariadas\nAs t√©cnicas de an√°lise multivariada podem ser classificadas com base em seus objetivos e na natureza das rela√ß√µes entre as vari√°veis. Uma distin√ß√£o fundamental √© entre t√©cnicas de depend√™ncia, que analisam a rela√ß√£o entre vari√°veis dependentes e independentes, e t√©cnicas de interdepend√™ncia, que exploram as rela√ß√µes em um √∫nico conjunto de vari√°veis.\n\nT√©cnicas de Depend√™ncia: Analisam a rela√ß√£o entre uma ou mais vari√°veis dependentes e um conjunto de vari√°veis independentes. O objetivo √© prever ou explicar o valor das vari√°veis dependentes.\nT√©cnicas de Interdepend√™ncia: Exploram as rela√ß√µes entre todas as vari√°veis de um conjunto, sem fazer distin√ß√£o entre dependentes e independentes. O foco √© entender a estrutura geral dos dados.\n\nAl√©m disso a escolha de uma determinada t√©cnica depende tamb√©m dos tipos de vari√°veis em quest√£o.\n\nVari√°veis Categ√≥ricas (Qualitativas): Representam categorias ou grupos (e.g., g√™nero, tipo de produto).\nVari√°veis M√©tricas (Quantitativas): Representam quantidades num√©ricas (e.g., idade, altura, renda, temperatura).\n\nCom o objetivo de classificar os m√©todos a serem apresentados nesse livro e posteriormente auxiliar na escolha da t√©cnica mais adequada para o tratamento de um conjunto de dados, apresentamos a seguir uma tabela com algumas caracter√≠sticas de cada m√©todo e na sequ√™ncia um fluxograma de decis√£o.\n\n\n\nTabela¬†1.1: Principais t√©cnicas abordadas neste livro.\n\n\n\n\n\n\n\n\n\n\n\nT√©cnica\nObjetivo Principal\nTipo de Vari√°vel\nTipo de An√°lise\n\n\n\n\nComponentes Principais (PCA)\nRedu√ß√£o de dimensionalidade\nQuantitativas\nInterdepend√™ncia\n\n\nAn√°lise Fatorial (FA)\nIdentifica√ß√£o de fatores latentes\nQuantitativas\nInterdepend√™ncia\n\n\nAn√°lise de Agrupamento\nForma√ß√£o de grupos homog√™neos\nQuantitativas/Qualitativas\nInterdepend√™ncia\n\n\nAn√°lise Discriminante\nClassifica√ß√£o de observa√ß√µes\nMista (Quali/Quanti)\nDepend√™ncia\n\n\nCorrela√ß√£o Can√¥nica\nRela√ß√£o entre conjuntos de vari√°veis\nQuantitativas\nDepend√™ncia\n\n\nAn√°lise de Correspond√™ncia\nRela√ß√£o entre vari√°veis categ√≥ricas\nQualitativas\nInterdepend√™ncia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\ncluster_blue_leaves\n\n\n\n\ncanonical_corr\n\nCorrela√ß√£o Can√¥nica\n\n\n\ndiscr_analysis\n\nAn√°lise Discriminante\n\n\n\nfactor_analysis\n\nAn√°lise Fatorial\n\n\n\npca\n\nComponentes Principais\n\n\n\ncluster_analysis\n\nAn√°lise de Agrupamento\n\n\n\ncorrespondence_analysis\n\nAn√°lise de Correspond√™ncia\n\n\n\nmulti_regression\n\nRegress√£o M√∫ltipla\n\n\n\nmanova\n\nMANOVA\n\n\n\nlogistic_regression\n\nRegress√£o Log√≠stica\n\n\n\nrelation_type\n\nTipo de rela√ß√£o estudada\n\n\n\ndep_vs_indep\n\nDepend√™ncia\n\n\n\nrelation_type-&gt;dep_vs_indep\n\n\n\n\n\ninterdep\n\nInterdepend√™ncia\n\n\n\nrelation_type-&gt;interdep\n\n\n\n\n\ndep_var_type\n\nVari√°vel Dependente?\n\n\n\ndep_vs_indep-&gt;dep_var_type\n\n\n\n\n\ngoal_interdep\n\nObjetivo?\n\n\n\ninterdep-&gt;goal_interdep\n\n\n\n\n\nnum_dependents\n\nQuantas Dependentes?\n\n\n\ndep_var_type-&gt;num_dependents\n\n\nM√©trica\n\n\n\ngoal_cat_dep\n\nObjetivo?\n\n\n\ndep_var_type-&gt;goal_cat_dep\n\n\nCateg√≥rica\n\n\n\nnum_dependents-&gt;multi_regression\n\n\nUma\n\n\n\ngoal_multi_dep\n\nObjetivo?\n\n\n\nnum_dependents-&gt;goal_multi_dep\n\n\nM√∫ltiplas\n\n\n\ngoal_multi_dep-&gt;canonical_corr\n\n\nRelacionar Conjuntos\n\n\n\ngoal_multi_dep-&gt;manova\n\n\nComparar Grupos\n\n\n\ngoal_cat_dep-&gt;discr_analysis\n\n\nClassificar\n\n\n\ngoal_cat_dep-&gt;logistic_regression\n\n\nPrever Probabilidade\n\n\n\ngoal_interdep-&gt;cluster_analysis\n\n\nAgrupar\n\n\n\nlatent_factors\n\nFatores Latentes?\n\n\n\ngoal_interdep-&gt;latent_factors\n\n\nReduzir Dimens√£o\n\n\n\ncategorical_vars\n\nVari√°veis Categ√≥ricas?\n\n\n\ngoal_interdep-&gt;categorical_vars\n\n\nAssociar\n\n\n\nlatent_factors-&gt;factor_analysis\n\n\nSim\n\n\n\nlatent_factors-&gt;pca\n\n\nN√£o\n\n\n\ncategorical_vars-&gt;correspondence_analysis\n\n\nSim\n\n\n\n\n\n\nFigura¬†1.1: Diagrama de decis√£o para escolha de t√©cnica de An√°lise Multivariada. N√≥s enfatizados fundo azul escuro indicam as t√©cnicas de an√°lise multivariada abordadas neste livro. Importante: Este diagrama √© um guia simplificado para auxiliar na escolha da t√©cnica mais adequada com base nas caracter√≠sticas dos dados e nos objetivos da an√°lise. Ele n√£o √© exaustivo e serve apenas para posicionar as t√©cnicas discutidas neste livro. A escolha final da t√©cnica deve sempre considerar o contexto espec√≠fico do problema e as caracter√≠sticas detalhadas dos dados.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "src/01_intro/vet_mat.html",
    "href": "src/01_intro/vet_mat.html",
    "title": "2¬† Fundamentos Matem√°ticos",
    "section": "",
    "text": "2.1 Medidas descritivas\nCom os conceitos e objetivos da an√°lise multivariada estabelecidos no cap√≠tulo anterior, este cap√≠tulo foca nos fundamentos matem√°ticos que sustentam as t√©cnicas. A representa√ß√£o de dados multivariados atrav√©s de vetores e matrizes √© o ponto de partida para a formaliza√ß√£o dos m√©todos.\nA estrutura central de um conjunto de dados multivariados √© a matriz de dados, \\(\\mathbf{X}\\), de dimens√£o \\(n \\times p\\):\n\\[\n\\mathbf{X} = \\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{pmatrix}\n\\tag{2.1}\\]\nNesta matriz, as \\(n\\) linhas correspondem √†s observa√ß√µes (ou amostras) e as \\(p\\) colunas √†s vari√°veis. O elemento \\(x_{ij}\\) representa o valor da \\(j\\)-√©sima vari√°vel para a \\(i\\)-√©sima observa√ß√£o. A partir desta estrutura, desenvolveremos as medidas descritivas multivariadas e as opera√ß√µes matriciais que formam a base das an√°lises subsequentes.\nPara cada uma das \\(p\\) vari√°veis, podemos calcular a m√©dia amostral:\n\\[\n\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}\n\\tag{2.2}\\]\nAssim como a m√©dia resume a tend√™ncia central de uma √∫nica vari√°vel, o vetor de m√©dias resume a tend√™ncia central de um conjunto de dados multivariados. Ele simplesmente cont√©m a m√©dia de cada vari√°vel.\n\\[\n\\bar{\\mathbf{x}} = \\begin{pmatrix}\n\\bar{x}_1 \\\\\n\\bar{x}_2 \\\\\n\\vdots \\\\\n\\bar{x}_p\n\\end{pmatrix}\n\\]\nDa mesma forma, a vari√¢ncia amostral, para cada vari√°vel, mede a dispers√£o dos dados em torno de sua m√©dia e √© calculada como:\n\\[\ns_j^2 = s_{jj} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2\n\\]\nA covari√¢ncia amostral entre duas vari√°veis, \\(j\\) e \\(k\\), mede como elas variam juntas:\n\\[\ns_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)\n\\]\nGeneralizando essas ideias, a dispers√£o e a inter-rela√ß√£o de todas as vari√°veis s√£o capturadas pela matriz de vari√¢ncias e covari√¢ncias amostral, denotada por \\(\\mathbf{S}\\). Esta √© uma matriz \\(p \\times p\\) cuja entrada \\((j, k)\\) √© a covari√¢ncia entre a \\(j\\)-√©sima e a \\(k\\)-√©sima vari√°vel.\n\\[\n\\mathbf{S} = \\begin{pmatrix}\ns_{11} & s_{12} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\cdots & s_{pp}\n\\end{pmatrix}\n\\]\nNote que a diagonal principal cont√©m as vari√¢ncias de cada vari√°vel, pois a covari√¢ncia de uma vari√°vel com ela mesma √© a sua pr√≥pria vari√¢ncia.\nA matriz de covari√¢ncias \\(\\mathbf{S}\\) √© sempre sim√©trica, isto √©, \\(s_{jk} = s_{kj}\\). Isso √© uma consequencia direta da comutatividade do produto:\n\\[\ns_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k) = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ik} - \\bar{x}_k)(x_{ij} - \\bar{x}_j) = s_{kj}\n\\]\n√â poss√≠vel calcular a matriz de covari√¢ncias de forma mais compacta usando opera√ß√µes matriciais. Primeiro, criamos a matriz de dados centralizados, \\(\\mathbf{X}_c\\), subtraindo o vetor de m√©dias de cada linha da matriz de dados original.\n\\[\n\\mathbf{X}_c = \\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^T\n\\]\nOnde \\(\\mathbf{1}\\) √© um vetor-coluna de uns de tamanho \\(n\\). A matriz de covari√¢ncias √© ent√£o calculada como:\n\\[\n\\mathbf{S} = \\frac{1}{n-1} \\mathbf{X}_c^T \\mathbf{X}_c\n\\]\nA covari√¢ncia √© uma medida √∫til, mas sua magnitude depende da escala das vari√°veis. Por exemplo, a covari√¢ncia entre altura (em metros) e peso (em quilogramas) ser√° muito diferente da covari√¢ncia entre altura (em cent√≠metros) e peso (em gramas), mesmo que a rela√ß√£o subjacente seja a mesma.\nPara obter uma medida de associa√ß√£o que seja independente de escala, usamos a correla√ß√£o. A correla√ß√£o de Pearson, \\(r_{jk}\\), padroniza a covari√¢ncia, dividindo-a pelo produto dos desvios padr√£o das duas vari√°veis:\n\\[\nr_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}} = \\frac{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum_{i=1}^n (x_{ik} - \\bar{x}_k)^2}}\n\\]\nA correla√ß√£o varia entre -1 e 1, onde 1 indica uma rela√ß√£o linear positiva perfeita, -1 indica uma rela√ß√£o linear negativa perfeita, e 0 indica aus√™ncia de rela√ß√£o linear.\nAssim como a matriz de covari√¢ncias, podemos organizar as correla√ß√µes em uma matriz de correla√ß√µes, \\(\\mathbf{R}\\):\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nA diagonal principal da matriz de correla√ß√µes √© sempre 1, pois a correla√ß√£o de uma vari√°vel com ela mesma √© perfeita. Assim como a matriz de covari√¢ncias, a matriz de correla√ß√µes tamb√©m √© sim√©trica (\\(r_{jk} = r_{kj}\\)).",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Fundamentos Matem√°ticos</span>"
    ]
  },
  {
    "objectID": "src/01_intro/vet_mat.html#medidas-descritivas",
    "href": "src/01_intro/vet_mat.html#medidas-descritivas",
    "title": "2¬† Fundamentos Matem√°ticos",
    "section": "",
    "text": "Nota Conceitual: Popula√ß√£o √ó Amostra\n\n\n\nNote que at√© agora trabalhamos apenas com conceitos amostrais, como m√©dias, vari√¢ncias e covari√¢ncias obtidas a partir dos dados observados. No entanto, √© importante frisar que existe uma base probabil√≠stica subjacente, que d√° sentido a essas medidas.\nDo ponto de vista probabil√≠stico, consideramos um vetor aleat√≥rio definido sobre um espa√ßo de probabilidade. \\[\n\\mathbf{x} = (X_1, X_2, \\ldots, X_p)^T,\n\\]\n\n\n\n\n\n\nCuidado\n\n\n\nAs nota√ß√µes nesse ponto podem se confundir. Uma letra min√∫scula em negrito (por exemplo, \\(\\mathbf{x}\\)) √© utilizada para o vetor aleat√≥rio, enquanto uma letra ma√≠scula em negrito (por exemplo, \\(\\mathbf{X}\\)) √© utilizada para a matriz de dados. Finalmente, uma letra ma√≠scula comum (por exemplo, X) √© utilizada para denotar uma vari√°vel aleat√≥ria.\n\n\nEsse vetor possui par√¢metros te√≥ricos fundamentais:\n\nM√©dia (vetor de expectativas):\n\\[\n\\mu_j = E[X_j], \\quad \\boldsymbol{\\mu} = E[\\mathbf{X}]\n\\]\nMatriz de covari√¢ncias populacional:\n\\[\n\\Sigma = (\\sigma_{jk}), \\quad \\sigma_{jk} = Cov(X_j, X_k) = E[(X_j - \\mu_j)(X_k - \\mu_k)]\n\\]\n\nAs observa√ß√µes \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) s√£o realiza√ß√µes independentes do vetor aleat√≥rio \\(\\mathbf{X}\\). A matriz de dados que manipulamos √© composta exatamente por essas realiza√ß√µes.\nNa pr√°tica, entretanto, n√£o temos acesso direto a \\(\\boldsymbol{\\mu}\\) e \\(\\Sigma\\), mas sim √†s suas estimativas amostrais:\n\nVetor de m√©dias amostrais:\n\\[\n\\bar{\\mathbf{x}} = (\\bar{x}_1, \\ldots, \\bar{x}_p)^T, \\quad\n\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij}\n\\]\nMatriz de covari√¢ncias amostral:\n\\[\n\\mathbf{S} = (s_{jk}), \\quad\ns_{jk} = \\frac{1}{n-1}\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)\n\\]\n\n\nEm resumo:\n- No n√≠vel populacional, trabalhamos com o vetor aleat√≥rio e seus par√¢metros te√≥ricos \\((\\boldsymbol{\\mu}, \\Sigma)\\).\n- No n√≠vel amostral, trabalhamos com as observa√ß√µes e estat√≠sticas que estimam esses par√¢metros \\((\\bar{\\mathbf{x}}, \\mathbf{S})\\).\nEste curso tem car√°ter pr√°tico, e por isso utilizaremos predominantemente as medidas amostrais. Ainda assim, √© essencial manter clara a distin√ß√£o entre par√¢metros populacionais e suas estimativas, pois toda an√°lise multivariada repousa nessa rela√ß√£o.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Fundamentos Matem√°ticos</span>"
    ]
  },
  {
    "objectID": "src/01_intro/vet_mat.html#sec-espectral",
    "href": "src/01_intro/vet_mat.html#sec-espectral",
    "title": "2¬† Fundamentos Matem√°ticos",
    "section": "2.2 Decomposi√ß√£o Espectral",
    "text": "2.2 Decomposi√ß√£o Espectral\nA decomposi√ß√£o espectral, tamb√©m conhecida como decomposi√ß√£o de autovalores, √© uma das fatora√ß√µes de matrizes mais importantes da √°lgebra linear, com vastas aplica√ß√µes em estat√≠stica e aprendizado de m√°quina. Ela nos permite decompor uma matriz em suas partes fundamentais: seus autovalores e autovetores. Esta decomposi√ß√£o se aplica a matrizes sim√©tricas, que √© o caso, por exemplo, das matrizes de covari√¢ncia e correla√ß√£o.\n\nAutovetores S√£o vetores especiais associados a uma matriz. A caracter√≠stica principal de um autovetor √© que, quando multiplicado pela matriz, sua dire√ß√£o n√£o muda. O vetor pode ser esticado, comprimido ou ter seu sentido invertido, mas ele permanecer√° na mesma linha.\nAutovalores: Para cada autovetor, existe um autovalor correspondente. O autovalor √© um escalar (um n√∫mero) que nos diz o fator pelo qual o autovetor foi esticado ou comprimido.\n\nMatematicamente, se \\(\\mathbf{v}\\) √© um autovetor da matriz \\(\\mathbf{A}\\) e \\(\\lambda\\) √© seu autovalor correspondente, a rela√ß√£o √© definida como:\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nEsta equa√ß√£o nos diz que o resultado da multiplica√ß√£o da matriz \\(\\mathbf{A}\\) pelo seu autovetor \\(\\mathbf{v}\\) √© o mesmo que simplesmente multiplicar o autovetor \\(\\mathbf{v}\\) pelo escalar \\(\\lambda\\).\nA decomposi√ß√£o espectral afirma que qualquer matriz sim√©trica \\(\\mathbf{A}\\) pode ser reescrita da seguinte forma:\n\\[\n\\mathbf{A} = \\mathbf{P}\\Lambda\\mathbf{P}^T\n\\]\nOnde:\n\n\\(\\mathbf{A}\\): √â a matriz sim√©trica original que queremos decompor.\n\\(\\mathbf{P}\\): √â uma matriz ortogonal cujas colunas s√£o os autovetores de \\(\\mathbf{A}\\). Uma matriz ortogonal tem a propriedade especial de que sua transposta √© tamb√©m sua inversa (\\(\\mathbf{P}^T = \\mathbf{P}^{-1}\\)), o que a torna muito conveniente para c√°lculos.\n\\(\\Lambda\\): √â uma matriz diagonal cujos elementos da diagonal principal s√£o os autovalores de \\(\\mathbf{A}\\), na mesma ordem dos autovetores correspondentes em \\(\\mathbf{P}\\). Todos os outros elementos de \\(\\Lambda\\) s√£o zero.\n\n\n2.2.1 Exemplo\nVamos decompor a seguinte matriz sim√©trica \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n\\]\n\nCalcular Autovalores: Encontramos os autovalores resolvendo a equa√ß√£o caracter√≠stica det(A - ŒªI) = 0. \\[\n\\det \\begin{pmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{pmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = 0\n\\] As ra√≠zes s√£o \\(\\lambda_1 = 3\\) e \\(\\lambda_2 = 1\\).\nCalcular Autovetores:\n\nPara \\(\\lambda_1 = 3\\): Resolvemos (A - 3I)v = 0, que nos d√° o autovetor \\(\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\).\nPara \\(\\lambda_2 = 1\\): Resolvemos (A - 1I)v = 0, que nos d√° o autovetor \\(\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\n\nConstruir as Matrizes P e D:\n\nNormalizamos os autovetores (dividindo por sua norma) para que tenham comprimento 1: \\(\\mathbf{v}'_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}\\) e \\(\\mathbf{v}'_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}\\).\nA matriz \\(\\mathbf{P}\\) √© formada por esses autovetores normalizados como colunas: \\[\n\\mathbf{P} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\\n1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix}\n\\]\nA matriz \\(\\Lambda\\) cont√©m os autovalores na diagonal: \\[\n\\Lambda = \\begin{pmatrix} 3 & 0 \\\\\n0 & 1 \\end{pmatrix}\n\\]\n\n\nA decomposi√ß√£o espectral de \\(\\mathbf{A}\\) √©, portanto, \\(\\mathbf{A} = \\mathbf{P}\\Lambda\\mathbf{P}^T\\).",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Fundamentos Matem√°ticos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html",
    "href": "src/02_tec_mult/03_acp.html",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "",
    "text": "3.1 A Intui√ß√£o Geom√©trica\nA An√°lise de Componentes Principais (ACP) √© uma t√©cnica estat√≠stica multivariada que transforma um conjunto de vari√°veis possivelmente correlacionadas em um novo conjunto de vari√°veis n√£o correlacionadas, chamadas de componentes principais. O objetivo prim√°rio da ACP √© a redu√ß√£o de dimensionalidade: representar a variabilidade presente nos dados originais com um n√∫mero menor de vari√°veis, minimizando a perda de informa√ß√£o.\nCada componente principal √© uma combina√ß√£o linear das vari√°veis originais. O primeiro componente principal √© constru√≠do para capturar a maior variabilidade poss√≠vel nos dados. O segundo componente principal, ortogonal ao primeiro, captura a maior parte da variabilidade restante, e assim por diante. Ao final, o n√∫mero de componentes principais √© igual ao n√∫mero de vari√°veis originais, mas a expectativa √© que os primeiros componentes concentrem a maior parte da informa√ß√£o relevante.\nPara construir a intui√ß√£o, vamos come√ßar com um exemplo simples. Suponha que coletamos dados de duas vari√°veis, Peso (em kg) e Altura (em cm), de um grupo de 10 pessoas.\nTabela de dados com Peso (kg) e Altura (cm).\n\n\nPeso\nAltura\n\n\n\n\n65\n170\n\n\n72\n182\n\n\n58\n165\n\n\n81\n190\n\n\n75\n178\n\n\n60\n168\n\n\n68\n175\n\n\n70\n172\n\n\n78\n185\n\n\n62\n169\nAo plotarmos esses dados (j√° centralizados), obtemos a nuvem de pontos abaixo. O sistema de eixos em preto (Peso, Altura) √© a nossa perspectiva padr√£o.\nFigura¬†3.1: Diagrama de dispers√£o para os dados de Peso e Altura.\nObservando o gr√°fico, notamos que a nuvem de pontos forma uma elipse inclinada, o que indica uma correla√ß√£o entre Peso e Altura. Descrever os dados usando os eixos originais √© perfeitamente v√°lido, mas talvez n√£o seja a forma mais eficiente. A maior parte da variabilidade ocorre ao longo de uma diagonal.\nA ACP prop√µe uma rota√ß√£o dos eixos para que eles se alinhem melhor com a estrutura dos dados. O resultado √© um novo sistema de eixos, os Componentes Principais (\\(CP_1\\) e \\(CP_2\\)), como mostrado abaixo.\nFigura¬†3.2: O sistema de eixos rotacionado (vermelho) se alinha com a m√°xima dispers√£o (vari√¢ncia) dos dados.\nO primeiro componente, \\(CP_1\\), agora aponta na dire√ß√£o de maior ‚Äúalongamento‚Äù da nuvem de pontos. O segundo, \\(CP_2\\), √© perpendicular ao primeiro e aponta na dire√ß√£o de maior variabilidade restante. Encontramos uma nova perspectiva que descreve a estrutura dos dados de forma mais natural e eficiente.\nEm casos com mais de duas vari√°veis (p &gt; 2), a l√≥gica se estende: O i-√©simo componente principal (\\(CP_i\\)) aponta para a dire√ß√£o de maior variabilidade, sob a restri√ß√£o de ser ortogonal (n√£o correlacionado) a todos os componentes anteriores, \\(Cov(CP_j, CP_i) = 0 \\, \\forall \\, j &lt; i\\).\nUma intui√ß√£o geom√©trica para o problema √© buscar o √¢ngulo \\(\\theta\\) de rota√ß√£o do eixo das vari√°veis tal que a vari√¢ncia dos componentes seja m√°xima. Essa rota√ß√£o √© simples de ser observada nesse exemplo bidimensional (veja Figura Figura¬†3.2).",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#vari√¢ncia-como-medida-de-informa√ß√£o",
    "href": "src/02_tec_mult/03_acp.html#vari√¢ncia-como-medida-de-informa√ß√£o",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "3.2 Vari√¢ncia como Medida de Informa√ß√£o",
    "text": "3.2 Vari√¢ncia como Medida de Informa√ß√£o\nA essa altura, voc√™ deve estar se perguntando: por que a dire√ß√£o do ‚Äúmaior alongamento‚Äù √© a mais importante? Em estat√≠stica, a vari√¢ncia √© frequentemente usada como uma medida de informa√ß√£o. Uma vari√°vel com alta vari√¢ncia indica que seus valores s√£o bem espalhados, o que nos ajuda a diferenciar as observa√ß√µes. Se a vari√¢ncia fosse zero, todos os pontos seriam id√™nticos, n√£o nos fornecendo nenhuma informa√ß√£o sobre suas diferen√ßas.\nA ACP utiliza essa ideia para encontrar os eixos mais informativos. Ao rotacionar o sistema de coordenadas, ela n√£o altera a variabilidade total dos dados, mas a redistribui de forma inteligente.\nNo nosso exemplo, as vari√¢ncias das vari√°veis originais s√£o:\n\nVari√¢ncia do Peso: 59.88\nVari√¢ncia da Altura: 66.71\nVari√¢ncia Total Original: 126.59\n\nAp√≥s a rota√ß√£o, as vari√¢ncias ao longo dos novos eixos (os componentes principais) s√£o:\n\nVari√¢ncia de \\(CP_1\\): 123.55\nVari√¢ncia de \\(CP_2\\): 3.04\nVari√¢ncia Total dos Componentes: 126.59\n\nDois fatos cruciais se destacam:\n\nA vari√¢ncia total √© conservada. A soma das vari√¢ncias √© a mesma nos dois sistemas de eixos. Nenhuma informa√ß√£o foi perdida; o ponto de vista foi apenas alterado.\nA vari√¢ncia foi eficientemente redistribu√≠da. O primeiro componente, \\(CP_1\\), agora concentra 97.60% da vari√¢ncia total. Isso significa que, se quis√©ssemos reduzir nossos dados de 2D para 1D, poder√≠amos manter apenas o \\(CP_1\\) e ainda reter a maior parte da informa√ß√£o original. Essa √© a ess√™ncia da redu√ß√£o de dimensionalidade com ACP.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#a-formaliza√ß√£o-matem√°tica",
    "href": "src/02_tec_mult/03_acp.html#a-formaliza√ß√£o-matem√°tica",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "3.3 A Formaliza√ß√£o Matem√°tica",
    "text": "3.3 A Formaliza√ß√£o Matem√°tica\nCom a intui√ß√£o geom√©trica estabelecida, podemos formalizar a An√°lise de Componentes Principais. O objetivo √© transformar um conjunto de vari√°veis correlacionadas \\(\\mathbf{x} = (X_1, \\dots, X_p)^T\\) em um novo conjunto de vari√°veis n√£o correlacionadas, os componentes principais \\(\\mathbf{y} = (Y_1, \\dots, Y_p)^T\\). Cada componente √© uma combina√ß√£o linear das vari√°veis originais:\n\\[\n\\begin{aligned}\nY_1 &= e_{11}X_1 + e_{12}X_2 + \\dots + e_{1p}X_p = \\mathbf{e}_1^T \\mathbf{x} \\\\\nY_2 &= e_{21}X_1 + e_{22}X_2 + \\dots + e_{2p}X_p = \\mathbf{e}_2^T \\mathbf{x} \\\\\n&\\vdots \\\\\nY_p &= e_{p1}X_1 + e_{p2}X_2 + \\dots + e_{pp}X_p = \\mathbf{e}_p^T \\mathbf{x}\n\\end{aligned}\n\\]\nEm nota√ß√£o matricial, a transforma√ß√£o pode ser escrita de forma compacta:\n\\[\n\\mathbf{Y} = \\mathbf{P}^T \\mathbf{X}\n\\tag{3.1}\\]\nOnde \\(\\mathbf{Y}\\) √© o vetor \\(p \\times 1\\) de autovalores e \\(\\mathbf{P}\\) √© a matriz \\(p \\times p\\) cujas colunas s√£o os vetores de coeficientes \\(\\mathbf{e}_k\\).\nEsses componentes s√£o constru√≠dos para satisfazer duas condi√ß√µes fundamentais:\n\nVari√¢ncias Ordenadas: A vari√¢ncia do primeiro componente √© a maior poss√≠vel, a do segundo √© a maior poss√≠vel entre as dire√ß√µes n√£o correlacionadas com o primeiro, e assim por diante. Ou seja, \\(Var(Y_1) \\ge Var(Y_2) \\ge \\dots \\ge Var(Y_p)\\).\nN√£o Correlacionados: Os componentes s√£o ortogonais entre si, o que significa que \\(Cov(Y_i, Y_k) = 0\\) para todo \\(i \\neq k\\).\n\n\n3.3.1 O Problema de Maximiza√ß√£o\nO primeiro componente principal, \\(Y_1 = \\mathbf{e}_1^T\\mathbf{x}\\), √© a combina√ß√£o linear com vari√¢ncia m√°xima. A vari√¢ncia de \\(Y_1\\) √© dada por:\n\\[\nVar(Y_1) = Var(\\mathbf{e}_1^T\\mathbf{x}) = \\mathbf{e}_1^T Var(\\mathbf{x}) \\mathbf{e}_1 = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1\n\\]\nOnde \\(\\mathbf{\\Sigma}\\) √© a matriz de covari√¢ncias de \\(\\mathbf{x}\\). Para evitar que a vari√¢ncia seja aumentada simplesmente inflando os coeficientes em \\(\\mathbf{e}_1\\), impomos a restri√ß√£o de que seu comprimento seja unit√°rio, \\(\\mathbf{e}_1^T\\mathbf{e}_1 = 1\\). Formalmente, o problema de maximiza√ß√£o para o primeiro componente principal se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_1} \\quad & \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1 \\\\\n    \\text{sujeito a} \\quad & \\mathbf{e}_1^T \\mathbf{e}_1 = 1\n\\end{aligned}\n\\]\nPara maximizar a vari√¢ncia sujeita √† restri√ß√£o, utilizamos o m√©todo dos multiplicadores de Lagrange. A fun√ß√£o a ser maximizada √©:\n\\[\nL(\\mathbf{e}_1, \\lambda_1) = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1 - \\lambda_1 (\\mathbf{e}_1^T \\mathbf{e}_1 - 1)\n\\]\nDerivando em rela√ß√£o a \\(\\mathbf{e}_1\\) e igualando a zero, obtemos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_1} = 2 \\mathbf{\\Sigma} \\mathbf{e}_1 - 2 \\lambda_1 \\mathbf{e}_1 = 0\n\\]\nO que nos leva √† equa√ß√£o fundamental de autovalores e autovetores:\n\\[\n\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\n\\]\nEsta equa√ß√£o mostra que o vetor de coeficientes \\(\\mathbf{e}_1\\) deve ser um autovetor da matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\). Para encontrar a vari√¢ncia, pr√©-multiplicamos a equa√ß√£o por \\(\\mathbf{e}_1^T\\):\n\\[\n\\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1^T \\mathbf{e}_1\n\\]\nComo \\(Var(Y_1) = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_1\\) e a restri√ß√£o √© \\(\\mathbf{e}_1^T \\mathbf{e}_1 = 1\\), temos:\n\\[\nVar(Y_1) = \\lambda_1\n\\]\nPara maximizar a vari√¢ncia de \\(Y_1\\), devemos escolher o maior autovalor poss√≠vel. Portanto, \\(\\lambda_1\\) √© o maior autovalor de \\(\\mathbf{\\Sigma}\\), e \\(\\mathbf{e}_1\\) √© o autovetor correspondente.\n\n\n\n\n\n\nNota\n\n\n\nA matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\) √©, por constru√ß√£o, uma matriz sim√©trica e positiva semi-definida. Conforme discutido em Se√ß√£o 2.2, o Teorema Espectral garante que os autovalores de tal matriz s√£o reais e n√£o-negativos, e que seus autovetores correspondentes a autovalores distintos s√£o ortogonais. Esta propriedade √© fundamental para a exist√™ncia e unicidade dos componentes principais.\n\n\n\n\n\n\n\n\nNota\n\n\n\nA demonstra√ß√£o acima, utilizando multiplicadores de Lagrange, √© uma maneira moderna e elegante de conduzir a deriva√ß√£o do problema de m√°ximiza√ß√£o. Uma abordagem cl√°ssica restringe a norma de \\(e\\) atrav√©s do quociente,\n\\[\nVar(Y_1) = \\max_{e_1} \\frac{\\mathbf{e}_1^T \\Sigma \\mathbf{e}_1}{\\mathbf{e}_1^T \\mathbf{e_1}}\n\\]\nEste √© um problema cl√°ssico na √°lgebra linear. Um teorema fundamental afirma que para qualquer matriz sim√©trica \\(A\\), o m√°ximo da forma quadr√°tica \\(\\mathbf{x}^T A \\mathbf{x}\\), sujeito √† restri√ß√£o \\(\\mathbf{x}^T \\mathbf{x} = 1\\), √© o maior autovalor de \\(A\\). O vetor \\(\\mathbf{x}\\) que atinge esse m√°ximo √© o autovetor correspondente. Como a matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\) √© sim√©trica, este teorema se aplica diretamente ao nosso problema.\n\n\n\n\n3.3.2 Componentes Subsequentes\nUma vez encontrada a primeira dire√ß√£o de m√°xima vari√¢ncia, o segundo componente principal, \\(Y_2 = \\mathbf{e}_2^T\\mathbf{x}\\), busca capturar o m√°ximo da variabilidade restante, sob a condi√ß√£o de ser n√£o correlacionado com \\(Y_1\\). A condi√ß√£o de componentes n√£o correlacionados garante que a informa√ß√£o presente no segundo componente principal n√£o √© redundante com rela√ß√£o aquela j√° presente no primeiro. Formalmente, temos:\n\\[\nCov(Y_1, Y_2) = Cov(\\mathbf{e}^T_1 \\mathbf{x}, \\mathbf{e}^T_2 \\mathbf{x}) = \\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_2\n\\]\nComo \\(\\mathbf{e}_1\\) √© o primeiro autovetor, temos \\(\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\\), Assim:\n\\[\n\\mathbf{e}_1^T \\mathbf{\\Sigma} \\mathbf{e}_2 = (\\lambda_1 \\mathbf{e}_1)^T \\mathbf{e}_2 = \\lambda_1 \\mathbf{e}_1^T \\mathbf{e}_2\n\\]\nLogo:\n\\[\nCov(Y_1, Y_2) = 0 \\iff \\mathbf{e}_1^T \\mathbf{e}_2 = 0\n\\]\nCom essa condi√ß√£o bem definida, o problema para o segundo componente se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_2} \\quad & \\mathbf{e}_2^T \\mathbf{\\Sigma} \\mathbf{e}_2 \\\\\n    \\text{sujeito a} \\quad & \\begin{cases}\n        \\mathbf{e}_2^T \\mathbf{e}_2 = 1 \\\\\n        \\mathbf{e}_1^T \\mathbf{e}_2 = 0\n    \\end{cases}\n\\end{aligned}\n\\]\nA fun√ß√£o Lagrangiana agora inclui dois multiplicadores, \\(\\lambda_2\\) e \\(\\phi\\):\n\\[\nL(\\mathbf{e}_2, \\lambda_2, \\phi) = \\mathbf{e}_2^T \\mathbf{\\Sigma} \\mathbf{e}_2 - \\lambda_2(\\mathbf{e}_2^T \\mathbf{e}_2 - 1) - \\phi(\\mathbf{e}_1^T \\mathbf{e}_2 - 0)\n\\]\nDerivando em rela√ß√£o a \\(\\mathbf{e}_2\\) e igualando a zero, temos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_2} = 2\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_2 - \\phi\\mathbf{e}_1 = \\mathbf{0}\n\\]\nPr√©-multiplicando por \\(\\mathbf{e}_1^T\\):\n\\[\n2\\mathbf{e}_1^T\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_1^T\\mathbf{e}_2 - \\phi\\mathbf{e}_1^T\\mathbf{e}_1 = 0\n\\]\nSabendo que:\n\n\\(\\mathbf{e}_1^T\\mathbf{\\Sigma} = \\lambda_1\\mathbf{e}_1^T\\)\n\\(\\mathbf{e}_1^T\\mathbf{e}_2 = 0\\)\n\\(\\mathbf{e}_1^T\\mathbf{e}_1 = 1\\)\n\nA equa√ß√£o se simplifica a \\(\\phi = 0\\). Substituindo \\(\\phi=0\\) de volta na derivada, a equa√ß√£o se torna:\n\\[\n\\mathbf{\\Sigma}\\mathbf{e}_2 = \\lambda_2\\mathbf{e}_2\n\\]\nAssim, \\(\\mathbf{e}_2\\) √© o autovetor de \\(\\mathbf{\\Sigma}\\) correspondente ao autovalor \\(\\lambda_2\\). Como \\(\\lambda_1\\) foi o maior autovalor, para maximizar a vari√¢ncia de \\(Y_2\\), \\(\\lambda_2\\) deve ser o segundo maior autovalor. Este processo se generaliza para os componentes subsequentes.\nEste processo continua: o \\(k\\)-√©simo componente principal (\\(Y_k\\)) √© definido pelo autovetor \\(\\mathbf{e}_k\\) associado ao \\(k\\)-√©simo maior autovalor \\(\\lambda_k\\), garantindo que \\(Var(Y_k) = \\lambda_k\\) e que todos os componentes sejam mutuamente n√£o correlacionados.\n√â neste ponto que a conex√£o com a Se√ß√£o 2.2 se torna expl√≠cita. A matriz \\(\\mathbf{P}\\) (Equa√ß√£o¬†3.1), cujas colunas s√£o os autovetores da matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\), √© exatamente a mesma matriz \\(\\mathbf{P}\\) da decomposi√ß√£o espectral \\(\\mathbf{\\Sigma} = \\mathbf{P}\\mathbf{D}\\mathbf{P}^T\\). Al√©m disso, \\(\\mathbf{D}\\) √© uma matriz diagonal contendo a vari√¢ncia de cada componente principal. Logo, podemos obter todos os componentes principais de maneira pr√°tica e simult√¢nea atrav√©s da decomposi√ß√£o espectral.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#a-import√¢ncia-do-pr√©-processamento-dos-dados",
    "href": "src/02_tec_mult/03_acp.html#a-import√¢ncia-do-pr√©-processamento-dos-dados",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "3.4 A Import√¢ncia do Pr√©-processamento dos Dados",
    "text": "3.4 A Import√¢ncia do Pr√©-processamento dos Dados\nA An√°lise de Componentes Principais √©, em sua ess√™ncia, uma an√°lise de variabilidade. A forma como medimos essa variabilidade impacta diretamente o resultado. Dois pr√©-processamentos s√£o cruciais: a centraliza√ß√£o e o escalonamento.\n\n3.4.1 Centraliza√ß√£o\nNa An√°lise de Componentes Principais (ACP), a centraliza√ß√£o dos dados ‚Äî ou seja, a subtra√ß√£o da m√©dia de cada vari√°vel ‚Äî √© uma etapa fundamental n√£o apenas para o c√°lculo da matriz de covari√¢ncias, mas tamb√©m para a proje√ß√£o dos dados nos componentes principais.\nAo projetar os dados em um componente \\(\\mathbf{e}_i\\), √© imprescind√≠vel que a proje√ß√£o seja feita a partir dos dados centralizados, ou seja:\n\\[\nY_i = \\mathbf{e}_i^ùëá(\\mathbf{x} ‚àí \\bar{\\mathbf{{x}}})\n\\]\nEsse detalhe √© essencial porque os autovetores da ACP s√£o obtidos com base na matriz de covari√¢ncias, a qual descreve a dispers√£o dos dados em torno da m√©dia, e n√£o em torno da origem. Se aplicarmos a proje√ß√£o diretamente sobre \\(\\mathbf{x}\\), sem subtrair a m√©dia, os componentes resultantes n√£o representar√£o adequadamente as dire√ß√µes de maior variabilidade ‚Äî e sim uma combina√ß√£o da dispers√£o com a posi√ß√£o m√©dia dos dados.\nPortanto, para que os componentes principais preservem a interpreta√ß√£o correta como combina√ß√µes lineares que explicam a vari√¢ncia dos dados em torno do centro da nuvem de pontos, √© indispens√°vel que tanto o c√°lculo da matriz de covari√¢ncias quanto a proje√ß√£o dos dados utilizem os dados centralizados.\n\n\n\n\n\n\nImportante\n\n\n\nNo contexto de ACP, √© comum e pr√°tico denotar por \\(\\mathbf{x}\\) o vetor de vari√°veis j√° centralizado. Utilizamos esse abuso de nota√ß√£o durante esse cap√≠tulo para simplifica√ß√£o do texto sem perda de generalidade.\n\n\n\n\n3.4.2 Por que Escalonar? O Dilema da Covari√¢ncia vs.¬†Correla√ß√£o\nA An√°lise de Componentes Principais (ACP) √© sens√≠vel √† escala das vari√°veis. Se uma vari√°vel tiver uma vari√¢ncia numericamente muito maior que as outras ‚Äî mesmo que apenas por causa da sua unidade de medida ‚Äî ela poder√° dominar os primeiros componentes principais.\nImagine incluir uma terceira vari√°vel no conjunto Altura/Peso: a renda mensal, medida em Reais. As vari√¢ncias poderiam ser aproximadamente:\n\nAltura: 80 cm¬≤\n\nPeso: 60 kg¬≤\n\nRenda: 4.000.000 (R$)¬≤\n\nNesse cen√°rio, a vari√¢ncia da Renda √© milhares de vezes maior que a das outras vari√°veis. Se aplicarmos a ACP diretamente na matriz de covari√¢ncias, o primeiro componente principal ser√° fortemente direcionado pela Renda, mesmo que sua correla√ß√£o com as demais vari√°veis seja baixa. Isso ocorre porque a ACP estar√° apenas ‚Äúseguindo‚Äù a dire√ß√£o da vari√°vel com maior vari√¢ncia ‚Äî n√£o necessariamente a mais informativa.\nPara evitar esse vi√©s, escalonamos as vari√°veis: cada uma √© dividida por seu desvio padr√£o. Isso padroniza todas para vari√¢ncia igual a 1. Ao fazer isso, estamos na pr√°tica realizando a ACP sobre a matriz de correla√ß√£o \\((\\mathbf{R})\\) em vez da matriz de covari√¢ncias \\((\\mathbf{\\Sigma})\\).\nVantagem do escalonamento:\nUsar a matriz de correla√ß√£o ‚Äúdemocratiza‚Äù a an√°lise. Todas as vari√°veis come√ßam com a mesma import√¢ncia inicial (vari√¢ncia 1), e a ACP passa a capturar a estrutura de correla√ß√µes, ao inv√©s de ser enviesada pelas diferen√ßas de escala.\nQuando usar a matriz de covari√¢ncias?\nSomente quando todas as vari√°veis est√£o na mesma unidade de medida e possuem uma interpreta√ß√£o compar√°vel. Por exemplo, comparar a temperatura em Celsius em diferentes regi√µes pode fazer sentido sem escalonamento. Fora isso, a matriz de correla√ß√£o √© geralmente a escolha mais robusta e segura.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "href": "src/02_tec_mult/03_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "3.5 Componentes Principais Populacionais vs.¬†Amostrais",
    "text": "3.5 Componentes Principais Populacionais vs.¬†Amostrais\nAt√© este ponto, discutimos os componentes principais em um contexto populacional, onde a matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\) (ou correla√ß√£o \\(\\mathbf{R}\\)) e seus autovalores \\(\\lambda_k\\) e autovetores \\(\\mathbf{e}_k\\) s√£o conhecidos. Na pr√°tica, quase sempre trabalhamos com uma amostra de dados. Nesse caso, n√£o conhecemos os verdadeiros par√¢metros populacionais e devemos estim√°-los.\nOs componentes principais amostrais s√£o obtidos da mesma maneira, mas usando a matriz de covari√¢ncias amostral \\(\\mathbf{S}\\) (ou a matriz de correla√ß√£o amostral \\(\\mathbf{R}\\)). As quantidades resultantes s√£o estimativas dos seus an√°logos populacionais:\n\nO \\(k\\)-√©simo autovalor amostral, \\(\\hat{\\lambda}_k\\), √© uma estimativa de \\(\\lambda_k\\).\nO \\(k\\)-√©simo autovetor amostral, \\(\\hat{\\mathbf{e}}_k\\), √© uma estimativa de \\(\\mathbf{e}_k\\).\nO \\(k\\)-√©simo componente principal amostral, \\(\\hat{Y}_k = \\hat{\\mathbf{e}}_k^T \\mathbf{x}\\), √© uma estimativa de \\(Y_k\\).\n\nA teoria e a interpreta√ß√£o permanecem as mesmas. Para simplificar a nota√ß√£o, ao longo deste cap√≠tulo, omitimos o acento circunflexo (\\(\\hat{\\phantom{a}}\\)), mas √© importante lembrar que, na aplica√ß√£o pr√°tica, estamos sempre lidando com estimativas amostrais.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#correla√ß√£o-entre-componentes-e-vari√°veis-originais",
    "href": "src/02_tec_mult/03_acp.html#correla√ß√£o-entre-componentes-e-vari√°veis-originais",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "3.6 Correla√ß√£o entre Componentes e Vari√°veis Originais",
    "text": "3.6 Correla√ß√£o entre Componentes e Vari√°veis Originais\nOs coeficientes \\(e_{kj}\\) do autovetor \\(\\mathbf{e}_k\\) s√£o chamados de cargas (loadings) e representam o peso da vari√°vel original \\(X_j\\) na forma√ß√£o do componente \\(Y_k\\). Embora as cargas sejam importantes, a sua interpreta√ß√£o pode ser complicada, pois sua magnitude depende das unidades das vari√°veis originais.\nUma medida mais interpret√°vel √© a correla√ß√£o entre os componentes principais e as vari√°veis originais, \\(Cor(Y_k, X_j)\\). Ela nos diz o qu√£o ‚Äúalinhado‚Äù um componente est√° com cada vari√°vel original, numa escala padronizada de -1 a 1. A f√≥rmula para essa correla√ß√£o √©:\n\\[\nCor(Y_k, X_j) = \\frac{e_{kj} \\sqrt{\\lambda_k}}{\\sqrt{s_{jj}}}\n\\]\nOnde:\n\n\\(e_{kj}\\) √© a carga da vari√°vel \\(j\\) no componente \\(k\\).\n\\(\\lambda_k\\) √© o autovalor (vari√¢ncia) do componente \\(k\\).\n\\(s_{jj}\\) √© a vari√¢ncia da vari√°vel original \\(j\\).\n\nQuando a ACP √© realizada sobre a matriz de correla√ß√£o (ou seja, com dados padronizados), as vari√¢ncias \\(s_{jj}\\) s√£o todas iguais a 1. Nesse caso, a f√≥rmula simplifica para \\(Cor(Y_k, X_j) = e_{kj} \\sqrt{\\lambda_k}\\). As correla√ß√µes se tornam proporcionais √†s cargas, facilitando a interpreta√ß√£o.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#escolhendo-o-n√∫mero-de-componentes",
    "href": "src/02_tec_mult/03_acp.html#escolhendo-o-n√∫mero-de-componentes",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "3.7 Escolhendo o N√∫mero de Componentes",
    "text": "3.7 Escolhendo o N√∫mero de Componentes\nA principal vantagem da ACP √© a redu√ß√£o de dimensionalidade. Mas como decidimos quantos componentes (\\(q &lt; p\\)) reter? A escolha de \\(q\\) envolve um trade-off entre a simplicidade (poucos componentes) e a fidelidade aos dados originais (muitos componentes). N√£o existe uma regra √∫nica, mas sim um conjunto de crit√©rios que devem ser avaliados em conjunto.\n\n3.7.1 Crit√©rio da Vari√¢ncia Explicada Acumulada\nEste √© o crit√©rio mais comum. Calculamos a propor√ß√£o da vari√¢ncia total explicada por cada componente e acumulamos essa propor√ß√£o.\n\\[\n\\text{Propor√ß√£o da Vari√¢ncia por } CP_k = \\frac{\\lambda_k}{\\sum_{j=1}^{p} \\lambda_j}\n\\]\nEm seguida, escolhemos o menor n√∫mero de componentes \\(q\\) cuja vari√¢ncia explicada acumulada atinja um limiar satisfat√≥rio, geralmente entre 70% e 90%. A escolha do limiar depende do contexto da an√°lise.\n\n\n3.7.2 Crit√©rio do Autovalor (Crit√©rio de Kaiser)\nProposto por Henry Kaiser, este crit√©rio sugere reter apenas os componentes cujos autovalores (\\(\\lambda_k\\)) s√£o maiores que 1. A intui√ß√£o por tr√°s dessa regra √© mais clara quando a ACP √© aplicada sobre a matriz de correla√ß√£o. Nesse caso, as vari√°veis originais s√£o padronizadas para ter vari√¢ncia 1. Um componente com autovalor (vari√¢ncia) menor que 1 est√°, portanto, explicando menos variabilidade do que uma √∫nica vari√°vel original. Reter tal componente n√£o traria uma ‚Äúeconomia‚Äù de informa√ß√£o‚Äù, tornando-o um candidato √† exclus√£o.\n\n\n3.7.3 Scree Plot (Gr√°fico de Cotovelo)\nO Scree Plot, proposto por Raymond Cattell, √© uma ferramenta visual que nos ajuda a identificar o n√∫mero ideal de componentes. Ele √© um gr√°fico de linha dos autovalores (vari√¢ncias dos componentes) em ordem decrescente.\nTipicamente, o gr√°fico mostra uma queda acentuada nos primeiros autovalores, seguida por um nivelamento gradual para os autovalores restantes. O ponto onde a curva ‚Äúdobra‚Äù ou forma um ‚Äúcotovelo‚Äù (elbow) √© considerado o ponto de corte. A ideia √© reter os componentes que aparecem antes do cotovelo, pois eles s√£o os que contribuem mais significativamente para a vari√¢ncia total. Os componentes ap√≥s o cotovelo formam o ‚Äúcascalho‚Äù (scree) na base de uma montanha e s√£o considerados ‚Äúru√≠do‚Äù.\n\n\n\n\n\n\n\n\nFigura¬†3.3: Exemplo de um Scree Plot. O ‚Äòcotovelo‚Äô em k=3 sugere a reten√ß√£o de 3 componentes.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/03_acp.html#interpretando-os-componentes-principais",
    "href": "src/02_tec_mult/03_acp.html#interpretando-os-componentes-principais",
    "title": "3¬† An√°lise de Componentes Principais (ACP)",
    "section": "3.8 Interpretando os Componentes Principais",
    "text": "3.8 Interpretando os Componentes Principais\nUma vez que selecionamos o n√∫mero de componentes a reter, o passo final √© a interpreta√ß√£o. O que esses novos eixos, que s√£o combina√ß√µes de nossas vari√°veis originais, realmente significam? A interpreta√ß√£o √© uma arte que se baseia na an√°lise das cargas (loadings) ou, mais diretamente, das correla√ß√µes entre os componentes e as vari√°veis originais.\nQuando a ACP √© realizada sobre a matriz de correla√ß√µes, as vari√°veis s√£o padronizadas. Nesse caso, uma op√ß√£o comum e direta √© avaliar os pr√≥prios loadings (os autovetores da matriz de correla√ß√£o) para entender a contribui√ß√£o de cada vari√°vel. Um loading alto (pr√≥ximo de 1 ou -1) indica que a vari√°vel tem uma forte influ√™ncia na constru√ß√£o daquele componente.\nA etapa mais crucial da ACP √© transformar os eixos matem√°ticos (os componentes) em descobertas pr√°ticos. A ferramenta visual mais adequada para essa tarefa √© o biplot. O termo ‚Äúbiplot‚Äù significa ‚Äúdois plots‚Äù (plot duplo), pois ele sobrep√µe duas informa√ß√µes em um √∫nico gr√°fico:\n\nOs scores: As coordenadas das observa√ß√µes no novo espa√ßo dos componentes principais.\nOs loadings: As contribui√ß√µes das vari√°veis originais para a cria√ß√£o desses componentes.\n\nO resultado √© um mapa rico que mostra n√£o apenas como as observa√ß√µes se agrupam, mas por que elas se agrupam daquela maneira. A interpreta√ß√£o de um biplot segue uma l√≥gica visual. Vamos quebrar em partes:\n\nEixos (Componentes Principais): O eixo horizontal √© o CP1 e o vertical √© o CP2. Eles s√£o as ‚Äúr√©guas‚Äù do nosso novo mapa e representam as dire√ß√µes de maior variabilidade nos dados. A porcentagem de vari√¢ncia que cada um explica √© mostrada nos seus r√≥tulos.\nPontos (Observa√ß√µes): Cada ponto no gr√°fico √© uma observa√ß√£o.\n\nProximidade: Pontos pr√≥ximos uns dos outros representam observa√ß√µes com perfis semelhantes (conforme capturado pelos dois primeiros CPs).\nAgrupamentos: Grupos de pontos (clusters) indicam subpopula√ß√µes nos dados.\n\nVetores (Vari√°veis Originais): Cada seta (vetor) representa uma das vari√°veis originais.\n\nDire√ß√£o: A dire√ß√£o da seta indica como a vari√°vel contribui para os dois componentes. Uma seta que aponta para a direita indica uma forte contribui√ß√£o positiva para o CP1. Uma que aponta para cima, uma forte contribui√ß√£o positiva para o CP2.\nComprimento: O comprimento da seta √© proporcional a qu√£o bem a vari√°vel √© representada no espa√ßo 2D do biplot. Setas mais longas significam que a vari√°vel tem uma forte influ√™ncia nos componentes mostrados e √© bem representada no gr√°fico. Setas curtas s√£o menos importantes para os dois primeiros CPs ou sua variabilidade est√° melhor explicada em outros componentes (CP3, CP4, etc.).\nRela√ß√µes entre Vari√°veis: O √¢ngulo entre os vetores nos informa sobre a correla√ß√£o entre as vari√°veis originais.\n\n√Çngulo pequeno (&lt; 90¬∞): As vari√°veis s√£o positivamente correlacionadas.\n√Çngulo de ~90¬∞: As vari√°veis n√£o s√£o correlacionadas.\n√Çngulo obtuso (&gt; 90¬∞): As vari√°veis s√£o negativamente correlacionadas.\n\n\nRela√ß√£o entre Pontos e Vetores: Para entender o perfil de um ponto (ou grupo de pontos), projete-o ortogonalmente sobre os vetores das vari√°veis. Se a proje√ß√£o de um ponto cai na dire√ß√£o de um vetor, aquela observa√ß√£o tem um valor alto para aquela vari√°vel. Se cai na dire√ß√£o oposta, tem um valor baixo.\n\nCom essas regras em mente, vamos analisar um biplot gen√©rico.\n\n\n\n\n\n\n\n\nFigura¬†3.4: Exemplo de um biplot gen√©rico para ilustrar a interpreta√ß√£o dos seus elementos. Os pontos representam as observa√ß√µes e as setas, as vari√°veis originais.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais (ACP)</span>"
    ]
  }
]