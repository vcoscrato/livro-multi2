[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estat√≠stica Multivariada 2",
    "section": "",
    "text": "Pref√°cio\nSeja Bem-vindo! Este livro aborda uma variedade de t√©cnicas de an√°lise multivariada, essenciais para a compreens√£o de dados complexos em diversas √°reas do conhecimento.\nEste material foi elaborado especialmente para estudantes tendo um primeiro contato com t√©cnicas estat√≠sticas multivariadas. S√£o cobertos os m√©todos a seguir:\n\nAn√°lise de Componentes Principais\nAn√°lise Fatorial\nAn√°lise de Agrupamento\nAn√°lise Discriminante\nAn√°lise de Correla√ß√£o Can√¥nica\nAn√°lise de Correspond√™ncia\n\nO objetivo √© fornecer uma base s√≥lida e pr√°tica para a aplica√ß√£o dessas t√©cnicas. antes, temos uma breve introdu√ß√£o dos conceitos fundamentais que norteiam a an√°lise multivariada e algumas defini√ß√µes e resultados vetores e matrizes que s√£o importantes para o acompanhamento do livro.",
    "crumbs": [
      "Pref√°cio"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html",
    "href": "src/01_intro/01_introducao.html",
    "title": "1¬† Introdu√ß√£o",
    "section": "",
    "text": "1.1 Por que usar An√°lise Multivariada?\nA an√°lise multivariada √© o campo da estat√≠stica dedicado a compreender conjuntos de dados com m√∫ltiplas vari√°veis inter-relacionadas. Em vez de analisar cada vari√°vel isoladamente, seu foco √© examinar simultaneamente as rela√ß√µes entre tr√™s ou mais vari√°veis para extrair padr√µes e estruturas que de outra forma permaneceriam ocultos.\nCada observa√ß√£o em um estudo ‚Äî seja um paciente descrito por indicadores de sa√∫de, um consumidor por h√°bitos de compra, ou uma empresa por m√©tricas financeiras ‚Äî pode ser representada como um vetor de observa√ß√µes. A an√°lise multivariada nos fornece as ferramentas para entender a estrutura de depend√™ncia e interdepend√™ncia dentro desses vetores.\nA an√°lise multivariada √© motivada pela necessidade de extrair informa√ß√µes significativas de conjuntos de dados complexos. Ao inv√©s de analisar vari√°veis de forma isolada, essas t√©cnicas permitem uma compreens√£o mais profunda e realista dos dados. Os principais objetivos s√£o:\nNos pr√≥ximos cap√≠tulos, construiremos a base te√≥rica para atingir esses objetivos, come√ßando pelo conceito de vetor aleat√≥rio e seus par√¢metros, para depois explorarmos como as amostras de dados nos permitem estimar e analisar essas estruturas.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html#por-que-usar-an√°lise-multivariada",
    "href": "src/01_intro/01_introducao.html#por-que-usar-an√°lise-multivariada",
    "title": "1¬† Introdu√ß√£o",
    "section": "",
    "text": "Simplifica√ß√£o Estrutural: Reduzir a dimensionalidade dos dados, identificando as principais fontes de varia√ß√£o e eliminando redund√¢ncias. Isso facilita a visualiza√ß√£o e a interpreta√ß√£o de dados complexos, revelando a estrutura subjacente de forma mais clara.\nAgrupamento e Classifica√ß√£o: Organizar as observa√ß√µes em grupos homog√™neos (agrupamento) ou atribuir observa√ß√µes a categorias predefinidas (classifica√ß√£o). O objetivo √© identificar padr√µes que permitam segmentar os dados de maneira significativa.\nInvestiga√ß√£o de Estruturas de Depend√™ncia: Explorar e quantificar as rela√ß√µes entre vari√°veis. Isso inclui desde a an√°lise de correla√ß√µes simples at√© a modelagem de intera√ß√µes complexas entre m√∫ltiplos conjuntos de vari√°veis.\nPredi√ß√£o: Construir modelos para prever o valor de uma ou mais vari√°veis com base em outras.\nInfer√™ncia: Realizar testes de hip√≥teses e infer√™ncias estat√≠sticas sobre as rela√ß√µes em um contexto multivariado.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html#vis√£o-geral-das-t√©cnicas-multivariadas",
    "href": "src/01_intro/01_introducao.html#vis√£o-geral-das-t√©cnicas-multivariadas",
    "title": "1¬† Introdu√ß√£o",
    "section": "1.2 Vis√£o Geral das T√©cnicas Multivariadas",
    "text": "1.2 Vis√£o Geral das T√©cnicas Multivariadas\nAs t√©cnicas de an√°lise multivariada podem ser classificadas com base em seus objetivos e na natureza das rela√ß√µes entre as vari√°veis. Uma distin√ß√£o fundamental √© entre t√©cnicas de depend√™ncia, que analisam a rela√ß√£o entre vari√°veis dependentes e independentes, e t√©cnicas de interdepend√™ncia, que exploram as rela√ß√µes em um √∫nico conjunto de vari√°veis.\n\nT√©cnicas de Depend√™ncia: Analisam a rela√ß√£o entre uma ou mais vari√°veis dependentes e um conjunto de vari√°veis independentes. O objetivo √© prever ou explicar o valor das vari√°veis dependentes.\nT√©cnicas de Interdepend√™ncia: Exploram as rela√ß√µes entre todas as vari√°veis de um conjunto, sem fazer distin√ß√£o entre dependentes e independentes. O foco √© entender a estrutura geral dos dados.\n\nAl√©m disso a escolha de uma determinada t√©cnica depende tamb√©m dos tipos de vari√°veis em quest√£o.\n\nVari√°veis Categ√≥ricas (Qualitativas): Representam categorias ou grupos (e.g., g√™nero, tipo de produto).\nVari√°veis M√©tricas (Quantitativas): Representam quantidades num√©ricas (e.g., idade, altura, renda, temperatura).\n\nCom o objetivo de classificar os m√©todos a serem apresentados nesse livro e posteriormente auxiliar na escolha da t√©cnica mais adequada para o tratamento de um conjunto de dados, apresentamos a seguir uma tabela com algumas caracter√≠sticas de cada m√©todo e na sequ√™ncia um fluxograma de decis√£o.\n\n\n\nTabela¬†1.1: Principais t√©cnicas abordadas neste livro.\n\n\n\n\n\n\n\n\n\n\n\nT√©cnica\nObjetivo Principal\nTipo de Vari√°vel\nTipo de An√°lise\n\n\n\n\nComponentes Principais (PCA)\nRedu√ß√£o de dimensionalidade\nQuantitativas\nInterdepend√™ncia\n\n\nAn√°lise Fatorial (FA)\nIdentifica√ß√£o de fatores latentes\nQuantitativas\nInterdepend√™ncia\n\n\nAn√°lise de Agrupamento\nForma√ß√£o de grupos homog√™neos\nQuantitativas/Qualitativas\nInterdepend√™ncia\n\n\nAn√°lise Discriminante\nClassifica√ß√£o de observa√ß√µes\nMista (Quali/Quanti)\nDepend√™ncia\n\n\nCorrela√ß√£o Can√¥nica\nRela√ß√£o entre conjuntos de vari√°veis\nQuantitativas\nDepend√™ncia\n\n\nAn√°lise de Correspond√™ncia\nRela√ß√£o entre vari√°veis categ√≥ricas\nQualitativas\nInterdepend√™ncia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\ncluster_blue_leaves\n\n\n\n\ncanonical_corr\n\nCorrela√ß√£o Can√¥nica\n\n\n\ndiscr_analysis\n\nAn√°lise Discriminante\n\n\n\nfactor_analysis\n\nAn√°lise Fatorial\n\n\n\npca\n\nComponentes Principais\n\n\n\ncluster_analysis\n\nAn√°lise de Agrupamento\n\n\n\ncorrespondence_analysis\n\nAn√°lise de Correspond√™ncia\n\n\n\nmulti_regression\n\nRegress√£o M√∫ltipla\n\n\n\nmanova\n\nMANOVA\n\n\n\nlogistic_regression\n\nRegress√£o Log√≠stica\n\n\n\nrelation_type\n\nTipo de rela√ß√£o estudada\n\n\n\ndep_vs_indep\n\nDepend√™ncia\n\n\n\nrelation_type-&gt;dep_vs_indep\n\n\n\n\n\ninterdep\n\nInterdepend√™ncia\n\n\n\nrelation_type-&gt;interdep\n\n\n\n\n\ndep_var_type\n\nVari√°vel Dependente?\n\n\n\ndep_vs_indep-&gt;dep_var_type\n\n\n\n\n\ngoal_interdep\n\nObjetivo?\n\n\n\ninterdep-&gt;goal_interdep\n\n\n\n\n\nnum_dependents\n\nQuantas Dependentes?\n\n\n\ndep_var_type-&gt;num_dependents\n\n\nM√©trica\n\n\n\ngoal_cat_dep\n\nObjetivo?\n\n\n\ndep_var_type-&gt;goal_cat_dep\n\n\nCateg√≥rica\n\n\n\nnum_dependents-&gt;multi_regression\n\n\nUma\n\n\n\ngoal_multi_dep\n\nObjetivo?\n\n\n\nnum_dependents-&gt;goal_multi_dep\n\n\nM√∫ltiplas\n\n\n\ngoal_multi_dep-&gt;canonical_corr\n\n\nRelacionar Conjuntos\n\n\n\ngoal_multi_dep-&gt;manova\n\n\nComparar Grupos\n\n\n\ngoal_cat_dep-&gt;discr_analysis\n\n\nClassificar\n\n\n\ngoal_cat_dep-&gt;logistic_regression\n\n\nPrever Probabilidade\n\n\n\ngoal_interdep-&gt;cluster_analysis\n\n\nAgrupar\n\n\n\nlatent_factors\n\nFatores Latentes?\n\n\n\ngoal_interdep-&gt;latent_factors\n\n\nReduzir Dimens√£o\n\n\n\ncategorical_vars\n\nVari√°veis Categ√≥ricas?\n\n\n\ngoal_interdep-&gt;categorical_vars\n\n\nAssociar\n\n\n\nlatent_factors-&gt;factor_analysis\n\n\nSim\n\n\n\nlatent_factors-&gt;pca\n\n\nN√£o\n\n\n\ncategorical_vars-&gt;correspondence_analysis\n\n\nSim\n\n\n\n\n\n\nFigura¬†1.1: Diagrama de decis√£o para escolha de t√©cnica de An√°lise Multivariada. N√≥s enfatizados fundo azul escuro indicam as t√©cnicas de an√°lise multivariada abordadas neste livro. Importante: Este diagrama √© um guia simplificado para auxiliar na escolha da t√©cnica mais adequada com base nas caracter√≠sticas dos dados e nos objetivos da an√°lise. Ele n√£o √© exaustivo e serve apenas para posicionar as t√©cnicas discutidas neste livro. A escolha final da t√©cnica deve sempre considerar o contexto espec√≠fico do problema e as caracter√≠sticas detalhadas dos dados.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html",
    "href": "src/01_intro/02_vetores_aleatorios.html",
    "title": "2¬† Vetores Aleat√≥rios e Suas Caracter√≠sticas",
    "section": "",
    "text": "2.1 O Vetor Aleat√≥rio\nNo cap√≠tulo anterior, estabelecemos a motiva√ß√£o para a an√°lise multivariada: a necessidade de entender sistemas complexos onde m√∫ltiplas vari√°veis interagem. Para fazer isso de maneira formal e rigorosa, precisamos primeiro definir o objeto matem√°tico central de nosso estudo. Em vez de come√ßar com uma tabela de dados, come√ßamos com o conceito que gera esses dados: o vetor aleat√≥rio.\nImagine que, para uma popula√ß√£o de interesse (e.g., todos os estudantes de uma universidade), associamos a cada membro um conjunto de \\(p\\) caracter√≠sticas que nos interessam (e.g., nota em matem√°tica, nota em hist√≥ria, horas de estudo). Antes de observarmos um membro espec√≠fico, os valores dessas caracter√≠sticas s√£o incertos. Podemos modelar essa incerteza tratando cada caracter√≠stica como uma vari√°vel aleat√≥ria.\nEste vetor √© a representa√ß√£o matem√°tica de uma ‚Äúobserva√ß√£o multivariada‚Äù em n√≠vel populacional. Toda a teoria da an√°lise multivariada se baseia na compreens√£o das propriedades e da estrutura de distribui√ß√£o deste vetor.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Vetores Aleat√≥rios e Suas Caracter√≠sticas</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#o-vetor-aleat√≥rio",
    "href": "src/01_intro/02_vetores_aleatorios.html#o-vetor-aleat√≥rio",
    "title": "2¬† Vetores Aleat√≥rios e Suas Caracter√≠sticas",
    "section": "",
    "text": "Defini√ß√£o 2.1 Um vetor aleat√≥rio \\(\\mathbf{x}\\) √© um vetor-coluna cujos componentes s√£o \\(p\\) vari√°veis aleat√≥rias, \\(X_1, X_2, \\ldots, X_p\\).\n\\[\n\\mathbf{x} = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\n\\vdots \\\\\nX_p\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\nCuidado com a Nota√ß√£o\n\n\n\nA nota√ß√£o em an√°lise multivariada exige aten√ß√£o. - Uma letra min√∫scula em negrito (e.g., \\(\\mathbf{x}\\)) denota um vetor aleat√≥rio. - Uma letra mai√∫scula comum (e.g., \\(X_j\\)) denota uma vari√°vel aleat√≥ria escalar, o \\(j\\)-√©simo componente do vetor. - Mais adiante, uma letra mai√∫scula em negrito (e.g., \\(\\mathbf{X}\\)) ser√° usada para a matriz de dados (amostral).",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Vetores Aleat√≥rios e Suas Caracter√≠sticas</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#par√¢metros-populacionais",
    "href": "src/01_intro/02_vetores_aleatorios.html#par√¢metros-populacionais",
    "title": "2¬† Vetores Aleat√≥rios e Suas Caracter√≠sticas",
    "section": "2.2 Par√¢metros Populacionais",
    "text": "2.2 Par√¢metros Populacionais\nAssim como vari√°veis aleat√≥rias escalares s√£o caracterizadas por par√¢metros como a m√©dia (expectativa) e a vari√¢ncia, os vetores aleat√≥rios tamb√©m o s√£o. Esses par√¢metros descrevem a tend√™ncia central, a dispers√£o e as inter-rela√ß√µes das vari√°veis que comp√µem o vetor.\n\nDefini√ß√£o 2.2 O vetor de m√©dias populacional, denotado por \\(\\boldsymbol{\\mu}\\), √© o vetor das expectativas de cada uma de suas vari√°veis componentes.\n\\[\n\\boldsymbol{\\mu} = E[\\mathbf{x}] = \\begin{pmatrix}\nE[X_1] \\\\\nE[X_2] \\\\\n\\vdots \\\\\nE[X_p]\n\\end{pmatrix} = \\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{pmatrix}\n\\]\nGeometricamente, \\(\\boldsymbol{\\mu}\\) representa o centr√≥ide (centro de massa) da distribui√ß√£o de probabilidade no espa√ßo \\(p\\)-dimensional.\n\n\nDefini√ß√£o 2.3 A matriz de covari√¢ncias populacional, denotada por \\(\\boldsymbol{\\Sigma}\\), √© uma matriz sim√©trica \\(p \\times p\\) cujo elemento \\((j, k)\\) √© a covari√¢ncia entre a \\(j\\)-√©sima e a \\(k\\)-√©sima vari√°vel aleat√≥ria, \\(\\sigma_{jk} = \\text{Cov}(X_j, X_k) = E[(X_j - \\mu_j)(X_k - \\mu_k)]\\).\n\\[\n\\boldsymbol{\\Sigma} = \\text{Cov}[\\mathbf{x}] = E[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})'] = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp}\n\\end{pmatrix}\n\\]\n\nDiagonal (\\(\\sigma_{jj}\\)): As vari√¢ncias, \\(\\text{Var}(X_j)\\), medem a dispers√£o de cada vari√°vel.\nFora da Diagonal (\\(\\sigma_{jk}\\)): As covari√¢ncias, medem a tend√™ncia de associa√ß√£o linear entre as vari√°veis \\(X_j\\) e \\(X_k\\).\nSimetria: A matriz √© sim√©trica, pois \\(\\text{Cov}(X_j, X_k) = \\text{Cov}(X_k, X_j)\\), o que implica \\(\\sigma_{jk} = \\sigma_{kj}\\).\n\n\n\nDefini√ß√£o 2.4 A matriz de correla√ß√µes populacional, denotada por \\(\\mathbf{P}\\), √© uma vers√£o reescalada da matriz de covari√¢ncias, com elementos \\(\\rho_{jk} = \\frac{\\sigma_{jk}}{\\sqrt{\\sigma_{jj}}\\sqrt{\\sigma_{kk}}}\\).\n\\[\n\\mathbf{P} = \\begin{pmatrix}\n1 & \\rho_{12} & \\cdots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\cdots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nSeus elementos \\(\\rho_{jk}\\) variam de -1 a 1, fornecendo uma medida de associa√ß√£o linear livre de escala.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Vetores Aleat√≥rios e Suas Caracter√≠sticas</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#propriedades-de-boldsymbolmu-e-boldsymbolsigma",
    "href": "src/01_intro/02_vetores_aleatorios.html#propriedades-de-boldsymbolmu-e-boldsymbolsigma",
    "title": "2¬† Vetores Aleat√≥rios e Suas Caracter√≠sticas",
    "section": "2.3 Propriedades de \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)",
    "text": "2.3 Propriedades de \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)\nAs propriedades de combina√ß√µes lineares s√£o generaliza√ß√µes diretas dos resultados univariados. Seja \\(\\mathbf{x}\\) um vetor aleat√≥rio \\(p\\)-dimensional com m√©dia \\(\\boldsymbol{\\mu}\\) e covari√¢ncia \\(\\boldsymbol{\\Sigma}\\). Sejam \\(\\mathbf{c}\\) um vetor de constantes \\(p \\times 1\\) e \\(\\mathbf{A}\\) uma matriz de constantes \\(q \\times p\\).\n\nEsperan√ßa de uma Combina√ß√£o Linear: \\[\nE[\\mathbf{A}\\mathbf{x} + \\mathbf{c}] = \\mathbf{A}E[\\mathbf{x}] + \\mathbf{c} = \\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{c}\n\\]\nCovari√¢ncia de uma Combina√ß√£o Linear: \\[\n\\text{Cov}[\\mathbf{A}\\mathbf{x} + \\mathbf{c}] = \\mathbf{A} \\text{Cov}[\\mathbf{x}] \\mathbf{A}' = \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}'\n\\]\n\nNo pr√≥ximo cap√≠tulo, veremos como, na pr√°tica, n√£o temos acesso a esses par√¢metros populacionais (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\Sigma}\\), \\(\\mathbf{P}\\)), mas podemos usar dados observados para obter estimativas confi√°veis deles.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Vetores Aleat√≥rios e Suas Caracter√≠sticas</span>"
    ]
  },
  {
    "objectID": "src/01_intro/03_amostragem_e_estimacao.html",
    "href": "src/01_intro/03_amostragem_e_estimacao.html",
    "title": "3¬† Amostra e Estima√ß√£o de Par√¢metros",
    "section": "",
    "text": "3.1 Da Popula√ß√£o √† Amostra\nNo cap√≠tulo anterior, introduzimos os conceitos te√≥ricos que descrevem uma popula√ß√£o multivariada: o vetor de m√©dias \\(\\boldsymbol{\\mu}\\) e a matriz de covari√¢ncias \\(\\boldsymbol{\\Sigma}\\). Esses par√¢metros s√£o constru√ß√µes ideais que existem no n√≠vel populacional. Na pr√°tica, quase nunca temos acesso a toda a popula√ß√£o para calcul√°-los diretamente.\nO nosso trabalho como estat√≠sticos e analistas de dados √© fazer infer√™ncias sobre esses par√¢metros desconhecidos com base em um conjunto limitado de dados. Fazemos isso atrav√©s da amostragem.\nAssumimos que coletamos uma amostra aleat√≥ria de \\(n\\) observa√ß√µes da popula√ß√£o. Cada observa√ß√£o, \\(\\mathbf{x}_i\\) (com \\(i=1, \\ldots, n\\)), √© uma realiza√ß√£o independente do vetor aleat√≥rio \\(\\mathbf{x}\\) que definimos no cap√≠tulo anterior.\nA cole√ß√£o de todas essas observa√ß√µes forma o nosso conjunto de dados. √â aqui que, finalmente, introduzimos a matriz de dados, \\(\\mathbf{X}\\), uma estrutura central em toda a an√°lise multivariada aplicada.\nA matriz \\(\\mathbf{X}\\) √© uma matriz de dimens√£o \\(n \\times p\\), onde cada linha √© uma observa√ß√£o multivariada e cada coluna representa uma vari√°vel.\n\\[\n\\mathbf{X} = \\begin{pmatrix}\n\\mathbf{x}_1' \\\\\n\\mathbf{x}_2' \\\\\n\\vdots \\\\\n\\mathbf{x}_n'\n\\end{pmatrix} = \\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{pmatrix}\n\\tag{3.1}\\]\nO elemento \\(x_{ij}\\) representa o valor da \\(j\\)-√©sima vari√°vel para a \\(i\\)-√©sima observa√ß√£o. Com esta matriz em m√£os, nosso objetivo √© calcular quantidades que sirvam como boas estimativas para os par√¢metros populacionais \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\).",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Amostra e Estima√ß√£o de Par√¢metros</span>"
    ]
  },
  {
    "objectID": "src/01_intro/03_amostragem_e_estimacao.html#estimadores-amostrais",
    "href": "src/01_intro/03_amostragem_e_estimacao.html#estimadores-amostrais",
    "title": "3¬† Amostra e Estima√ß√£o de Par√¢metros",
    "section": "3.2 Estimadores Amostrais",
    "text": "3.2 Estimadores Amostrais\nAs quantidades que calculamos a partir da amostra s√£o chamadas de estat√≠sticas amostrais ou estimadores, e s√£o as contrapartes amostrais dos par√¢metros populacionais.\n\nDefini√ß√£o 3.1 O estimador de \\(\\boldsymbol{\\mu}\\) √© o vetor de m√©dias amostral, \\(\\bar{\\mathbf{x}}\\), cujos componentes \\(\\bar{x}_j\\) s√£o a m√©dia das observa√ß√µes para a \\(j\\)-√©sima vari√°vel.\n\\[\n\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}, \\quad \\text{resultando em} \\quad \\bar{\\mathbf{x}} = \\begin{pmatrix}\n\\bar{x}_1 \\\\\n\\vdots \\\\\n\\bar{x}_p\n\\end{pmatrix}\n\\]\n\n\nDefini√ß√£o 3.2 O estimador de \\(\\boldsymbol{\\Sigma}\\) √© a matriz de covari√¢ncias amostral, \\(\\mathbf{S}\\). Seus elementos s√£o a vari√¢ncia amostral (\\(s_{jj}\\)) e a covari√¢ncia amostral (\\(s_{jk}\\)).\n\\(s_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)\\)\nA matriz resultante √©:\n\\[\n\\mathbf{S} = \\begin{pmatrix}\ns_{11} & s_{12} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\cdots & s_{pp}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nPor que dividir por \\(n-1\\)?\n\n\n\nA divis√£o por \\(n-1\\) (graus de liberdade) em vez de \\(n\\) √© feita para garantir que \\(s_{jk}\\) seja um estimador n√£o-viesado de \\(\\sigma_{jk}\\), ou seja, \\(E[s_{jk}] = \\sigma_{jk}\\).\n\n\n\n\nDefini√ß√£o 3.3 O estimador de \\(\\mathbf{P}\\) √© a matriz de correla√ß√µes amostral, \\(\\mathbf{R}\\), cujos elementos \\(r_{jk}\\) s√£o obtidos padronizando a covari√¢ncia amostral.\n\\(r_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}} = \\frac{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum_{i=1}^n (x_{ik} - \\bar{x}_k)^2}}\\)\nA matriz resultante √©:\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nA matriz \\(\\mathbf{R}\\) √© uma matriz sim√©trica com 1s na diagonal.\n\n\nEm resumo: Neste cap√≠tulo, fizemos a ponte crucial entre a teoria e a pr√°tica. - No n√≠vel populacional, temos par√¢metros te√≥ricos e n√£o observ√°veis (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\Sigma}\\)). - No n√≠vel amostral, temos dados observ√°veis na matriz \\(\\mathbf{X}\\), a partir da qual calculamos estat√≠sticas (\\(\\bar{\\mathbf{x}}\\), \\(\\mathbf{S}\\)) que estimam esses par√¢metros.\nA maior parte das t√©cnicas que veremos neste livro opera sobre as matrizes \\(\\mathbf{S}\\) ou \\(\\mathbf{R}\\) para fazer infer√™ncias sobre a estrutura da popula√ß√£o.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Amostra e Estima√ß√£o de Par√¢metros</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html",
    "href": "src/01_intro/04_normal_multivariada.html",
    "title": "4¬† A Distribui√ß√£o Normal Multivariada",
    "section": "",
    "text": "4.1 A Fun√ß√£o de Densidade\nAt√© agora, discutimos os par√¢metros de um vetor aleat√≥rio (\\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)) sem assumir uma forma espec√≠fica para sua distribui√ß√£o de probabilidade. No entanto, para desenvolvermos uma teoria de infer√™ncia estat√≠stica robusta e compreendermos o funcionamento de muitas t√©cnicas cl√°ssicas, precisamos de um modelo de distribui√ß√£o de refer√™ncia. Na an√°lise multivariada, esse papel √© desempenhado pela distribui√ß√£o Normal Multivariada (NMV).\nA NMV √© uma generaliza√ß√£o da distribui√ß√£o normal (Gaussiana) para o caso de \\(p\\) vari√°veis. Ela √©, de longe, a distribui√ß√£o mais importante da an√°lise multivariada, por v√°rias raz√µes: 1. Muitos fen√¥menos naturais podem ser aproximados pela NMV. 2. O Teorema do Limite Central, em sua forma multivariada, garante que a m√©dia de vetores aleat√≥rios de (quase) qualquer distribui√ß√£o tende a se comportar como uma NMV para amostras grandes. 3. Suas propriedades matem√°ticas s√£o extremamente convenientes e bem compreendidas, o que facilita muito a deriva√ß√£o de resultados te√≥ricos.\nUm vetor aleat√≥rio \\(\\mathbf{x}\\) de dimens√£o \\(p\\) segue uma distribui√ß√£o Normal Multivariada com vetor de m√©dias \\(\\boldsymbol{\\mu}\\) e matriz de covari√¢ncias \\(\\boldsymbol{\\Sigma}\\) (positiva definida), denotado por \\(\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), se sua fun√ß√£o de densidade de probabilidade (FDP) for dada por:\n\\[\nf(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right)\n\\]\nOnde: - \\(|\\boldsymbol{\\Sigma}|\\) √© o determinante da matriz de covari√¢ncias. - \\(\\boldsymbol{\\Sigma}^{-1}\\) √© a inversa da matriz de covari√¢ncias.\nApesar de parecer intimidante, a estrutura da FDP √© bastante l√≥gica. O termo no expoente, \\((\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\), √© uma forma quadr√°tica que mede a ‚Äúdist√¢ncia‚Äù do ponto \\(\\mathbf{x}\\) ao centro \\(\\boldsymbol{\\mu}\\), ponderada pela estrutura de covari√¢ncia \\(\\boldsymbol{\\Sigma}\\). Essa dist√¢ncia √© chamada de dist√¢ncia de Mahalanobis. Quanto maior essa dist√¢ncia, menor o valor da fun√ß√£o de densidade, o que faz todo o sentido.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>A Distribui√ß√£o Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html#propriedades-da-distribui√ß√£o-normal-multivariada",
    "href": "src/01_intro/04_normal_multivariada.html#propriedades-da-distribui√ß√£o-normal-multivariada",
    "title": "4¬† A Distribui√ß√£o Normal Multivariada",
    "section": "4.2 Propriedades da Distribui√ß√£o Normal Multivariada",
    "text": "4.2 Propriedades da Distribui√ß√£o Normal Multivariada\nA popularidade da NMV vem de suas propriedades elegantes:\n\nCombina√ß√µes Lineares: Se \\(\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), ent√£o qualquer combina√ß√£o linear de suas componentes, \\(\\mathbf{a}'\\mathbf{x} = a_1X_1 + \\dots + a_pX_p\\), segue uma distribui√ß√£o normal univariada. Mais geralmente, se \\(\\mathbf{A}\\) √© uma matriz de constantes, ent√£o \\(\\mathbf{Ax}\\) segue uma distribui√ß√£o Normal Multivariada.\nDistribui√ß√µes Marginais: Qualquer subconjunto de vari√°veis de um vetor Normal Multivariado tamb√©m segue uma distribui√ß√£o Normal Multivariada. Por exemplo, se \\(\\mathbf{x} = [X_1, X_2, X_3]'\\) √© NMV, ent√£o o vetor \\([X_1, X_3]'\\) tamb√©m √© NMV.\nIndepend√™ncia e Covari√¢ncia Zero: Para a maioria das distribui√ß√µes, covari√¢ncia zero n√£o implica independ√™ncia. No entanto, para a NMV, essa implica√ß√£o √© verdadeira. Se um subconjunto de vari√°veis em um vetor NMV tem covari√¢ncia zero com outro subconjunto, ent√£o esses dois subconjuntos s√£o independentes. Esta √© uma propriedade extremamente poderosa.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>A Distribui√ß√£o Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html#visualizando-a-normal-bivariada",
    "href": "src/01_intro/04_normal_multivariada.html#visualizando-a-normal-bivariada",
    "title": "4¬† A Distribui√ß√£o Normal Multivariada",
    "section": "4.3 Visualizando a Normal Bivariada",
    "text": "4.3 Visualizando a Normal Bivariada\nPara ganhar intui√ß√£o, √© √∫til visualizar o caso bivariado (\\(p=2\\)). A fun√ß√£o de densidade forma uma superf√≠cie em forma de sino no espa√ßo 3D. Os contornos de densidade constante, quando projetados no plano \\((x_1, x_2)\\), formam elipses.\n\\[\n(\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) = c^2\n\\]\nA forma e a orienta√ß√£o dessas elipses s√£o inteiramente determinadas pela matriz de covari√¢ncias \\(\\boldsymbol{\\Sigma}\\).\n\nSe \\(\\sigma_{12} = 0\\) e \\(\\sigma_{11} = \\sigma_{22}\\): As vari√°veis s√£o n√£o correlacionadas (e, portanto, independentes) e t√™m a mesma vari√¢ncia. Os contornos s√£o c√≠rculos.\nSe \\(\\sigma_{12} = 0\\) e \\(\\sigma_{11} \\neq \\sigma_{22}\\): As vari√°veis s√£o n√£o correlacionadas, mas com vari√¢ncias diferentes. Os contornos s√£o elipses alinhadas com os eixos de coordenadas.\nSe \\(\\sigma_{12} \\neq 0\\): As vari√°veis s√£o correlacionadas. Os contornos s√£o elipses rotacionadas. A dire√ß√£o do eixo principal da elipse √© determinada pelos autovetores de \\(\\boldsymbol{\\Sigma}\\), e o comprimento dos eixos √© determinado pelos autovalores.\n\n\n\n\n\n\n\n\n\nG\n\ncluster_A\n\nœÉ‚ÇÅ‚ÇÇ = 0, œÉ‚ÇÅ‚ÇÅ = œÉ‚ÇÇ‚ÇÇ\n\n\ncluster_B\n\nœÉ‚ÇÅ‚ÇÇ = 0, œÉ‚ÇÅ‚ÇÅ &gt; œÉ‚ÇÇ‚ÇÇ\n\n\ncluster_C\n\nœÉ‚ÇÅ‚ÇÇ &gt; 0\n\n\n\nA\n\n\n\n\nB\n\n\n\n\n\nC\n\nrotacionada\n\n\n\n\n\n\n\nFigura¬†4.1: Contornos de densidade para uma distribui√ß√£o Normal Bivariada, ilustrando o efeito da matriz de covari√¢ncia.\n\n\n\n\n\nEssa conex√£o entre a √°lgebra da matriz \\(\\boldsymbol{\\Sigma}\\) e a geometria da distribui√ß√£o de dados √© um dos temas mais importantes da an√°lise multivariada e ser√° a base para a t√©cnica de Componentes Principais, que exploraremos mais adiante.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>A Distribui√ß√£o Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html",
    "href": "src/01_intro/05_algebra_matricial.html",
    "title": "5¬† Fundamentos Matem√°ticos: √Ålgebra Matricial",
    "section": "",
    "text": "5.1 Formas Quadr√°ticas\nCom os conceitos estat√≠sticos fundamentais estabelecidos, voltamos nossa aten√ß√£o para as ferramentas matem√°ticas necess√°rias para manipular esses objetos. A linguagem da an√°lise multivariada √© a √°lgebra linear.\nNeste cap√≠tulo, revisaremos conceitos-chave ‚Äî formas quadr√°ticas, matrizes positiva-definidas e a decomposi√ß√£o espectral ‚Äî que s√£o a base para muitas das t√©cnicas que veremos, como a An√°lise de Componentes Principais (PCA).\nUma forma quadr√°tica √© uma fun√ß√£o polinomial de v√°rias vari√°veis que cont√©m apenas termos de grau dois. Para um vetor \\(\\mathbf{x}\\) de dimens√£o \\(p \\times 1\\) e uma matriz sim√©trica \\(\\mathbf{A}\\) de dimens√£o \\(p \\times p\\), a forma quadr√°tica √© expressa como:\n\\[\nQ(\\mathbf{x}) = \\mathbf{x}' \\mathbf{A} \\mathbf{x} = \\sum_{i=1}^p \\sum_{j=1}^p a_{ij} x_i x_j\n\\]\nUm exemplo fundamental que j√° encontramos √© a dist√¢ncia de Mahalanobis ao quadrado, \\((\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\), que aparece no expoente da distribui√ß√£o normal multivariada. Esta forma quadr√°tica define as elipses de contorno de densidade constante da distribui√ß√£o.",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentos Matem√°ticos: √Ålgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html#matrizes-positiva-definidas",
    "href": "src/01_intro/05_algebra_matricial.html#matrizes-positiva-definidas",
    "title": "5¬† Fundamentos Matem√°ticos: √Ålgebra Matricial",
    "section": "5.2 Matrizes positiva-Definidas",
    "text": "5.2 Matrizes positiva-Definidas\nO conceito de positividade para um n√∫mero escalar √© estendido para matrizes atrav√©s das formas quadr√°ticas. Uma matriz sim√©trica \\(\\mathbf{A}\\) √© dita:\n\npositiva-definida se \\(\\mathbf{x}' \\mathbf{A} \\mathbf{x} &gt; 0\\) para todos os vetores n√£o-nulos \\(\\mathbf{x}\\).\npositiva-semidefinida se \\(\\mathbf{x}' \\mathbf{A} \\mathbf{x} \\geq 0\\) para todos os vetores n√£o-nulos \\(\\mathbf{x}\\).\n\nPropriedades de uma matriz positiva-definida: - Todos os seus autovalores s√£o estritamente positivos (\\(\\lambda_i &gt; 0\\)). - A matriz √© invert√≠vel (n√£o-singular). - Seu determinante √© positivo.\nMatrizes de covari√¢ncia (\\(\\boldsymbol{\\Sigma}\\)) e correla√ß√£o (\\(\\mathbf{R}\\)) s√£o, por natureza, positiva-semidefinidas. Para que a fun√ß√£o de densidade da normal multivariada seja bem definida e a matriz \\(\\boldsymbol{\\Sigma}\\) seja invert√≠vel, exigimos que ela seja positiva-definida. Isso implica que nenhuma vari√°vel no vetor aleat√≥rio √© uma combina√ß√£o linear perfeita de outras (ou seja, n√£o h√° redund√¢ncia linear total nos dados).",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentos Matem√°ticos: √Ålgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html#sec-espectral",
    "href": "src/01_intro/05_algebra_matricial.html#sec-espectral",
    "title": "5¬† Fundamentos Matem√°ticos: √Ålgebra Matricial",
    "section": "5.3 Decomposi√ß√£o Espectral",
    "text": "5.3 Decomposi√ß√£o Espectral\nA decomposi√ß√£o espectral (ou de autovalores) √© uma fatora√ß√£o de uma matriz sim√©trica em seus autovalores e autovetores. Ela revela a estrutura fundamental da transforma√ß√£o linear representada pela matriz.\nToda matriz sim√©trica \\(\\mathbf{A}\\) de dimens√£o \\(p \\times p\\) pode ser reescrita como:\n\\[\n\\mathbf{A} = \\mathbf{E}\\Lambda\\mathbf{E}'\n\\]\nOnde:\n\n\\(\\lambda_1, \\dots, \\lambda_p\\) s√£o os autovalores de \\(\\mathbf{A}\\).\n\\(\\mathbf{e}_1, \\dots, \\mathbf{e}_p\\) s√£o os autovetores ortonormais correspondentes.\n\\(\\Lambda\\) √© a matriz diagonal com os autovalores \\(\\lambda_i\\) na diagonal.\n\\(\\mathbf{E}\\) √© a matriz ortogonal cujas colunas s√£o os autovetores \\(\\mathbf{e}_i\\).\n\n\nExemplo 5.1 Vamos decompor a seguinte matriz de covari√¢ncias \\(\\mathbf{S}\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2\n\\end{pmatrix}\n\\]\n\nAutovalores: Resolvendo a equa√ß√£o caracter√≠stica \\(\\det(\\mathbf{S} - \\lambda\\mathbf{I}) = 0\\), encontramos \\(\\lambda_1 = 3\\) e \\(\\lambda_2 = 1\\).\nAutovetores:\n\nPara \\(\\lambda_1 = 3\\): O autovetor correspondente √© \\(\\mathbf{e}_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2}\n\\end{pmatrix}\\).\nPara \\(\\lambda_2 = 1\\): O autovetor correspondente √© \\(\\mathbf{e}_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2}\n\\end{pmatrix}\\).\n\n\nA decomposi√ß√£o √© \\(\\mathbf{S} = \\mathbf{E}\\Lambda\\mathbf{E}'\\), com: \\[\n\\mathbf{E} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2}\n\\end{pmatrix}, \\quad\n\\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1\n\\end{pmatrix}\n\\]\nIsso nos diz que a maior vari√¢ncia dos dados (igual a 3) est√° na dire√ß√£o do vetor \\((1, 1)\\), enquanto a vari√¢ncia na dire√ß√£o ortogonal \\((1, -1)\\) √© menor (igual a 1).",
    "crumbs": [
      "Parte I: Introdu√ß√£o",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentos Matem√°ticos: √Ålgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html",
    "href": "src/02_tec_mult/06_acp.html",
    "title": "6¬† An√°lise de Componentes Principais",
    "section": "",
    "text": "6.1 Vari√¢ncia como Medida de Informa√ß√£o\nA An√°lise de Componentes Principais (ACP ou PCA do acr√¥nimo em ingl√™s) √© uma t√©cnica estat√≠stica multivariada que transforma um conjunto de vari√°veis possivelmente correlacionadas em um novo conjunto de vari√°veis n√£o correlacionadas, chamadas de componentes principais. O objetivo prim√°rio da ACP √© a redu√ß√£o de dimensionalidade: representar a variabilidade presente nos dados originais com um n√∫mero menor de vari√°veis, minimizando a perda de informa√ß√£o.\nCada componente principal √© uma combina√ß√£o linear das vari√°veis originais. O primeiro componente principal √© constru√≠do para capturar a maior variabilidade poss√≠vel nos dados. O segundo componente principal, ortogonal ao primeiro, captura a maior parte da variabilidade restante, e assim por diante. Ao final, o n√∫mero de componentes principais √© igual ao n√∫mero de vari√°veis originais, mas a expectativa √© que os primeiros componentes concentrem a maior parte da informa√ß√£o relevante.\nGeometricamente, a ACP √© uma proje√ß√£o do espa√ßo original de vari√°veis para um outro espa√ßo com caracter√≠sticas mais interessantes: A vari√¢ncia dos dados √© concentrada em dire√ß√µes especificas (os componentes principais) e n√£o existem correla√ß√µes entre os novos eixos.\nPara construir a intui√ß√£o geom√©trica, vamos come√ßar com um exemplo simples, em duas dimens√µes.\nA essa altura, voc√™ deve estar se perguntando: por que a dire√ß√£o do ‚Äúmaior alongamento‚Äù √© a mais importante? Em estat√≠stica, a vari√¢ncia √© frequentemente usada como uma medida de informa√ß√£o. Uma vari√°vel com alta vari√¢ncia indica que seus valores s√£o bem espalhados, o que nos ajuda a diferenciar as observa√ß√µes. Se a vari√¢ncia fosse zero, todos os pontos seriam id√™nticos, n√£o nos fornecendo nenhuma informa√ß√£o sobre suas diferen√ßas.\nA ACP utiliza essa ideia para encontrar os eixos mais informativos. Ao rotacionar o sistema de coordenadas, ela n√£o altera a variabilidade total dos dados, mas a redistribui de forma inteligente.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#vari√¢ncia-como-medida-de-informa√ß√£o",
    "href": "src/02_tec_mult/06_acp.html#vari√¢ncia-como-medida-de-informa√ß√£o",
    "title": "6¬† An√°lise de Componentes Principais",
    "section": "",
    "text": "Defini√ß√£o 6.1 A vari√¢ncia total de um conjunto de dados com \\(p\\) vari√°veis √© a soma das vari√¢ncias de cada vari√°vel individual. Matematicamente, se \\(\\mathbf{x} = (X_1, \\dots, X_p)'\\) √© o vetor de vari√°veis aleat√≥rias com matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\), a vari√¢ncia total √© definida como:\n\\[\n\\text{Vari√¢ncia Total} = \\sum_{j=1}^{p} \\text{Var}(X_j) = \\sum_{j=1}^{p} \\sigma_{jj} = tr(\\mathbf{\\Sigma})\n\\]\nOnde \\(\\sigma_{jj}\\) √© a vari√¢ncia da \\(j\\)-√©sima vari√°vel e \\(tr(\\mathbf{\\Sigma})\\) √© o tra√ßo da matriz de covari√¢ncias (a soma dos elementos da diagonal principal). Essa medida representa a dispers√£o total na nuvem de pontos, somando a variabilidade em cada uma das dire√ß√µes dos eixos originais.\n\n\nExemplo 6.2 Voltando ao exemplo Exemplo¬†6.1, as vari√¢ncias das vari√°veis originais s√£o:\n\nVari√¢ncia do Peso: 59.88\nVari√¢ncia da Altura: 66.71\nVari√¢ncia Total Original: 126.59\n\nAp√≥s a rota√ß√£o, as vari√¢ncias ao longo dos novos eixos (os componentes principais) s√£o:\n\nVari√¢ncia de \\(CP_1\\): 123.55\nVari√¢ncia de \\(CP_2\\): 3.04\nVari√¢ncia Total dos Componentes: 126.59\n\nDois fatos cruciais se destacam:\n\nA vari√¢ncia total √© conservada. A soma das vari√¢ncias √© a mesma nos dois sistemas de eixos. Nenhuma informa√ß√£o foi perdida; o ponto de vista foi apenas alterado.\nA vari√¢ncia foi eficientemente redistribu√≠da. O primeiro componente, \\(CP_1\\), agora concentra 97.60% da vari√¢ncia total. Isso significa que, se quis√©ssemos reduzir nossos dados de 2D para 1D, poder√≠amos manter apenas o \\(CP_1\\) e ainda reter a maior parte da informa√ß√£o original. Essa √© a ess√™ncia da redu√ß√£o de dimensionalidade com ACP.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#a-formaliza√ß√£o-matem√°tica",
    "href": "src/02_tec_mult/06_acp.html#a-formaliza√ß√£o-matem√°tica",
    "title": "6¬† An√°lise de Componentes Principais",
    "section": "6.2 A Formaliza√ß√£o Matem√°tica",
    "text": "6.2 A Formaliza√ß√£o Matem√°tica\nCom a intui√ß√£o geom√©trica estabelecida, podemos formalizar a An√°lise de Componentes Principais. O objetivo √© transformar um conjunto de vari√°veis correlacionadas \\(\\mathbf{x} = (X_1, \\dots, X_p)'\\) em um novo conjunto de vari√°veis n√£o correlacionadas, os componentes principais \\(\\mathbf{y} = (Y_1, \\dots, Y_p)'\\). Cada componente √© uma combina√ß√£o linear das vari√°veis originais:\n\\[\n\\begin{aligned}\nY_1 &= e_{11}X_1 + e_{12}X_2 + \\dots + e_{1p}X_p = \\mathbf{e}_1' \\mathbf{x} \\\\\nY_2 &= e_{21}X_1 + e_{22}X_2 + \\dots + e_{2p}X_p = \\mathbf{e}_2' \\mathbf{x} \\\\\n&\\vdots \\\\\nY_p &= e_{p1}X_1 + e_{p2}X_2 + \\dots + e_{pp}X_p = \\mathbf{e}_p' \\mathbf{x}\n\\end{aligned}\n\\]\nEm nota√ß√£o matricial, a transforma√ß√£o pode ser escrita de forma compacta:\n\\[\n\\mathbf{Y} = \\mathbf{E}' \\mathbf{X}\n\\tag{6.1}\\]\nOnde \\(\\mathbf{Y}\\) √© o vetor \\(p \\times 1\\) de autovalores e \\(\\mathbf{E}\\) √© a matriz \\(p \\times p\\) cujas colunas s√£o os vetores de coeficientes \\(\\mathbf{e}_k\\).\nEsses componentes s√£o constru√≠dos para satisfazer duas condi√ß√µes fundamentais:\n\nVari√¢ncias Ordenadas: A vari√¢ncia do primeiro componente √© a maior poss√≠vel, a do segundo √© a maior poss√≠vel entre as dire√ß√µes n√£o correlacionadas com o primeiro, e assim por diante. Ou seja, \\(\\text{Var}(Y_1) \\ge \\text{Var}(Y_2) \\ge \\dots \\ge \\text{Var}(Y_p)\\).\nN√£o Correlacionados: Os componentes s√£o ortogonais entre si, o que significa que \\(\\text{Cov}[Y_i, Y_k] = 0\\) para todo \\(i \\neq k\\).\n\n\n6.2.1 O Problema de Maximiza√ß√£o\nO primeiro componente principal, \\(Y_1 = \\mathbf{e}_1'\\mathbf{x}\\), √© a combina√ß√£o linear com vari√¢ncia m√°xima. A vari√¢ncia de \\(Y_1\\) √© dada por:\n\\[\n\\text{Var}(Y_1) = \\text{Var}(\\mathbf{e}_1'\\mathbf{x}) = \\mathbf{e}_1' \\text{Var}(\\mathbf{x}) \\mathbf{e}_1 = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1\n\\]\nOnde \\(\\mathbf{\\Sigma}\\) √© a matriz de covari√¢ncias de \\(\\mathbf{x}\\). Para evitar que a vari√¢ncia seja aumentada simplesmente inflando os coeficientes em \\(\\mathbf{e}_1\\), impomos a restri√ß√£o de que seu comprimento seja unit√°rio, \\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\). Formalmente, o problema de maximiza√ß√£o para o primeiro componente principal se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_1} \\quad & \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 \\\\\n    \\text{sujeito a} \\quad & \\mathbf{e}_1' \\mathbf{e}_1 = 1\n\\end{aligned}\n\\]\nPara maximizar a vari√¢ncia sujeita √† restri√ß√£o, utilizamos o m√©todo dos multiplicadores de Lagrange. A fun√ß√£o a ser maximizada √©:\n\\[\nL(\\mathbf{e}_1, \\lambda_1) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 - \\lambda_1 (\\mathbf{e}_1' \\mathbf{e}_1 - 1)\n\\]\nDerivando em rela√ß√£o a \\(\\mathbf{e}_1\\) e igualando a zero, obtemos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_1} = 2 \\mathbf{\\Sigma} \\mathbf{e}_1 - 2 \\lambda_1 \\mathbf{e}_1 = 0\n\\]\nO que nos leva √† equa√ß√£o fundamental de autovalores e autovetores:\n\\[\n\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\n\\]\nEsta equa√ß√£o mostra que o vetor de coeficientes \\(\\mathbf{e}_1\\) deve ser um autovetor da matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\). Para encontrar a vari√¢ncia, pr√©-multiplicamos a equa√ß√£o por \\(\\mathbf{e}_1'\\):\n\\[\n\\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1' \\mathbf{e}_1\n\\]\nComo \\(\\text{Var}(Y_1) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1\\) e a restri√ß√£o √© \\(\\mathbf{e}_1' \\mathbf{e}_1 = 1\\), temos:\n\\[\n\\text{Var}(Y_1) = \\lambda_1\n\\]\nPara maximizar a vari√¢ncia de \\(Y_1\\), devemos escolher o maior autovalor poss√≠vel. Portanto, \\(\\lambda_1\\) √© o maior autovalor de \\(\\mathbf{\\Sigma}\\), e \\(\\mathbf{e}_1\\) √© o autovetor correspondente.\n\n\n\n\n\n\nNota\n\n\n\nA matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\) √©, por constru√ß√£o, uma matriz sim√©trica e positiva semi-definida. Conforme discutido em Se√ß√£o 5.3, o Teorema Espectral garante que os autovalores de tal matriz s√£o reais e n√£o-negativos, e que seus autovetores correspondentes a autovalores distintos s√£o ortogonais. Esta propriedade √© fundamental para a exist√™ncia e unicidade dos componentes principais.\n\n\n\n\n\n\n\n\nNota\n\n\n\nA demonstra√ß√£o acima, utilizando multiplicadores de Lagrange, √© uma maneira moderna e elegante de conduzir a deriva√ß√£o do problema de m√°ximiza√ß√£o. Uma abordagem cl√°ssica restringe a norma de \\(e\\) atrav√©s do quociente,\n\\[\n\\text{Var}(Y_1) = \\max_{e_1} \\frac{\\mathbf{e}_1' \\Sigma \\mathbf{e}_1}{\\mathbf{e}_1' \\mathbf{e_1}}\n\\]\nEste √© um problema cl√°ssico na √°lgebra linear. Um teorema fundamental afirma que para qualquer matriz sim√©trica \\(A\\), o m√°ximo da forma quadr√°tica \\(\\mathbf{x}' A \\mathbf{x}\\), sujeito √† restri√ß√£o \\(\\mathbf{x}' \\mathbf{x} = 1\\), √© o maior autovalor de \\(A\\). O vetor \\(\\mathbf{x}\\) que atinge esse m√°ximo √© o autovetor correspondente. Como a matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\) √© sim√©trica, este teorema se aplica diretamente ao nosso problema.\n\n\n\n\n6.2.2 Componentes Subsequentes\nUma vez encontrada a primeira dire√ß√£o de m√°xima vari√¢ncia, o segundo componente principal, \\(Y_2 = \\mathbf{e}_2'\\mathbf{x}\\), busca capturar o m√°ximo da variabilidade restante, sob a condi√ß√£o de ser n√£o correlacionado com \\(Y_1\\). A condi√ß√£o de componentes n√£o correlacionados garante que a informa√ß√£o presente no segundo componente principal n√£o √© redundante com rela√ß√£o aquela j√° presente no primeiro. Formalmente, temos:\n\\[\nCov(Y_1, Y_2) = Cov(\\mathbf{e}'_1 \\mathbf{x}, \\mathbf{e}'_2 \\mathbf{x}) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_2\n\\]\nComo \\(\\mathbf{e}_1\\) √© o primeiro autovetor, temos \\(\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\\), Assim:\n\\[\n\\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_2 = (\\lambda_1 \\mathbf{e}_1)' \\mathbf{e}_2 = \\lambda_1 \\mathbf{e}_1' \\mathbf{e}_2\n\\]\nLogo:\n\\[\nCov(Y_1, Y_2) = 0 \\iff \\mathbf{e}_1' \\mathbf{e}_2 = 0\n\\]\nCom essa condi√ß√£o bem definida, o problema para o segundo componente se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_2} \\quad & \\mathbf{e}_2' \\mathbf{\\Sigma} \\mathbf{e}_2 \\\\\n    \\text{sujeito a} \\quad & \\begin{cases}\n        \\mathbf{e}_2' \\mathbf{e}_2 = 1 \\\\\n        \\mathbf{e}_1' \\mathbf{e}_2 = 0\n    \\end{cases}\n\\end{aligned}\n\\]\nA fun√ß√£o Lagrangiana agora inclui dois multiplicadores, \\(\\lambda_2\\) e \\(\\phi\\):\n\\[\nL(\\mathbf{e}_2, \\lambda_2, \\phi) = \\mathbf{e}_2' \\mathbf{\\Sigma} \\mathbf{e}_2 - \\lambda_2(\\mathbf{e}_2' \\mathbf{e}_2 - 1) - \\phi(\\mathbf{e}_1' \\mathbf{e}_2 - 0)\n\\]\nDerivando em rela√ß√£o a \\(\\mathbf{e}_2\\) e igualando a zero, temos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_2} = 2\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_2 - \\phi\\mathbf{e}_1 = \\mathbf{0}\n\\]\nPr√©-multiplicando por \\(\\mathbf{e}_1'\\):\n\\[\n2\\mathbf{e}_1'\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_1'\\mathbf{e}_2 - \\phi\\mathbf{e}_1'\\mathbf{e}_1 = 0\n\\]\nSabendo que:\n\n\\(\\mathbf{e}_1'\\mathbf{\\Sigma} = \\lambda_1\\mathbf{e}_1'\\)\n\\(\\mathbf{e}_1'\\mathbf{e}_2 = 0\\)\n\\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\)\n\nA equa√ß√£o se simplifica a \\(\\phi = 0\\). Substituindo \\(\\phi=0\\) de volta na derivada, a equa√ß√£o se torna:\n\\[\n\\mathbf{\\Sigma}\\mathbf{e}_2 = \\lambda_2\\mathbf{e}_2\n\\]\nAssim, \\(\\mathbf{e}_2\\) √© o autovetor de \\(\\mathbf{\\Sigma}\\) correspondente ao autovalor \\(\\lambda_2\\). Como \\(\\lambda_1\\) foi o maior autovalor, para maximizar a vari√¢ncia de \\(Y_2\\), \\(\\lambda_2\\) deve ser o segundo maior autovalor. Este processo se generaliza para os componentes subsequentes.\nEste processo continua: o \\(k\\)-√©simo componente principal (\\(Y_k\\)) √© definido pelo autovetor \\(\\mathbf{e}_k\\) associado ao \\(k\\)-√©simo maior autovalor \\(\\lambda_k\\), garantindo que \\(\\text{Var}(Y_k) = \\lambda_k\\) e que todos os componentes sejam mutuamente n√£o correlacionados.\n√â neste ponto que a conex√£o com a Se√ß√£o 5.3 se torna expl√≠cita. A matriz \\(\\mathbf{P}\\) (Equa√ß√£o¬†6.1), cujas colunas s√£o os autovetores da matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\), √© exatamente a mesma matriz \\(\\mathbf{P}\\) da decomposi√ß√£o espectral \\(\\mathbf{\\Sigma} = \\mathbf{P}\\Lambda\\mathbf{P}'\\). Al√©m disso, \\(\\Lambda\\) √© uma matriz diagonal contendo a vari√¢ncia de cada componente principal. Logo, podemos obter todos os componentes principais de maneira pr√°tica e simult√¢nea atrav√©s da decomposi√ß√£o espectral.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#a-import√¢ncia-do-pr√©-processamento-dos-dados",
    "href": "src/02_tec_mult/06_acp.html#a-import√¢ncia-do-pr√©-processamento-dos-dados",
    "title": "6¬† An√°lise de Componentes Principais",
    "section": "6.3 A Import√¢ncia do Pr√©-processamento dos Dados",
    "text": "6.3 A Import√¢ncia do Pr√©-processamento dos Dados\nA An√°lise de Componentes Principais √©, em sua ess√™ncia, uma an√°lise de variabilidade. A forma como medimos essa variabilidade impacta diretamente o resultado. Dois pr√©-processamentos s√£o cruciais: a centraliza√ß√£o e o escalonamento.\n\n6.3.1 Centraliza√ß√£o\nNa An√°lise de Componentes Principais (ACP), a centraliza√ß√£o dos dados ‚Äî ou seja, a subtra√ß√£o da m√©dia de cada vari√°vel ‚Äî √© uma etapa fundamental n√£o apenas para o c√°lculo da matriz de covari√¢ncias, mas tamb√©m para a proje√ß√£o dos dados nos componentes principais.\nAo projetar os dados em um componente \\(\\mathbf{e}_i\\), √© imprescind√≠vel que a proje√ß√£o seja feita a partir dos dados centralizados, ou seja:\n\\[\nY_i = \\mathbf{e}_i^ùëá(\\mathbf{x} ‚àí \\bar{\\mathbf{{x}}})\n\\]\nEsse detalhe √© essencial porque os autovetores da ACP s√£o obtidos com base na matriz de covari√¢ncias, a qual descreve a dispers√£o dos dados em torno da m√©dia, e n√£o em torno da origem. Se aplicarmos a proje√ß√£o diretamente sobre \\(\\mathbf{x}\\), sem subtrair a m√©dia, os componentes resultantes n√£o representar√£o adequadamente as dire√ß√µes de maior variabilidade ‚Äî e sim uma combina√ß√£o da dispers√£o com a posi√ß√£o m√©dia dos dados.\nPortanto, para que os componentes principais preservem a interpreta√ß√£o correta como combina√ß√µes lineares que explicam a vari√¢ncia dos dados em torno do centro da nuvem de pontos, √© indispens√°vel que tanto o c√°lculo da matriz de covari√¢ncias quanto a proje√ß√£o dos dados utilizem os dados centralizados.\n\n\n\n\n\n\nImportante\n\n\n\nNo contexto de ACP, √© comum e pr√°tico denotar por \\(\\mathbf{x}\\) o vetor de vari√°veis j√° centralizado. Utilizamos esse abuso de nota√ß√£o durante esse cap√≠tulo para simplifica√ß√£o do texto sem perda de generalidade.\n\n\n\n\n6.3.2 Por que Escalonar? O Dilema da Covari√¢ncia vs.¬†Correla√ß√£o\nA An√°lise de Componentes Principais (ACP) √© sens√≠vel √† escala das vari√°veis. Se uma vari√°vel tiver uma vari√¢ncia numericamente muito maior que as outras ‚Äî mesmo que apenas por causa da sua unidade de medida ‚Äî ela poder√° dominar os primeiros componentes principais.\nImagine incluir uma terceira vari√°vel no conjunto Altura/Peso: a renda mensal, medida em Reais. As vari√¢ncias poderiam ser aproximadamente:\n\nAltura: 80 cm¬≤\n\nPeso: 60 kg¬≤\n\nRenda: 4.000.000 (RS)¬≤\n\nNesse cen√°rio, a vari√¢ncia da Renda √© milhares de vezes maior que a das outras vari√°veis. Se aplicarmos a ACP diretamente na matriz de covari√¢ncias, o primeiro componente principal ser√° fortemente direcionado pela Renda, mesmo que sua correla√ß√£o com as demais vari√°veis seja baixa. Isso ocorre porque a ACP estar√° apenas ‚Äúseguindo‚Äù a dire√ß√£o da vari√°vel com maior vari√¢ncia ‚Äî n√£o necessariamente a mais informativa.\nPara evitar esse vi√©s, escalonamos as vari√°veis: cada uma √© dividida por seu desvio padr√£o. Isso padroniza todas para vari√¢ncia igual a 1. Ao fazer isso, estamos na pr√°tica realizando a ACP sobre a matriz de correla√ß√£o \\((\\mathbf{R})\\) em vez da matriz de covari√¢ncias \\((\\mathbf{\\Sigma})\\).\nVantagem do escalonamento:\nUsar a matriz de correla√ß√£o ‚Äúdemocratiza‚Äù a an√°lise. Todas as vari√°veis come√ßam com a mesma import√¢ncia inicial (vari√¢ncia 1), e a ACP passa a capturar a estrutura de correla√ß√µes, ao inv√©s de ser enviesada pelas diferen√ßas de escala.\nQuando usar a matriz de covari√¢ncias?\nSomente quando todas as vari√°veis est√£o na mesma unidade de medida e possuem uma interpreta√ß√£o compar√°vel. Por exemplo, comparar a temperatura em Celsius em diferentes regi√µes pode fazer sentido sem escalonamento. Fora isso, a matriz de correla√ß√£o √© geralmente a escolha mais robusta e segura.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "href": "src/02_tec_mult/06_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "title": "6¬† An√°lise de Componentes Principais",
    "section": "6.4 Componentes Principais Populacionais vs.¬†Amostrais",
    "text": "6.4 Componentes Principais Populacionais vs.¬†Amostrais\nAt√© este ponto, discutimos os componentes principais em um contexto populacional, onde a matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\) (ou correla√ß√£o \\(\\mathbf{R}\\)) e seus autovalores \\(\\lambda_k\\) e autovetores \\(\\mathbf{e}_k\\) s√£o conhecidos. Na pr√°tica, quase sempre trabalhamos com uma amostra de dados. Nesse caso, n√£o conhecemos os verdadeiros par√¢metros populacionais e devemos estim√°-los.\nOs componentes principais amostrais s√£o obtidos da mesma maneira, mas usando a matriz de covari√¢ncias amostral \\(\\mathbf{S}\\) (ou a matriz de correla√ß√£o amostral \\(\\mathbf{R}\\)). As quantidades resultantes s√£o estimativas dos seus an√°logos populacionais:\n\nO \\(k\\)-√©simo autovalor amostral, \\(\\hat{\\lambda}_k\\), √© uma estimativa de \\(\\lambda_k\\).\nO \\(k\\)-√©simo autovetor amostral, \\(\\hat{\\mathbf{e}}_k\\), √© uma estimativa de \\(\\mathbf{e}_k\\).\nO \\(k\\)-√©simo componente principal amostral, \\(\\hat{Y}_k = \\hat{\\mathbf{e}}_k' \\mathbf{x}\\), √© uma estimativa de \\(Y_k\\).\n\nA teoria e a interpreta√ß√£o permanecem as mesmas. Para simplificar a nota√ß√£o, ao longo deste cap√≠tulo, omitimos o acento circunflexo (\\(\\hat{\\phantom{a}}\\)), mas √© importante lembrar que, na aplica√ß√£o pr√°tica, estamos sempre lidando com estimativas amostrais.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#escolhendo-o-n√∫mero-de-componentes",
    "href": "src/02_tec_mult/06_acp.html#escolhendo-o-n√∫mero-de-componentes",
    "title": "6¬† An√°lise de Componentes Principais",
    "section": "6.5 Escolhendo o N√∫mero de Componentes",
    "text": "6.5 Escolhendo o N√∫mero de Componentes\nA principal vantagem da ACP √© a redu√ß√£o de dimensionalidade. Mas como decidimos quantos componentes (\\(q &lt; p\\)) reter? A escolha de \\(q\\) envolve um trade-off entre a simplicidade (poucos componentes) e a fidelidade aos dados originais (muitos componentes). N√£o existe uma regra √∫nica, mas sim um conjunto de crit√©rios que devem ser avaliados em conjunto.\n\n6.5.1 Crit√©rio da Vari√¢ncia Explicada Acumulada\nEste √© o crit√©rio mais comum. Calculamos a propor√ß√£o da vari√¢ncia total explicada por cada componente e acumulamos essa propor√ß√£o.\n\\[\n\\text{Propor√ß√£o da Vari√¢ncia por } CP_k = \\frac{\\lambda_k}{\\sum_{j=1}^{p} \\lambda_j}\n\\]\nEm seguida, escolhemos o menor n√∫mero de componentes \\(q\\) cuja vari√¢ncia explicada acumulada atinja um limiar satisfat√≥rio, geralmente entre 70% e 90%. A escolha do limiar depende do contexto da an√°lise.\n\n\n6.5.2 Crit√©rio do Autovalor (Crit√©rio de Kaiser)\nProposto por Henry Kaiser, este crit√©rio sugere reter apenas os componentes cujos autovalores (\\(\\lambda_k\\)) s√£o maiores que 1. A intui√ß√£o por tr√°s dessa regra √© mais clara quando a ACP √© aplicada sobre a matriz de correla√ß√£o. Nesse caso, as vari√°veis originais s√£o padronizadas para ter vari√¢ncia 1. Um componente com autovalor (vari√¢ncia) menor que 1 est√°, portanto, explicando menos variabilidade do que uma √∫nica vari√°vel original. Reter tal componente n√£o traria uma ‚Äúeconomia‚Äù de informa√ß√£o‚Äù, tornando-o um candidato √† exclus√£o.\n\n\n6.5.3 Scree Plot (Gr√°fico de Cotovelo)\nO Scree Plot, proposto por Raymond Cattell, √© uma ferramenta visual que nos ajuda a identificar o n√∫mero ideal de componentes. Ele √© um gr√°fico de linha dos autovalores (vari√¢ncias dos componentes) em ordem decrescente.\nTipicamente, o gr√°fico mostra uma queda acentuada nos primeiros autovalores, seguida por um nivelamento gradual para os autovalores restantes. O ponto onde a curva ‚Äúdobra‚Äù ou forma um ‚Äúcotovelo‚Äù (elbow) √© considerado o ponto de corte. A ideia √© reter os componentes que aparecem antes do cotovelo, pois eles s√£o os que contribuem mais significativamente para a vari√¢ncia total. Os componentes ap√≥s o cotovelo formam o ‚Äúcascalho‚Äù (scree) na base de uma montanha e s√£o considerados ‚Äúru√≠do‚Äù.\n\n\n\n\n\n\n\n\nFigura¬†6.3: Exemplo de um Scree Plot. O ‚Äòcotovelo‚Äô em k=3 sugere a reten√ß√£o de 3 componentes.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#interpretando-os-componentes-principais",
    "href": "src/02_tec_mult/06_acp.html#interpretando-os-componentes-principais",
    "title": "6¬† An√°lise de Componentes Principais",
    "section": "6.6 Interpretando os Componentes Principais",
    "text": "6.6 Interpretando os Componentes Principais\nUma vez que selecionamos o n√∫mero de componentes a reter, o passo final √© a interpreta√ß√£o. O que esses novos eixos, que s√£o combina√ß√µes de nossas vari√°veis originais, realmente significam?\nOs coeficientes \\(e_{kj}\\) do autovetor \\(\\mathbf{e}_k\\) s√£o chamados de cargas (loadings) e representam o peso da vari√°vel original \\(X_j\\) na forma√ß√£o do componente \\(Y_k\\). Embora as cargas sejam importantes, a sua interpreta√ß√£o pode ser complicada, pois sua magnitude depende das unidades das vari√°veis originais.\nUma medida mais interpret√°vel √© a correla√ß√£o entre os componentes principais e as vari√°veis originais, \\(Cor(Y_k, X_j)\\). Ela nos diz o qu√£o ‚Äúalinhado‚Äù um componente est√° com cada vari√°vel original, numa escala padronizada de -1 a 1. A f√≥rmula para essa correla√ß√£o √©:\n\\[\nCor(Y_k, X_j) = \\frac{e_{kj} \\sqrt{\\lambda_k}}{\\sqrt{s_{jj}}}\n\\]\nOnde:\n\n\\(e_{kj}\\) √© a carga da vari√°vel \\(j\\) no componente \\(k\\).\n\\(\\lambda_k\\) √© o autovalor (vari√¢ncia) do componente \\(k\\).\n\\(s_{jj}\\) √© a vari√¢ncia da vari√°vel original \\(j\\).\n\nQuando a ACP √© realizada sobre a matriz de correla√ß√£o (ou seja, com dados padronizados), as vari√¢ncias \\(s_{jj}\\) s√£o todas iguais a 1. Nesse caso, a f√≥rmula simplifica para \\(Cor(Y_k, X_j) = e_{kj} \\sqrt{\\lambda_k}\\). As correla√ß√µes se tornam proporcionais √†s cargas, facilitando a interpreta√ß√£o.\nAl√©m disso, quando a ACP √© realizada sobre a matriz de correla√ß√µes, as vari√°veis s√£o padronizadas. Nesse caso, uma op√ß√£o comum e direta √© avaliar os pr√≥prios loadings (os autovetores da matriz de correla√ß√£o) para entender a contribui√ß√£o de cada vari√°vel. Um loading alto (pr√≥ximo de 1 ou -1) indica que a vari√°vel tem uma forte influ√™ncia na constru√ß√£o daquele componente.\nA etapa mais crucial da ACP √© transformar os eixos matem√°ticos (os componentes) em descobertas pr√°ticos. A ferramenta visual mais adequada para essa tarefa √© o biplot. O termo ‚Äúbiplot‚Äù significa ‚Äúdois plots‚Äù (plot duplo), pois ele sobrep√µe duas informa√ß√µes em um √∫nico gr√°fico:\n\nOs scores: As coordenadas das observa√ß√µes no novo espa√ßo dos componentes principais.\nOs loadings: As contribui√ß√µes das vari√°veis originais para a cria√ß√£o desses componentes.\n\nO resultado √© um mapa rico que mostra n√£o apenas como as observa√ß√µes se agrupam, mas por que elas se agrupam daquela maneira. A interpreta√ß√£o de um biplot segue uma l√≥gica visual. Vamos quebrar em partes:\n\nEixos (Componentes Principais): O eixo horizontal √© o CP1 e o vertical √© o CP2. Eles s√£o as ‚Äúr√©guas‚Äù do nosso novo mapa e representam as dire√ß√µes de maior variabilidade nos dados. A porcentagem de vari√¢ncia que cada um explica √© mostrada nos seus r√≥tulos.\nPontos (Observa√ß√µes): Cada ponto no gr√°fico √© uma observa√ß√£o.\n\nProximidade: Pontos pr√≥ximos uns dos outros representam observa√ß√µes com perfis semelhantes (conforme capturado pelos dois primeiros CPs).\nAgrupamentos: Grupos de pontos (clusters) indicam subpopula√ß√µes nos dados.\n\nVetores (Vari√°veis Originais): Cada seta (vetor) representa uma das vari√°veis originais.\n\nDire√ß√£o: A dire√ß√£o da seta indica como a vari√°vel contribui para os dois componentes. Uma seta que aponta para a direita indica uma forte contribui√ß√£o positiva para o CP1. Uma que aponta para cima, uma forte contribui√ß√£o positiva para o CP2.\nComprimento: O comprimento da seta √© proporcional a qu√£o bem a vari√°vel √© representada no espa√ßo 2D do biplot. Setas mais longas significam que a vari√°vel tem uma forte influ√™ncia nos componentes mostrados e √© bem representada no gr√°fico. Setas curtas s√£o menos importantes para os dois primeiros CPs ou sua variabilidade est√° melhor explicada em outros componentes (CP3, CP4, etc.).\nRela√ß√µes entre Vari√°veis: O √¢ngulo entre os vetores nos informa sobre a correla√ß√£o entre as vari√°veis originais.\n\n√Çngulo pequeno (&lt; 90¬∞): As vari√°veis s√£o positivamente correlacionadas.\n√Çngulo de ~90¬∞: As vari√°veis n√£o s√£o correlacionadas.\n√Çngulo obtuso (&gt; 90¬∞): As vari√°veis s√£o negativamente correlacionadas.\n\n\nRela√ß√£o entre Pontos e Vetores: Para entender o perfil de um ponto (ou grupo de pontos), projete-o ortogonalmente sobre os vetores das vari√°veis. Se a proje√ß√£o de um ponto cai na dire√ß√£o de um vetor, aquela observa√ß√£o tem um valor alto para aquela vari√°vel. Se cai na dire√ß√£o oposta, tem um valor baixo.\n\nCom essas regras em mente, vamos analisar um biplot gen√©rico.\n\n\n\n\n\n\n\n\nFigura¬†6.4: Exemplo de um biplot gen√©rico para ilustrar a interpreta√ß√£o dos seus elementos. Os pontos representam as observa√ß√µes e as setas, as vari√°veis originais.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html",
    "href": "src/02_tec_mult/07_af.html",
    "title": "7¬† An√°lise Fatorial",
    "section": "",
    "text": "7.1 O Modelo Fatorial Ortogonal\nA An√°lise Fatorial √© uma t√©cnica estat√≠stica utilizada para descrever a estrutura de covari√¢ncia entre um conjunto de vari√°veis observadas. A hip√≥tese central √© que essa estrutura √© gerada por um n√∫mero menor de vari√°veis latentes n√£o observ√°veis, denominadas fatores comuns.\nCome√ßamos com uma intui√ß√£o. Suponha que temos as seguintes vari√°veis de gastos para diferentes fam√≠lias:\n√â razo√°vel supor que essas vari√°veis sejam correlacionadas. Mais do que isso, pode existir um fator latente, como a renda familiar (\\(F_1\\)), que influencia todos esses gastos. A An√°lise Fatorial busca formalizar e quantificar essa rela√ß√£o.\nO modelo sup√µe que cada vari√°vel observada √© linearmente dependente de um conjunto de fatores comuns, somado a um termo de vari√¢ncia individual, ou espec√≠fico.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#o-modelo-fatorial-ortogonal",
    "href": "src/02_tec_mult/07_af.html#o-modelo-fatorial-ortogonal",
    "title": "7¬† An√°lise Fatorial",
    "section": "",
    "text": "Defini√ß√£o 7.1 Seja \\(\\mathbf{x}\\) um vetor aleat√≥rio de p vari√°veis observadas com vetor de m√©dias \\(\\boldsymbol{\\mu}\\) e matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\). O modelo fatorial com m fatores comuns (\\(m &lt; p\\)) postula que \\(\\mathbf{x}\\) √© linearmente dependente de m fatores comuns \\(F_1, F_2, \\dots, F_m\\) e p termos de erro \\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p\\). Em nota√ß√£o matricial, o modelo √©:\n\\[\n\\mathbf{x}_{(p \\times 1)} - \\boldsymbol{\\mu}_{(p \\times 1)} = \\mathbf{L}_{(p \\times m)}\\mathbf{F}_{(m \\times 1)} + \\boldsymbol{\\epsilon}_{(p \\times 1)}\n\\tag{7.1}\\]\nOnde:\n\n\\(\\mathbf{L}\\) √© a matriz de cargas fatoriais: Uma matriz de pesos respons√°vel por quantificar as rela√ß√µes entre as p vari√°veis e os m fatores.\n\\(\\mathbf{F}\\) √© o vetor de fatores comuns, ou seja \\(\\mathbf{F} = [F_1, F_2, \\dots, F_m]'\\).\n\\(\\boldsymbol{\\epsilon}\\) √© o vetor de erros, ou vari√¢ncias espec√≠ficas, ou seja, \\(\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p]'\\)\n\nPara que o ajuste desse modelo seja fact√≠vel, as seguintes suposi√ß√µes s√£o feitas para o modelo ortogonal:\n\n\\(E[\\mathbf{F}] = \\mathbf{0}\\) e \\(Cov(\\mathbf{F}) = E[\\mathbf{F}\\mathbf{F}'] = \\mathbf{I}\\).\n\\(E[\\boldsymbol{\\epsilon}] = \\mathbf{0}\\) e \\(Cov(\\boldsymbol{\\epsilon}) = E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] = \\mathbf{\\Psi}\\), onde \\(\\mathbf{\\Psi}\\) √© uma matriz diagonal.\n\\(Cov(\\mathbf{F}, \\boldsymbol{\\epsilon}) = E[\\mathbf{F}\\boldsymbol{\\epsilon}'] = \\mathbf{0}\\).",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#a-estrutura-de-covari√¢ncia-impl√≠cita",
    "href": "src/02_tec_mult/07_af.html#a-estrutura-de-covari√¢ncia-impl√≠cita",
    "title": "7¬† An√°lise Fatorial",
    "section": "7.2 A Estrutura de Covari√¢ncia Impl√≠cita",
    "text": "7.2 A Estrutura de Covari√¢ncia Impl√≠cita\nAs suposi√ß√µes do modelo implicam uma estrutura espec√≠fica para a matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\).\n\nTeorema 7.1 Sob as premissas do modelo fatorial ortogonal (Defini√ß√£o¬†7.1), a matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\) do vetor \\(\\mathbf{x}\\) √© dada por:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\n\\tag{7.2}\\]\n\n\nComprova√ß√£o. A partir do modelo fatorial em Equa√ß√£o¬†7.1, temos que \\(\\mathbf{x} - \\boldsymbol{\\mu} = \\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon}\\). A matriz de covari√¢ncias de \\(\\mathbf{x}\\) √©, por defini√ß√£o, \\(\\mathbf{\\Sigma} = E[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})']\\). Substituindo a express√£o do modelo, obtemos:\n\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= E[(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})'] \\\\\n&= E[(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})(\\mathbf{F}'\\mathbf{L}' + \\boldsymbol{\\epsilon}')] \\\\\n&= E[\\mathbf{L}\\mathbf{F}\\mathbf{F}'\\mathbf{L}' + \\mathbf{L}\\mathbf{F}\\boldsymbol{\\epsilon}' + \\boldsymbol{\\epsilon}\\mathbf{F}'\\mathbf{L}' + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] \\\\\n&= \\mathbf{L}E[\\mathbf{F}\\mathbf{F}']\\mathbf{L}' + \\mathbf{L}E[\\mathbf{F}\\boldsymbol{\\epsilon}'] + E[\\boldsymbol{\\epsilon}\\mathbf{F}']\\mathbf{L}' + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}']\n\\end{aligned}\n\\]\nPelas suposi√ß√µes do modelo ortogonal (Defini√ß√£o¬†7.1):\n\n\\(E[\\mathbf{F}\\mathbf{F}'] = \\text{Cov}(\\mathbf{F}) = \\mathbf{I}\\) (os fatores s√£o n√£o correlacionados e t√™m vari√¢ncia unit√°ria).\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] = \\text{Cov}(\\boldsymbol{\\epsilon}) = \\mathbf{\\Psi}\\) (os erros s√£o n√£o correlacionados entre si).\n\\(E[\\mathbf{F}\\boldsymbol{\\epsilon}'] = \\text{Cov}(\\mathbf{F}, \\boldsymbol{\\epsilon}) = \\mathbf{0}\\) (os fatores e os erros s√£o n√£o correlacionados).\n\nSubstituindo essas esperan√ßas na equa√ß√£o de \\(\\mathbf{\\Sigma}\\), temos:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{I}\\mathbf{L}' + \\mathbf{L}\\mathbf{0} + \\mathbf{0}\\mathbf{L}' + \\mathbf{\\Psi} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\n\\]\nIsso completa a prova.\n\nEsta equa√ß√£o decomp√µe a vari√¢ncia de cada vari√°vel \\(X_i\\) em:\n\nComunalidade (\\(h_i^2\\)): A por√ß√£o da vari√¢ncia de \\(X_i\\) explicada pelos m fatores comuns (\\(h_i^2 = \\sum_{j=1}^{m} l_{ij}^2\\)).\nVari√¢ncia Espec√≠fica (\\(\\psi_i\\)): A por√ß√£o da vari√¢ncia de \\(X_i\\) n√£o explicada pelos fatores comuns (\\(Var(X_i) = \\sigma_{ii} = h_i^2 + \\psi_i\\)).\n\n\nExemplo 7.1 Suponha que a matriz de covari√¢ncias de um vetor aleat√≥rio \\(\\mathbf{x}\\) com \\(p=4\\) vari√°veis seja:\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n19 & 30 & 2 & 12 \\\\\n30 & 57 & 5 & 23 \\\\\n2 & 5 & 37 & 47 \\\\\n12 & 23 & 47 & 68\n\\end{pmatrix}\n\\]\n√â poss√≠vel mostrar que um modelo fatorial com \\(m=2\\) fatores comuns pode gerar essa estrutura de covari√¢ncia. Uma solu√ß√£o poss√≠vel para \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) √© dada por:\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n4 & 1 \\\\\n7 & 2 \\\\\n-1 & 6 \\\\\n1 & 8\n\\end{pmatrix}\n,\n\\quad\n\\mathbf{\\Psi} =\n\\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n0 & 4 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{pmatrix}\n\\]\nO leitor pode verificar que \\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\). O modelo decomp√µe a vari√¢ncia de cada vari√°vel.\n\nPara \\(X_1\\), a comunalidade √© \\(h_1^2 = 4^2 + 1^2 = 17\\), e sua vari√¢ncia total √© \\(Var(X_1) = \\sigma_{11} = h_1^2 + \\psi_1 = 17 + 2 = 19\\).\nPara \\(X_2\\), a comunalidade √© \\(h_2^2 = 7^2 + 2^2 = 53\\), e sua vari√¢ncia total √© \\(Var(X_2) = \\sigma_{22} = h_2^2 + \\psi_2 = 53 + 4 = 57\\).",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#problemas-no-modelo-fatorial",
    "href": "src/02_tec_mult/07_af.html#problemas-no-modelo-fatorial",
    "title": "7¬† An√°lise Fatorial",
    "section": "7.3 Problemas no Modelo Fatorial",
    "text": "7.3 Problemas no Modelo Fatorial\n\nExist√™ncia da Solu√ß√£o: Nem sempre existe uma solu√ß√£o fact√≠vel para o modelo fatorial com m fatores. A estima√ß√£o dos par√¢metros, especialmente com um n√∫mero inadequado de fatores, pode levar a solu√ß√µes impr√≥prias, como uma vari√¢ncia espec√≠fica negativa (\\(\\hat{\\psi}_i &lt; 0\\)), conhecida como caso de Heywood. Isso viola a premissa de que \\(\\psi_i\\) √© uma vari√¢ncia e, portanto, deve ser n√£o-negativa. Geralmente, uma solu√ß√£o impr√≥pria indica que o modelo √© inadequado para os dados.\n\n\nExemplo 7.2 Considere um modelo de um fator (\\(m=1\\)) para \\(p=3\\) vari√°veis, com a seguinte matriz de correla√ß√£o populacional:\n\\[\n\\mathbf{R} =\n\\begin{pmatrix}\n1.0 & 0.4 & 0.9 \\\\\n0.4 & 1.0 & 0.7 \\\\\n0.9 & 0.7 & 1.0\n\\end{pmatrix}\n\\]\nO modelo fatorial para a matriz de correla√ß√£o √© \\(\\mathbf{P} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\). Para \\(m=1\\), as cargas s√£o um vetor \\(\\mathbf{L} = [l_{11}, l_{21}, l_{31}]'\\). As covari√¢ncias (correla√ß√µes) s√£o dadas por \\(\\rho_{ij} = l_{i1}l_{j1}\\). Temos o sistema:\n\n\\(\\rho_{12} = l_{11}l_{21} = 0.4\\)\n\\(\\rho_{13} = l_{11}l_{31} = 0.9\\)\n\\(\\rho_{23} = l_{21}l_{31} = 0.7\\)\n\nMultiplicando as tr√™s equa√ß√µes, obtemos \\((l_{11}l_{21}l_{31})^2 = 0.4 \\times 0.9 \\times 0.7 = 0.252\\). Isso nos permite resolver para as cargas:\n\n\\(l_{11}^2 = (l_{11}l_{21})(l_{11}l_{31}) / (l_{21}l_{31}) = (0.4 \\times 0.9) / 0.7 \\approx 0.514\\)\n\\(l_{21}^2 = (l_{11}l_{21})(l_{21}l_{31}) / (l_{11}l_{31}) = (0.4 \\times 0.7) / 0.9 \\approx 0.311\\)\n\\(l_{31}^2 = (l_{11}l_{31})(l_{21}l_{31}) / (l_{11}l_{21}) = (0.9 \\times 0.7) / 0.4 = 1.575\\)\n\nA comunalidade da terceira vari√°vel √© \\(h_3^2 = l_{31}^2 = 1.575\\). Como estamos modelando uma matriz de correla√ß√£o, a vari√¢ncia total de cada vari√°vel √© 1. A vari√¢ncia espec√≠fica seria \\(\\psi_3 = 1 - h_3^2 = 1 - 1.575 = -0.575\\). Uma vari√¢ncia negativa √© imposs√≠vel, indicando que o modelo de um fator n√£o √© apropriado para descrever a estrutura de correla√ß√£o dada.\n\n\nIndetermina√ß√£o da Solu√ß√£o (Rota√ß√£o Fatorial): A solu√ß√£o para a matriz de cargas \\(\\mathbf{L}\\) n√£o √© √∫nica. Para qualquer matriz ortogonal \\(\\mathbf{T}\\) de dimens√£o \\(m \\times m\\) (ou seja, uma matriz tal que \\(\\mathbf{T}\\mathbf{T}' = \\mathbf{T}'\\mathbf{T} = \\mathbf{I}\\)), podemos definir uma nova matriz de cargas \\(\\mathbf{L}^* = \\mathbf{L}\\mathbf{T}\\) que resulta na mesma matriz de covari√¢ncias.\n\nIsso ocorre porque a parte da covari√¢ncia explicada pelos fatores, \\(\\mathbf{L}\\mathbf{L}'\\), permanece inalterada:\n\\[\n\\mathbf{L}^*(\\mathbf{L}^*)' = (\\mathbf{L}\\mathbf{T})(\\mathbf{L}\\mathbf{T})' = \\mathbf{L}\\mathbf{T}\\mathbf{T}'\\mathbf{L}' = \\mathbf{L}(\\mathbf{T}\\mathbf{T}')\\mathbf{L}' = \\mathbf{L}\\mathbf{I}\\mathbf{L}' = \\mathbf{L}\\mathbf{L}'\n\\]\nPortanto, o modelo \\(\\mathbf{\\Sigma} = \\mathbf{L}^*(\\mathbf{L}^*)' + \\mathbf{\\Psi}\\) √© equivalente ao modelo original. Essa propriedade √© a base para a rota√ß√£o fatorial, um procedimento que busca a solu√ß√£o \\(\\mathbf{L}^*\\) mais simples e interpret√°vel, sem alterar o ajuste do modelo.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#adequabilidade-do-modelo-fatorial",
    "href": "src/02_tec_mult/07_af.html#adequabilidade-do-modelo-fatorial",
    "title": "7¬† An√°lise Fatorial",
    "section": "7.4 Adequabilidade do Modelo Fatorial",
    "text": "7.4 Adequabilidade do Modelo Fatorial\nAntes de aplicar os m√©todos de estima√ß√£o, pode-se avaliar se os dados s√£o adequados para a An√°lise Fatorial. A lema fundamental da AF √© que as vari√°veis observadas s√£o correlacionadas e que essa correla√ß√£o pode ser explicada por fatores latentes. Se as vari√°veis s√£o ortogonais ou se a correla√ß√£o entre elas √© esp√∫ria, o modelo fatorial n√£o √© apropriado.\nDois dos principais diagn√≥sticos para verificar a adequabilidade dos dados s√£o o Teste de Esfericidade de Bartlett e a medida de adequa√ß√£o da amostra de Kaiser-Meyer-Olkin (KMO).\n\n7.4.1 Teste de Esfericidade de Bartlett\nO Teste de Esfericidade de Bartlett avalia a hip√≥tese nula (\\(H_0\\)) de que a matriz de correla√ß√£o populacional \\(\\mathbf{P}\\) √© uma matriz identidade (\\(H_0: \\mathbf{P} = \\mathbf{I}\\)). Se essa hip√≥tese for verdadeira, as vari√°veis s√£o n√£o correlacionadas, e n√£o h√° estrutura latente para ser extra√≠da.\nA estat√≠stica de teste √© baseada no determinante da matriz de correla√ß√£o amostral \\(\\mathbf{R}\\) e, sob \\(H_0\\), segue aproximadamente uma distribui√ß√£o Qui-quadrado. Para uma amostra de tamanho n e p vari√°veis, a estat√≠stica √©:\n\\[\n\\chi^2 = -\\left[(n - 1) - \\frac{2p + 5}{6}\\right] \\ln(|\\mathbf{R}|)\n\\]\nEsta estat√≠stica tem, aproximadamente, uma distribui√ß√£o \\(\\chi^2\\) com \\(p(p-1)/2\\) graus de liberdade. Um p-valor baixo (e.g., &lt; 0.05) leva √† rejei√ß√£o de \\(H_0\\), indicando que existe correla√ß√£o suficiente entre as vari√°veis para justificar a aplica√ß√£o da An√°lise Fatorial.\n\n\n7.4.2 Medida de Adequa√ß√£o da Amostra (KMO)\nEnquanto o teste de Bartlett avalia se a matriz de correla√ß√£o como um todo se desvia significativamente da identidade, a medida de Kaiser-Meyer-Olkin (KMO) quantifica o qu√£o adequados os dados s√£o para a fatoriza√ß√£o. O KMO compara a magnitude dos coeficientes de correla√ß√£o observados com a magnitude dos coeficientes de correla√ß√£o parcial.\nA l√≥gica √© que, se as vari√°veis compartilham fatores comuns, as correla√ß√µes parciais entre pares de vari√°veis (controlando pelas outras vari√°veis) devem ser pequenas. A estat√≠stica KMO √© calculada como:\n\\[\n\\text{KMO} = \\frac{\\sum_{i \\neq j} r_{ij}^2}{\\sum_{i \\neq j} r_{ij}^2 + \\sum_{i \\neq j} a_{ij}^2}\n\\]\nOnde \\(r_{ij}\\) √© o coeficiente de correla√ß√£o simples entre as vari√°veis \\(X_i\\) e \\(X_j\\), e \\(a_{ij}\\) √© o coeficiente de correla√ß√£o parcial.\nO valor do KMO varia de 0 a 1. Valores mais altos indicam que a An√°lise Fatorial √© mais apropriada. Uma regra pr√°tica para a interpreta√ß√£o do KMO √©:\n\n&gt; 0.9: Maravilhoso\n0.8 - 0.9: Merit√≥rio\n0.7 - 0.8: Razo√°vel\n0.6 - 0.7: Med√≠ocre\n0.5 - 0.6: Ruim\n&lt; 0.5: Inaceit√°vel\n\nValores abaixo de 0.5 sugerem que a An√°lise Fatorial pode n√£o ser uma boa ideia.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#m√©todos-de-estima√ß√£o",
    "href": "src/02_tec_mult/07_af.html#m√©todos-de-estima√ß√£o",
    "title": "7¬† An√°lise Fatorial",
    "section": "7.5 M√©todos de Estima√ß√£o",
    "text": "7.5 M√©todos de Estima√ß√£o\nAssumindo uma amostra aleat√≥ria \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\) de uma popula√ß√£o com matriz de covari√¢ncias \\(\\mathbf{\\Sigma}\\), o desafio √© estimar \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) usando a matriz de covari√¢ncias amostral \\(\\mathbf{S}\\) ou a matriz de correla√ß√£o amostral \\(\\mathbf{R}\\).\nExistem diversos m√©todos para estimar os par√¢metros do modelo fatorial, cada um com suas pr√≥prias premissas e propriedades. Alguns dos mais conhecidos incluem:\n\nM√©todo de Componentes Principais (MCP)\nM√©todo da M√°xima Verossimilhan√ßa (MMV)\nM√©todo dos Fatores Principais (Principal Axis Factoring)\nM√≠nimos Quadrados Ponderados\nM√≠nimos Quadrados Generalizados\n\nNeste cap√≠tulo, focaremos nos dois m√©todos mais amplamente utilizados na pr√°tica: o M√©todo de Componentes Principais, por sua simplicidade computacional, e o M√©todo da M√°xima Verossimilhan√ßa, por sua fundamenta√ß√£o estat√≠stica robusta.\n\n7.5.1 A Solu√ß√£o por Componentes Principais\nO m√©todo de componentes principais (MCP) prov√™ uma solu√ß√£o para \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) a partir da decomposi√ß√£o espectral da matriz de covari√¢ncias amostral \\(\\mathbf{S}\\) (ou da matriz de correla√ß√µes \\(\\mathbf{R}\\)).\nA ideia √© que a matriz \\(\\mathbf{S}\\) pode ser decomposta em termos de seus pares de autovalor-autovetor \\((\\hat{\\lambda}_i, \\hat{\\mathbf{e}}_i)\\):\n\\[\n\\mathbf{S} = \\hat{\\lambda}_1\\hat{\\mathbf{e}}_1\\hat{\\mathbf{e}}_1' + \\hat{\\lambda}_2\\hat{\\mathbf{e}}_2\\hat{\\mathbf{e}}_2' + \\dots + \\hat{\\lambda}_p\\hat{\\mathbf{e}}_p\\hat{\\mathbf{e}}_p'\n\\]\nA estrutura do modelo fatorial √© \\(\\mathbf{S} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\). O MCP busca uma aproxima√ß√£o para \\(\\mathbf{S}\\) retendo apenas os m primeiros componentes, que explicam a maior parte da variabilidade total. A matriz \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\) √© constru√≠da para igualar a contribui√ß√£o desses componentes:\n\\[\n\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' = \\hat{\\lambda}_1\\hat{\\mathbf{e}}_1\\hat{\\mathbf{e}}_1' + \\hat{\\lambda}_2\\hat{\\mathbf{e}}_2\\hat{\\mathbf{e}}_2' + \\dots + \\hat{\\lambda}_m\\hat{\\mathbf{e}}_m\\hat{\\mathbf{e}}_m'\n\\]\nUma solu√ß√£o expl√≠cita para \\(\\hat{\\mathbf{L}}\\) que satisfaz essa equa√ß√£o √© uma matriz \\(p \\times m\\) cujas colunas s√£o os autovetores reescalados pelos respectivos autovalores. A matriz \\(\\hat{\\mathbf{\\Psi}}\\) √© ent√£o definida para garantir que as vari√¢ncias do modelo (\\(\\text{diag}(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}})\\)) sejam iguais √†s vari√¢ncias amostrais (\\(\\text{diag}(\\mathbf{S})\\)).\nIsso nos leva √† seguinte defini√ß√£o formal.\n\nDefini√ß√£o 7.2 Seja \\(\\mathbf{S}\\) a matriz de covari√¢ncia amostral com pares de autovalor-autovetor \\((\\hat{\\lambda}_i, \\hat{\\mathbf{e}}_i)\\). A solu√ß√£o de componentes principais com m fatores √© definida por:\n\nMatriz de Cargas Estimada (\\(\\hat{\\mathbf{L}}\\)): \\[\n\\hat{\\mathbf{L}} = [\\sqrt{\\hat{\\lambda}_1}\\hat{\\mathbf{e}}_1 | \\sqrt{\\hat{\\lambda}_2}\\hat{\\mathbf{e}}_2 | \\dots | \\sqrt{\\hat{\\lambda}_m}\\hat{\\mathbf{e}}_m]\n\\]\nMatriz de Vari√¢ncias Espec√≠ficas Estimada (\\(\\hat{\\mathbf{\\Psi}}\\)): \\[\n\\hat{\\mathbf{\\Psi}} = \\text{diag}(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}')\n\\]\n\nonde \\(\\hat{\\psi}_i = s_{ii} - \\sum_{j=1}^{m} \\hat{l}_{ij}^2\\).\nSe a matriz de correla√ß√µes \\(\\mathbf{R}\\) for utilizada, as cargas \\(\\hat{\\mathbf{L}}\\) s√£o calculadas a partir dos autovalores e autovetores de \\(\\mathbf{R}\\), e as vari√¢ncias espec√≠ficas s√£o \\(\\hat{\\psi}_i = 1 - \\sum_{j=1}^{m} \\hat{l}_{ij}^2\\).\n\nPor constru√ß√£o, este m√©todo for√ßa a diagonal da matriz de covari√¢ncias do modelo, \\(\\hat{\\mathbf{\\Sigma}} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\), a ser id√™ntica √† diagonal de \\(\\mathbf{S}\\). O ajuste do modelo √© ent√£o avaliado pela magnitude dos res√≠duos fora da diagonal. A matriz de res√≠duos √©:\n\\[\n\\mathbf{S} - \\hat{\\mathbf{\\Sigma}} = \\mathbf{S} - (\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}})\n\\]\nComo \\(\\hat{\\mathbf{\\Psi}} = \\text{diag}(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}')\\), os elementos da diagonal da matriz de res√≠duos s√£o zero. Os res√≠duos fora da diagonal s√£o os elementos de \\(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\). Pode-se demonstrar que a soma dos quadrados de todos os elementos da matriz \\(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\) (incluindo a diagonal) √©:\n\\[\n\\sum_{i=1}^p \\sum_{j=1}^p (s_{ij} - \\sum_{k=1}^m \\hat{l}_{ik}\\hat{l}_{jk})^2 = \\sum_{k=m+1}^p \\hat{\\lambda}_k^2\n\\]\nIsso mostra que, para que o ajuste seja bom, a soma dos autovalores descartados (\\(\\hat{\\lambda}_{m+1}, \\dots, \\hat{\\lambda}_p\\)) deve ser pequena.\n\n\n7.5.2 M√©todo da M√°xima Verossimilhan√ßa (MMV)\nO m√©todo da m√°xima verossimilhan√ßa (MMV) √© uma abordagem mais rigorosa para a estima√ß√£o, baseada em suposi√ß√µes sobre a distribui√ß√£o dos dados.\nSuposi√ß√µes Adicionais:\n\nO vetor de fatores comuns \\(\\mathbf{F}\\) e o vetor de erros \\(\\boldsymbol{\\epsilon}\\) seguem uma distribui√ß√£o normal multivariada:\n\n\\(\\mathbf{F} \\sim N_m(\\mathbf{0}, \\mathbf{I})\\)\n\\(\\boldsymbol{\\epsilon} \\sim N_p(\\mathbf{0}, \\mathbf{\\Psi})\\)\n\n\\(\\mathbf{F}\\) e \\(\\boldsymbol{\\epsilon}\\) s√£o independentes.\n\nSob essas condi√ß√µes, o vetor de vari√°veis observ√°veis \\(\\mathbf{x}\\) segue uma distribui√ß√£o normal multivariada \\(N_p(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})\\), onde \\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\).\nDada uma amostra aleat√≥ria \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\), a fun√ß√£o de log-verossimilhan√ßa (ignorando constantes) para os par√¢metros \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) √©:\n\\[\n\\log L(\\mathbf{L}, \\mathbf{\\Psi}) = -\\frac{n}{2} \\ln |\\mathbf{\\Sigma}| - \\frac{n}{2} \\text{tr}(\\mathbf{\\Sigma}^{-1}\\mathbf{S})\n\\]\nonde \\(\\mathbf{S}\\) √© a matriz de covari√¢ncias amostral (vers√£o ML, com divisor n). O objetivo √© encontrar as estimativas \\(\\hat{\\mathbf{L}}\\) e \\(\\hat{\\mathbf{\\Psi}}\\) que maximizam essa fun√ß√£o, sujeito √† restri√ß√£o de que \\(\\hat{\\mathbf{L}}'\\hat{\\mathbf{\\Psi}}^{-1}\\hat{\\mathbf{L}}\\) seja uma matriz diagonal para garantir a unicidade da solu√ß√£o.\nA maximiza√ß√£o √© realizada por meio de algoritmos num√©ricos (como o de Newton-Raphson), pois n√£o h√° uma solu√ß√£o anal√≠tica fechada. As estimativas resultantes, \\(\\hat{\\mathbf{L}}\\) e \\(\\hat{\\mathbf{\\Psi}}\\), satisfazem um conjunto complexo de equa√ß√µes.\nA principal vantagem do MMV √© que ele permite um teste de hip√≥teses para a adequa√ß√£o do n√∫mero de fatores m, comparando a matriz de covari√¢ncias do modelo, \\(\\hat{\\mathbf{\\Sigma}} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\), com a matriz amostral \\(\\mathbf{S}\\). Isso √© fundamental na An√°lise Fatorial Confirmat√≥ria (AFC)",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#a-escolha-do-n√∫mero-de-fatores-m",
    "href": "src/02_tec_mult/07_af.html#a-escolha-do-n√∫mero-de-fatores-m",
    "title": "7¬† An√°lise Fatorial",
    "section": "7.6 A Escolha do N√∫mero de Fatores (m)",
    "text": "7.6 A Escolha do N√∫mero de Fatores (m)\nA determina√ß√£o do n√∫mero de fatores, m, √© uma das decis√µes mais importantes na An√°lise Fatorial. Um n√∫mero muito baixo de fatores pode n√£o capturar a estrutura de covari√¢ncia subjacente, enquanto um n√∫mero muito alto pode levar a um modelo superajustado e de dif√≠cil interpreta√ß√£o, violando o princ√≠pio da parcim√¥nia.\nA escolha de m geralmente envolve uma combina√ß√£o de crit√©rios estat√≠sticos e julgamento pr√°tico. V√°rios dos m√©todos utilizados s√£o an√°logos aos empregados na An√°lise de Componentes Principais (Cap√≠tulo 6). Os mais comuns s√£o:\n\nPropor√ß√£o da Vari√¢ncia Total Explicada: Um crit√©rio comum √© reter fatores suficientes para explicar uma propor√ß√£o substancial (e.g., 70-90%) da vari√¢ncia total. No contexto do m√©todo de componentes principais para AF, a propor√ß√£o da vari√¢ncia explicada pelo fator j √© \\(\\hat{\\lambda}_j / \\text{tr}(\\mathbf{S})\\).\nCrit√©rio de Kaiser (Autovalores &gt; 1): Ao trabalhar com a matriz de correla√ß√£o \\(\\mathbf{R}\\), o crit√©rio de Kaiser sugere reter apenas os fatores correspondentes a autovalores maiores que 1. A l√≥gica √© que um fator deve explicar pelo menos a vari√¢ncia de uma vari√°vel original.\nGr√°fico de cotovelo (Scree Plot): Este √© um gr√°fico dos autovalores ordenados (\\(\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots\\)). Procura-se por um ‚Äúcotovelo‚Äù no gr√°fico, um ponto onde a magnitude dos autovalores come√ßa a diminuir drasticamente. O n√∫mero de fatores a reter seria o n√∫mero de pontos antes do in√≠cio do plat√¥.\nTeste de Hip√≥teses (para MMV): Quando o m√©todo da m√°xima verossimilhan√ßa √© utilizado, √© poss√≠vel realizar um teste de raz√£o de verossimilhan√ßas para testar a hip√≥tese nula de que m fatores s√£o suficientes para descrever a estrutura de covari√¢ncia.\n\nNa pr√°tica, √© recomend√°vel utilizar uma combina√ß√£o desses crit√©rios. A interpretabilidade da solu√ß√£o fatorial resultante √©, em √∫ltima an√°lise, o guia mais importante.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#rota√ß√£o-fatorial",
    "href": "src/02_tec_mult/07_af.html#rota√ß√£o-fatorial",
    "title": "7¬† An√°lise Fatorial",
    "section": "7.7 Rota√ß√£o Fatorial",
    "text": "7.7 Rota√ß√£o Fatorial\nComo visto anteriormente, a solu√ß√£o para a matriz de cargas fatoriais \\(\\hat{\\mathbf{L}}\\) n√£o √© √∫nica. Qualquer rota√ß√£o ortogonal dos fatores resulta em uma nova matriz de cargas \\(\\hat{\\mathbf{L}}^* = \\hat{\\mathbf{L}}\\mathbf{T}\\) que explica a estrutura de covari√¢ncias dos dados exatamente da mesma forma, pois \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' = \\hat{\\mathbf{L}}^* (\\hat{\\mathbf{L}}^*)'\\).\nEssa indetermina√ß√£o, que a princ√≠pio parece um problema, √© na verdade uma das ferramentas mais poderosas da An√°lise Fatorial. Ela nos permite girar a estrutura fatorial para uma posi√ß√£o que seja mais simples e interpret√°vel, sem sacrificar o ajuste do modelo. O objetivo √© alcan√ßar o que o psic√≥logo Louis Thurstone chamou de estrutura simples.\nA estrutura simples ideal teria as seguintes propriedades:\n\nCada vari√°vel deve ter pelo menos uma carga fatorial pr√≥xima de zero.\nCada fator deve ter v√°rias cargas pr√≥ximas de zero e algumas cargas altas.\nPara cada par de fatores, deve haver vari√°veis com cargas altas em um fator, mas n√£o no outro.\n\nEm suma, busca-se uma matriz de cargas onde cada vari√°vel esteja fortemente associada a apenas um ou poucos fatores, e cada fator represente claramente um subconjunto de vari√°veis. Os m√©todos de rota√ß√£o s√£o algoritmos que buscam, de forma objetiva, uma matriz \\(\\mathbf{T}\\) que aproxime a matriz de cargas rotacionada \\(\\hat{\\mathbf{L}}^*\\) a essa estrutura ideal.\nAs rota√ß√µes dividem-se em duas categorias principais.\n\n7.7.1 Rota√ß√µes Ortogonais\nNeste tipo de rota√ß√£o, a matriz de transforma√ß√£o \\(\\mathbf{T}\\) √© ortogonal, o que significa que os eixos dos fatores s√£o girados, mas mantidos em um √¢ngulo de 90 graus entre si. A consequ√™ncia fundamental √© que os fatores rotacionados permanecem n√£o correlacionados.\nOs m√©todos mais comuns de rota√ß√£o ortogonal s√£o:\n\nVarimax: √â o m√©todo de rota√ß√£o ortogonal mais popular. O objetivo do Varimax √© simplificar as colunas da matriz de cargas fatoriais. Para cada fator, ele busca maximizar a vari√¢ncia das cargas ao quadrado, efetivamente empurrando as cargas para perto de 0 ou \\(\\pm 1\\). Isso facilita a identifica√ß√£o de quais vari√°veis est√£o associadas a cada fator. O crit√©rio Varimax maximiza a seguinte fun√ß√£o:\n\\[\nV = \\sum_{j=1}^{m} \\left[ \\frac{1}{p} \\sum_{i=1}^{p} (\\hat{l}_{ij}^* / h_i)^4 - \\left( \\frac{1}{p} \\sum_{i=1}^{p} (\\hat{l}_{ij}^* / h_i)^2 \\right)^2 \\right]\n\\]\nOnde \\(\\hat{l}_{ij}^*\\) s√£o as cargas rotacionadas e \\(h_i^2\\) s√£o as comunalidades (que permanecem invariantes sob rota√ß√£o).\nQuartimax: Este m√©todo foca em simplificar as linhas da matriz de cargas. Ele tenta fazer com que cada vari√°vel tenha carga alta em apenas um fator. O Quartimax foi o primeiro m√©todo anal√≠tico proposto, mas tende a criar um fator geral com cargas altas para muitas vari√°veis, o que pode dificultar a interpreta√ß√£o.\nEquimax: √â um meio termo entre o Varimax e o Quartimax. Ele tenta simplificar tanto as linhas quanto as colunas da matriz de cargas simultaneamente.\n\n\n\n7.7.2 Rota√ß√µes Obl√≠quas\nEm muitos campos, especialmente nas ci√™ncias sociais, √© teoricamente razo√°vel esperar que os fatores latentes sejam correlacionados. Por exemplo, os fatores ‚Äúhabilidade verbal‚Äù e ‚Äúhabilidade matem√°tica‚Äù s√£o distintos, mas √© prov√°vel que sejam positivamente correlacionados.\nAs rota√ß√µes obl√≠quas permitem que os fatores se tornem correlacionados. A matriz de transforma√ß√£o \\(\\mathbf{T}\\) n√£o √© mais ortogonal, e os eixos dos fatores podem ter √¢ngulos diferentes de 90 graus. A vantagem √© a capacidade de encontrar uma estrutura mais simples e teoricamente mais realista, ao custo de uma complexidade maior na interpreta√ß√£o, pois √© preciso analisar tanto a matriz de cargas quanto a matriz de correla√ß√£o entre os fatores.\nOs m√©todos mais comuns incluem:\n\nPromax: √â um m√©todo muito utilizado que funciona em duas etapas. Primeiro, ele realiza uma rota√ß√£o ortogonal (geralmente Varimax). Em seguida, ele relaxa a restri√ß√£o de ortogonalidade, permitindo que os fatores se correlacionem para buscar uma estrutura ainda mais simples (com mais cargas pr√≥ximas de zero).\nOblimin Direto: √â um m√©todo mais geral que busca minimizar a covari√¢ncia das cargas ao quadrado para pares de fatores. Ele possui um par√¢metro (delta) que controla o grau de correla√ß√£o permitido entre os fatores.\n\nA escolha entre uma rota√ß√£o ortogonal e obl√≠qua depende de considera√ß√µes te√≥ricas. Se n√£o h√° uma raz√£o forte para acreditar que os fatores s√£o correlacionados, a rota√ß√£o ortogonal (como a Varimax) √© geralmente preferida por sua simplicidade. Se a correla√ß√£o entre os fatores √© esperada, uma rota√ß√£o obl√≠qua pode fornecer uma representa√ß√£o mais fiel da realidade.",
    "crumbs": [
      "Parte II: T√©cnicas Multivariadas",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>An√°lise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html",
    "href": "src/exemplos/acp_manual.html",
    "title": "Exemplo manual: ACP",
    "section": "",
    "text": "O Cen√°rio\nNeste exemplo, vamos detalhar passo a passo a aplica√ß√£o da An√°lise de Componentes Principais (ACP) em um pequeno conjunto de dados. O objetivo √© demonstrar manualmente todos os c√°lculos, desde a prepara√ß√£o dos dados at√© a interpreta√ß√£o dos resultados, seguindo a metodologia apresentada no Cap√≠tulo 3.\nVamos expandir o exemplo da intui√ß√£o geom√©trica, que usava Peso e Altura. Adicionaremos uma terceira vari√°vel, Renda (em milhares de R$), para um grupo de 5 indiv√≠duos. A ideia √© que Peso e Altura sejam correlacionados, mas a Renda n√£o tenha uma correla√ß√£o forte com eles.\nNosso conjunto de dados inicial √©:",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#o-cen√°rio",
    "href": "src/exemplos/acp_manual.html#o-cen√°rio",
    "title": "Exemplo manual: ACP",
    "section": "",
    "text": "Indiv√≠duo\nPeso (kg)\nAltura (cm)\nRenda (R$ 1000)\n\n\n\n\n1\n65\n170\n5.5\n\n\n2\n72\n182\n4.0\n\n\n3\n58\n165\n7.0\n\n\n4\n81\n190\n3.5\n\n\n5\n75\n178\n5.0",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-1-prepara√ß√£o-dos-dados",
    "href": "src/exemplos/acp_manual.html#passo-1-prepara√ß√£o-dos-dados",
    "title": "Exemplo manual: ACP",
    "section": "Passo 1: Prepara√ß√£o dos Dados",
    "text": "Passo 1: Prepara√ß√£o dos Dados\nConforme discutido no Cap√≠tulo 3, a ACP √© sens√≠vel √† escala das vari√°veis. Portanto, o primeiro passo √© padronizar os dados. Isso envolve duas etapas: centralizar (subtrair a m√©dia) e escalonar (dividir pelo desvio padr√£o).\n\n1.1. Calcular a M√©dia e o Desvio Padr√£o\nPrimeiro, calculamos a m√©dia e o desvio padr√£o para cada vari√°vel.\n\\[\n\\bar{x}_{peso} = \\frac{65+72+58+81+75}{5} = 70.2 \\, \\text{kg}\n\\] \\[\n\\bar{x}_{altura} = \\frac{170+182+165+190+178}{5} = 177.0 \\, \\text{cm}\n\\] \\[\n\\bar{x}_{renda} = \\frac{5.5+4.0+7.0+3.5+5.0}{5} = 5.0 \\, (RS 1000)\n\\]\nAgora, os desvios padr√£o (usando a f√≥rmula com denominador \\(n-1\\)):\n\\[\ns_{peso} = \\sqrt{\\frac{(65-70.2)^2 + ... + (75-70.2)^2}{4}} = 8.64 \\, \\text{kg}\n\\] \\[\ns_{altura} = \\sqrt{\\frac{(170-177)^2 + ... + (178-177)^2}{4}} = 9.67 \\, \\text{cm}\n\\] \\[\ns_{renda} = \\sqrt{\\frac{(5.5-5.0)^2 + ... + (5.0-5.0)^2}{4}} = 1.35 \\, (RS 1000)\n\\]\n\n\n1.2. Padronizar os Dados\nCom as m√©dias e desvios padr√£o, podemos padronizar cada observa√ß√£o \\(x_{ij}\\) usando a f√≥rmula \\(z_{ij} = (x_{ij} - \\bar{x}_j) / s_j\\).\nPor exemplo, para o Indiv√≠duo 1: \\[\nz_{1, peso} = \\frac{65 - 70.2}{8.64} = -0.60\n\\] \\[\nz_{1, altura} = \\frac{170 - 177}{9.67} = -0.72\n\\] \\[\nz_{1, renda} = \\frac{5.5 - 5.0}{1.35} = 0.37\n\\]\nAplicando isso a todos os dados, obtemos a matriz de dados padronizados \\(\\mathbf{Z}\\):\n\\[\n\\mathbf{Z} = \\begin{pmatrix}\n-0.60 & -0.72 & 0.37 \\\\\n0.21 & 0.52 & -0.74 \\\\\n-1.41 & -1.24 & 1.48 \\\\\n1.25 & 1.34 & -1.11 \\\\\n0.56 & 0.10 & 0.00\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-2-calcular-a-matriz-de-correla√ß√£o",
    "href": "src/exemplos/acp_manual.html#passo-2-calcular-a-matriz-de-correla√ß√£o",
    "title": "Exemplo manual: ACP",
    "section": "Passo 2: Calcular a Matriz de Correla√ß√£o",
    "text": "Passo 2: Calcular a Matriz de Correla√ß√£o\nComo estamos trabalhando com dados padronizados, a ACP ser√° realizada sobre a matriz de correla√ß√£o \\(\\mathbf{R}\\). A matriz de correla√ß√£o pode ser calculada como:\n\\[\n\\mathbf{R} = \\frac{1}{n-1} \\mathbf{Z}' \\mathbf{Z}\n\\]\nCalculando \\(\\mathbf{Z}' \\mathbf{Z}\\): \\[\n\\mathbf{Z}' \\mathbf{Z} = \\begin{pmatrix}\n4.00 & 3.85 & -0.81 \\\\\n3.85 & 4.00 & -1.18 \\\\\n-0.81 & -1.18 & 4.00\n\\end{pmatrix}\n\\]\nDividindo por \\(n-1 = 4\\), obtemos a matriz de correla√ß√£o \\(\\mathbf{R}\\):\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1.00 & 0.96 & -0.20 \\\\\n0.96 & 1.00 & -0.29 \\\\\n-0.20 & -0.29 & 1.00\n\\end{pmatrix}\n\\]\nComo esperado, a correla√ß√£o entre Peso e Altura (0.96) √© muito alta, enquanto a Renda tem uma correla√ß√£o fraca e negativa com as outras duas vari√°veis.",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-3-decomposi√ß√£o-espectral-da-matriz-de-correla√ß√£o",
    "href": "src/exemplos/acp_manual.html#passo-3-decomposi√ß√£o-espectral-da-matriz-de-correla√ß√£o",
    "title": "Exemplo manual: ACP",
    "section": "Passo 3: Decomposi√ß√£o Espectral da Matriz de Correla√ß√£o",
    "text": "Passo 3: Decomposi√ß√£o Espectral da Matriz de Correla√ß√£o\nO pr√≥ximo passo √© encontrar os autovalores (\\(\\lambda\\)) e autovetores (\\(\\mathbf{e}\\)) da matriz de correla√ß√£o \\(\\mathbf{R}\\). Eles s√£o a solu√ß√£o da equa√ß√£o \\(\\mathbf{R}\\mathbf{e} = \\lambda\\mathbf{e}\\), que √© equivalente a resolver \\((\\mathbf{R} - \\lambda\\mathbf{I})\\mathbf{e} = \\mathbf{0}\\).\nIsso requer encontrar as ra√≠zes do polin√¥mio caracter√≠stico \\(det(\\mathbf{R} - \\lambda\\mathbf{I}) = 0\\).\n\\[\ndet \\begin{pmatrix}\n1.00 - \\lambda & 0.96 & -0.20 \\\\\n0.96 & 1.00 - \\lambda & -0.29 \\\\\n-0.20 & -0.29 & 1.00 - \\lambda\n\\end{pmatrix} = 0\n\\]\nResolver este determinante c√∫bico manualmente √© trabalhoso. Usando uma calculadora ou software, encontramos os seguintes autovalores:\n\\[\n\\lambda_1 = 1.98 \\quad \\lambda_2 = 1.05 \\quad \\lambda_3 = 0.02\n\\]\n\nInterpreta√ß√£o dos Autovalores\nA vari√¢ncia total no sistema √© a soma dos autovalores (que √© igual ao tra√ßo da matriz \\(\\mathbf{R}\\), ou seja, 3). - Vari√¢ncia Total = \\(1.98 + 1.05 + 0.02 = 3.00\\)\nA propor√ß√£o da vari√¢ncia explicada por cada componente √©: - CP1: \\(\\frac{1.98}{3.00} = 66.0\\%\\) - CP2: \\(\\frac{1.05}{3.00} = 35.0\\%\\) - CP3: \\(\\frac{0.02}{3.00} = 0.7\\%\\)\nOs dois primeiros componentes juntos explicam \\(66.0\\% + 35.0\\% = 101.0\\%\\) da vari√¢ncia total. Isso indica que podemos reduzir a dimensionalidade de 3 para 2 com uma perda m√≠nima de informa√ß√£o.",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-4-calcular-os-autovetores",
    "href": "src/exemplos/acp_manual.html#passo-4-calcular-os-autovetores",
    "title": "Exemplo manual: ACP",
    "section": "Passo 4: Calcular os Autovetores",
    "text": "Passo 4: Calcular os Autovetores\nAgora, para cada autovalor, resolvemos o sistema \\((\\mathbf{R} - \\lambda_i\\mathbf{I})\\mathbf{e}_i = \\mathbf{0}\\) para encontrar o autovetor correspondente \\(\\mathbf{e}_i\\).\n\nPara \\(\\lambda_1 = 1.98\\): \\[\n\\begin{pmatrix}\n-0.98 & 0.96 & -0.20 \\\\\n0.96 & -0.98 & -0.29 \\\\\n-0.20 & -0.29 & -0.98\n\\end{pmatrix}\n\\begin{pmatrix} e_{11} \\\\ e_{12} \\\\ e_{13} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solu√ß√£o, ap√≥s normaliza√ß√£o (para que \\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\)), √©: \\[\n\\mathbf{e}_1 = \\begin{pmatrix} 0.69 \\\\ 0.71 \\\\ -0.15 \\end{pmatrix}\n\\]\nPara \\(\\lambda_2 = 1.05\\): \\[\n\\begin{pmatrix}\n-0.05 & 0.96 & -0.20 \\\\\n0.96 & -0.05 & -0.29 \\\\\n-0.20 & -0.29 & -0.05\n\\end{pmatrix}\n\\begin{pmatrix} e_{21} \\\\ e_{22} \\\\ e_{23} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solu√ß√£o normalizada √©: \\[\n\\mathbf{e}_2 = \\begin{pmatrix} 0.18 \\\\ -0.22 \\\\ -0.96 \\end{pmatrix}\n\\]\nPara \\(\\lambda_3 = 0.02\\): \\[\n\\begin{pmatrix}\n0.98 & 0.96 & -0.20 \\\\\n0.96 & 0.98 & -0.29 \\\\\n-0.20 & -0.29 & 0.98\n\\end{pmatrix}\n\\begin{pmatrix} e_{31} \\\\ e_{32} \\\\ e_{33} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solu√ß√£o normalizada √©: \\[\n\\mathbf{e}_3 = \\begin{pmatrix} -0.70 \\\\ 0.67 \\\\ -0.24 \\end{pmatrix}\n\\]",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-5-interpreta√ß√£o-dos-componentes",
    "href": "src/exemplos/acp_manual.html#passo-5-interpreta√ß√£o-dos-componentes",
    "title": "Exemplo manual: ACP",
    "section": "Passo 5: Interpreta√ß√£o dos Componentes",
    "text": "Passo 5: Interpreta√ß√£o dos Componentes\nOs autovetores (ou loadings) nos dizem como as vari√°veis originais se combinam para formar cada componente.\n\nComponente Principal 1 (\\(CP_1\\)): \\[\nY_1 = 0.69 \\cdot Z_{peso} + 0.71 \\cdot Z_{altura} - 0.15 \\cdot Z_{renda}\n\\] Este componente √© basicamente uma m√©dia ponderada de Peso e Altura, com uma pequena contribui√ß√£o negativa da Renda. Podemos interpret√°-lo como um √≠ndice de ‚ÄúTamanho Corporal‚Äù. As cargas altas e positivas para Peso e Altura confirmam a alta correla√ß√£o entre essas vari√°veis.\nComponente Principal 2 (\\(CP_2\\)): \\[\nY_2 = 0.18 \\cdot Z_{peso} - 0.22 \\cdot Z_{altura} - 0.96 \\cdot Z_{renda}\n\\] Este componente √© dominado pela Renda, com uma carga muito alta e negativa. As cargas para Peso e Altura s√£o pequenas. Podemos interpretar o \\(CP_2\\) como um √≠ndice de ‚ÄúStatus Socioecon√¥mico Inverso‚Äù, j√° que ele √© quase que inteiramente uma representa√ß√£o da Renda (com sinal trocado).",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-6-calcular-os-scores-dos-componentes",
    "href": "src/exemplos/acp_manual.html#passo-6-calcular-os-scores-dos-componentes",
    "title": "Exemplo manual: ACP",
    "section": "Passo 6: Calcular os Scores dos Componentes",
    "text": "Passo 6: Calcular os Scores dos Componentes\nFinalmente, podemos calcular os valores (scores) dos componentes principais para cada indiv√≠duo. Usamos a f√≥rmula \\(\\mathbf{Y} = \\mathbf{Z} \\mathbf{P}\\), onde \\(\\mathbf{P}\\) √© a matriz cujas colunas s√£o os autovetores.\n\\[\n\\mathbf{P} = \\begin{pmatrix}\n0.69 & 0.18 & -0.70 \\\\\n0.71 & -0.22 & 0.67 \\\\\n-0.15 & -0.96 & -0.24\n\\end{pmatrix}\n\\]\nPara o Indiv√≠duo 1, com dados padronizados \\((-0.60, -0.72, 0.37)\\): \\[\ny_{11} = (-0.60)(0.69) + (-0.72)(0.71) + (0.37)(-0.15) = -0.98\n\\] \\[\ny_{12} = (-0.60)(0.18) + (-0.72)(-0.22) + (0.37)(-0.96) = -0.31\n\\]\nCalculando para todos os indiv√≠duos, obtemos a matriz de scores \\(\\mathbf{Y}\\):\n\n\n\nIndiv√≠duo\nCP1 (Tamanho)\nCP2 (Renda Inversa)\n\n\n\n\n1\n-0.98\n-0.31\n\n\n2\n0.57\n0.85\n\n\n3\n-2.19\n-1.18\n\n\n4\n2.09\n1.35\n\n\n5\n0.46\n0.08",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#conclus√£o",
    "href": "src/exemplos/acp_manual.html#conclus√£o",
    "title": "Exemplo manual: ACP",
    "section": "Conclus√£o",
    "text": "Conclus√£o\nEste exemplo demonstra o poder da ACP. Come√ßamos com tr√™s vari√°veis e, atrav√©s de uma deriva√ß√£o passo a passo, conseguimos: 1. Reduzir a dimensionalidade: Mostramos que 99.3% da informa√ß√£o est√° contida em dois componentes. 2. Criar vari√°veis n√£o correlacionadas: O \\(CP_1\\) e o \\(CP_2\\) s√£o, por constru√ß√£o, ortogonais. 3. Interpretar a estrutura latente: Identificamos que a principal fonte de varia√ß√£o nos dados √© o ‚ÄúTamanho Corporal‚Äù (uma combina√ß√£o de Peso e Altura), seguida pelo ‚ÄúStatus Socioecon√¥mico‚Äù (representado pela Renda).\nA an√°lise manual, embora trabalhosa, revela a mec√¢nica exata da t√©cnica, solidificando a compreens√£o te√≥rica apresentada no cap√≠tulo principal.",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html",
    "href": "src/exemplos/af_big_five.html",
    "title": "Rota√ß√£o fatorial em R",
    "section": "",
    "text": "Passo 1: An√°lise Descritiva e Adequa√ß√£o dos Dados\nA An√°lise Fatorial (AF) √© uma t√©cnica estat√≠stica poderosa usada para identificar estruturas latentes (fatores) subjacentes a um conjunto de vari√°veis observadas. No entanto, a solu√ß√£o matem√°tica inicial de uma AF raramente √© interpret√°vel. √â aqui que a rota√ß√£o fatorial se torna a etapa mais cr√≠tica do processo. A rota√ß√£o transforma a matriz de cargas fatoriais inicial em uma solu√ß√£o mais simples e teoricamente mais significativa, sem alterar as propriedades matem√°ticas fundamentais da solu√ß√£o.\nEste documento oferece um exemplo pr√°tico e did√°tico, totalmente focado em demonstrar o impacto das diferentes estrat√©gias de rota√ß√£o. Usaremos o software R e o cl√°ssico conjunto de dados bfi (Big Five Inventory) do pacote psych.\nObjetivos:\nPrimeiro, carregamos os pacotes necess√°rios e o conjunto de dados bfi. Este dataset cont√©m respostas de 2800 indiv√≠duos a 25 itens de personalidade.\nC√≥digo\n# Carregar pacotes\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(corrplot)\n\n# Carregar os dados do Big Five Inventory\ndata(bfi, package = \"psych\")\n\n# Selecionar apenas as 25 vari√°veis de itens de personalidade\nbfi_items &lt;- bfi[, 1:25]\n\n# Remover linhas com dados ausentes para simplificar\nbfi_complete &lt;- na.omit(bfi_items)\n\nknitr::kable(head(bfi_complete))\n\n\n\n\nTabela¬†1: Exemplo de respostas no banco de dados Big Five\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1\nA2\nA3\nA4\nA5\nC1\nC2\nC3\nC4\nC5\nE1\nE2\nE3\nE4\nE5\nN1\nN2\nN3\nN4\nN5\nO1\nO2\nO3\nO4\nO5\n\n\n\n\n61617\n2\n4\n3\n4\n4\n2\n3\n3\n4\n4\n3\n3\n3\n4\n4\n3\n4\n2\n2\n3\n3\n6\n3\n4\n3\n\n\n61618\n2\n4\n5\n2\n5\n5\n4\n4\n3\n4\n1\n1\n6\n4\n3\n3\n3\n3\n5\n5\n4\n2\n4\n3\n3\n\n\n61620\n5\n4\n5\n4\n4\n4\n5\n4\n2\n5\n2\n4\n4\n4\n5\n4\n5\n4\n2\n3\n4\n2\n5\n5\n2\n\n\n61621\n4\n4\n6\n5\n5\n4\n4\n3\n5\n5\n5\n3\n4\n4\n4\n2\n5\n2\n4\n1\n3\n3\n4\n3\n5\n\n\n61622\n2\n3\n3\n4\n5\n4\n4\n5\n3\n2\n2\n2\n5\n4\n5\n2\n3\n4\n4\n3\n3\n3\n4\n3\n3\n\n\n61623\n6\n6\n5\n6\n5\n6\n6\n6\n1\n3\n2\n1\n6\n5\n6\n3\n5\n2\n2\n3\n4\n3\n5\n6\n1\nAs 25 vari√°veis correspondem a 5 itens para cada um dos tra√ßos do ‚ÄúBig Five‚Äù:\nA hip√≥tese te√≥rica √© que os 5 itens que medem o mesmo tra√ßo (e.g., N1 a N5) estar√£o altamente correlacionados entre si e se agrupar√£o em um √∫nico fator latente (Neuroticismo).\nUma boa pr√°tica √© verificar se os dados s√£o fatoriz√°veis. Para isso, podemos usar o teste de Bartlett e a medida KMO.\nC√≥digo\n# Teste de Bartlett\nbartlett_test &lt;- cortest.bartlett(bfi_complete)\n\n\nR was not square, finding R from data\n\n\nC√≥digo\n# Teste KMO\nkmo_test &lt;- KMO(bfi_complete)\n\n# Exibindo os resultados de forma concisa\ncat(\"Teste de Bartlett: p-valor =\", bartlett_test$p.value, \"\\n\")\n\n\nTeste de Bartlett: p-valor = 0 \n\n\nC√≥digo\ncat(\"Medida KMO Geral (Overall MSA):\", round(kmo_test$MSA, 2), \"\\n\")\n\n\nMedida KMO Geral (Overall MSA): 0.85\nO p-valor de Bartlett pr√≥ximo de zero e o KMO de 0.85 (‚Äúmerit√≥rio‚Äù) sugerem que os dados t√™m correla√ß√µes suficientes para justificar uma an√°lise fatorial.",
    "crumbs": [
      "Exemplos",
      "Rota√ß√£o fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-1-an√°lise-descritiva-e-adequa√ß√£o-dos-dados",
    "href": "src/exemplos/af_big_five.html#passo-1-an√°lise-descritiva-e-adequa√ß√£o-dos-dados",
    "title": "Rota√ß√£o fatorial em R",
    "section": "",
    "text": "A1-A5: Amabilidade (Agreeableness)\nC1-C5: Conscienciosidade (Conscientiousness)\nE1-E5: Extrovers√£o (Extraversion)\nN1-N5: Neuroticismo (Neuroticism)\nO1-O5: Abertura √† Experi√™ncia (Openness)",
    "crumbs": [
      "Exemplos",
      "Rota√ß√£o fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-2-extra√ß√£o-inicial-dos-fatores-e-escolha-do-n√∫mero-de-fatores",
    "href": "src/exemplos/af_big_five.html#passo-2-extra√ß√£o-inicial-dos-fatores-e-escolha-do-n√∫mero-de-fatores",
    "title": "Rota√ß√£o fatorial em R",
    "section": "Passo 2: Extra√ß√£o Inicial dos Fatores e Escolha do N√∫mero de Fatores",
    "text": "Passo 2: Extra√ß√£o Inicial dos Fatores e Escolha do N√∫mero de Fatores\nAntes de rotacionar, precisamos extrair os fatores. Dois m√©todos comuns s√£o a Fatora√ß√£o do Eixo Principal (ou ‚Äúcomponentes principais‚Äù para o modelo fatorial) e a M√°xima Verossimilhan√ßa. Vamos extrair 5 fatores usando ambos os m√©todos (sem rota√ß√£o) para ver a solu√ß√£o inicial.\n\n\nC√≥digo\n# Extra√ß√£o via Fatora√ß√£o do Eixo Principal (Principal Axis Factoring)\nfa_pa &lt;- fa(bfi_complete, nfactors = 5, rotate = \"none\", fm = \"pa\")\ncat(\"Cargas - Fatora√ß√£o do Eixo Principal (PA):\\n\")\n\n\nCargas - Fatora√ß√£o do Eixo Principal (PA):\n\n\nC√≥digo\nprint(fa_pa$loadings, cutoff = 0.3)\n\n\n\nLoadings:\n   PA1    PA2    PA3    PA4    PA5   \nA1                             -0.371\nA2  0.467                       0.340\nA3  0.534  0.302                     \nA4  0.417                            \nA5  0.581                            \nC1  0.343         0.446              \nC2  0.336         0.477              \nC3  0.319         0.351  0.310       \nC4 -0.465        -0.452              \nC5 -0.493                            \nE1 -0.408                            \nE2 -0.619                       0.323\nE3  0.527  0.328                     \nE4  0.599        -0.329              \nE5  0.513                            \nN1 -0.441  0.636                     \nN2 -0.423  0.616                     \nN3 -0.407  0.611                     \nN4 -0.528  0.416                     \nN5 -0.345  0.413                     \nO1  0.328               -0.360       \nO2                       0.370       \nO3  0.407               -0.446       \nO4                                   \nO5                       0.412       \n\n                 PA1   PA2   PA3   PA4   PA5\nSS loadings    4.600 2.268 1.549 1.218 0.956\nProportion Var 0.184 0.091 0.062 0.049 0.038\nCumulative Var 0.184 0.275 0.337 0.385 0.424\n\n\nC√≥digo\n# Extra√ß√£o via M√°xima Verossimilhan√ßa (Maximum Likelihood)\nfa_ml &lt;- fa(bfi_complete, nfactors = 5, rotate = \"none\", fm = \"ml\")\ncat(\"\\nCargas - M√°xima Verossimilhan√ßa (ML):\\n\")\n\n\n\nCargas - M√°xima Verossimilhan√ßa (ML):\n\n\nC√≥digo\nprint(fa_ml$loadings, cutoff = 0.3)\n\n\n\nLoadings:\n   ML1    ML2    ML3    ML4    ML5   \nA1                             -0.322\nA2 -0.396  0.354                0.334\nA3 -0.462  0.401                0.321\nA4 -0.386                            \nA5 -0.546                            \nC1                0.465              \nC2                0.511              \nC3                0.404              \nC4  0.441        -0.512              \nC5  0.485        -0.358              \nE1  0.355 -0.309                     \nE2  0.585                       0.336\nE3 -0.446  0.436                     \nE4 -0.552  0.333                     \nE5 -0.409  0.429                     \nN1  0.609  0.566                     \nN2  0.587  0.543                     \nN3  0.533  0.479                     \nN4  0.591                            \nN5  0.421                            \nO1                      -0.409       \nO2                       0.388       \nO3 -0.329  0.349        -0.491       \nO4                      -0.307  0.311\nO5                       0.433       \n\n                 ML1   ML2   ML3   ML4   ML5\nSS loadings    4.451 2.379 1.546 1.221 0.977\nProportion Var 0.178 0.095 0.062 0.049 0.039\nCumulative Var 0.178 0.273 0.335 0.384 0.423\n\n\nAs duas solu√ß√µes iniciais s√£o numericamente diferentes, mas conceitualmente iguais: s√£o ininterpret√°veis. Um primeiro fator geral domina, e as vari√°veis se distribuem de forma confusa nos demais. Isso refor√ßa a necessidade da rota√ß√£o.\nPara determinar o n√∫mero de fatores a extrair de forma mais objetiva, usamos a An√°lise Paralela.\n\n\nC√≥digo\nfa.parallel(bfi_complete, fa = \"fa\", fm = \"pa\")\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  NA \n\n\n\n\n\n\n\n\nFigura¬†1: An√°lise Paralela sugerindo a extra√ß√£o de 6 fatores.\n\n\n\n\n\nA An√°lise Paralela (Figura¬†1) sugere 6 fatores. No entanto, como nosso objetivo √© testar a teoria dos Big Five, prosseguiremos com a extra√ß√£o de 5 fatores, uma decis√£o comum quando a teoria √© forte.",
    "crumbs": [
      "Exemplos",
      "Rota√ß√£o fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-3-rota√ß√£o-ortogonal-varimax",
    "href": "src/exemplos/af_big_five.html#passo-3-rota√ß√£o-ortogonal-varimax",
    "title": "Rota√ß√£o fatorial em R",
    "section": "Passo 3: Rota√ß√£o Ortogonal (Varimax)",
    "text": "Passo 3: Rota√ß√£o Ortogonal (Varimax)\nA rota√ß√£o Varimax ‚Äúlimpa‚Äù a estrutura sob a suposi√ß√£o de que os fatores n√£o s√£o correlacionados entre si.\n\n\nC√≥digo\n# An√°lise Fatorial com rota√ß√£o Varimax\nfa_varimax &lt;- factanal(bfi_complete, \n                       factors = 5, \n                       rotation = \"varimax\")\n\ncat(\"Cargas Fatoriais (AF) - Rota√ß√£o Varimax:\\n\")\n\n\nCargas Fatoriais (AF) - Rota√ß√£o Varimax:\n\n\nC√≥digo\nprint(fa_varimax$loadings, cutoff = 0.3, sort = TRUE)\n\n\n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5\nN1  0.816                                 \nN2  0.787                                 \nN3  0.714                                 \nN4  0.562  -0.367                         \nN5  0.518                                 \nE1         -0.587                         \nE2         -0.674                         \nE4          0.613           0.363         \nC1                  0.533                 \nC2                  0.624                 \nC3                  0.554                 \nC4                 -0.653                 \nC5                 -0.573                 \nA2                          0.601         \nA3                          0.662         \nA5          0.351           0.580         \nO1                                  0.524 \nO3                                  0.614 \nO5                                 -0.512 \nA1                         -0.393         \nA4                          0.454         \nE3          0.490           0.315   0.313 \nE5          0.491   0.310                 \nO2                                 -0.454 \nO4                                  0.368 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      2.687   2.320   2.034   1.978   1.557\nProportion Var   0.107   0.093   0.081   0.079   0.062\nCumulative Var   0.107   0.200   0.282   0.361   0.423\n\n\nA estrutura agora √© muito mais ‚Äúsimples‚Äù e alinhada com a teoria. Podemos visualizar essa transforma√ß√£o de forma clara comparando o c√≠rculo de correla√ß√µes antes e depois da rota√ß√£o.\nPrimeiro, a solu√ß√£o n√£o rotacionada (Figura¬†2). Note como as vari√°veis (vetores) se espalham pelo espa√ßo fatorial sem um padr√£o claro. √â dif√≠cil tra√ßar os eixos (fatores) de forma que representem grupos distintos de vari√°veis.\n\n\nC√≥digo\nlibrary(ggrepel)\n\n# Extrair cargas da solu√ß√£o N√ÉO ROTACIONADA (ml) para um dataframe\nloadings_unrotated_df &lt;- as.data.frame(unclass(fa_ml$loadings))\nloadings_unrotated_df$Variable &lt;- rownames(loadings_unrotated_df)\n\n# Selecionar algumas vari√°veis para anotar e evitar polui√ß√£o\nvars_to_label &lt;- c(\"N1\", \"N3\", \"E2\", \"E4\", \"A1\", \"C1\", \"O1\")\nannotations_df_unrotated &lt;- loadings_unrotated_df %&gt;% filter(Variable %in% vars_to_label)\n\n# Criar dados para o c√≠rculo unit√°rio\ncircle &lt;- data.frame(\n  angle = seq(-pi, pi, length = 100),\n  x = sin(seq(-pi, pi, length = 100)),\n  y = cos(seq(-pi, pi, length = 100))\n)\n\n# Gerar o gr√°fico com ggplot2\nggplot(data = loadings_unrotated_df, aes(x = ML1, y = ML2)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_path(data = circle, aes(x = x, y = y), inherit.aes = FALSE, color = \"gray60\") +\n  geom_segment(aes(x = 0, y = 0, xend = ML1, yend = ML2),\n               arrow = arrow(length = unit(0.1, \"inches\")),\n               color = \"steelblue\") +\n  geom_text_repel(data = annotations_df_unrotated, aes(label = Variable), min.segment.length = 0) +\n  coord_fixed(ratio = 1, xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  labs(title = \"C√≠rculo de Correla√ß√µes - Solu√ß√£o N√£o Rotacionada\",\n       x = \"Fator 1\",\n       y = \"Fator 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura¬†2: C√≠rculo de correla√ß√µes da solu√ß√£o n√£o rotacionada. Os vetores n√£o est√£o alinhados com os eixos.\n\n\n\n\n\nAgora, veja o resultado ap√≥s a rota√ß√£o Varimax (Figura¬†3). A rota√ß√£o funcionou como um ajuste dos eixos, alinhando-os com os agrupamentos de vari√°veis.\n\n\nC√≥digo\n# Extrair cargas da solu√ß√£o ROTACIONADA (Varimax) para um dataframe\nloadings_rotated_df &lt;- as.data.frame(unclass(fa_varimax$loadings))\nloadings_rotated_df$Variable &lt;- rownames(loadings_rotated_df)\n\n# Selecionar as mesmas vari√°veis para anotar\nannotations_df_rotated &lt;- loadings_rotated_df %&gt;% filter(Variable %in% vars_to_label)\n\n# Calcular a vari√¢ncia explicada para os eixos\nss_loadings &lt;- colSums(fa_varimax$loadings^2)\nprop_variance &lt;- ss_loadings / ncol(bfi_complete)\nxlab_text &lt;- sprintf(\"Fator 1 (%.2f%% da vari√¢ncia)\", prop_variance[1] * 100)\nylab_text &lt;- sprintf(\"Fator 2 (%.2f%% da vari√¢ncia)\", prop_variance[2] * 100)\n\n# Gerar o gr√°fico com ggplot2\nggplot(data = loadings_rotated_df, aes(x = Factor1, y = Factor2)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_path(data = circle, aes(x = x, y = y), inherit.aes = FALSE, color = \"gray60\") +\n  geom_segment(aes(x = 0, y = 0, xend = Factor1, yend = Factor2),\n               arrow = arrow(length = unit(0.1, \"inches\")),\n               color = \"steelblue\") +\n  geom_text_repel(data = annotations_df_rotated, aes(label = Variable), min.segment.length = 0) +\n  coord_fixed(ratio = 1, xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  labs(title = \"C√≠rculo de Correla√ß√µes - Rota√ß√£o Varimax\",\n       x = xlab_text,\n       y = ylab_text) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura¬†3: C√≠rculo de correla√ß√µes ap√≥s a rota√ß√£o Varimax. A rota√ß√£o alinhou os vetores com os eixos, revelando uma estrutura simples.\n\n\n\n\n\nO resultado √© uma ‚Äúestrutura simples‚Äù, onde os itens de Neuroticismo (como N1 e N3) carregam quase exclusivamente no Fator 1 (correla√ß√£o pr√≥xima de 1 ou -1 em um eixo e de 0 no outro), e os itens de Extrovers√£o (como E2 e E4) carregam no Fator 2. A interpreta√ß√£o se torna direta.",
    "crumbs": [
      "Exemplos",
      "Rota√ß√£o fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-4-rota√ß√£o-obl√≠qua-promax",
    "href": "src/exemplos/af_big_five.html#passo-4-rota√ß√£o-obl√≠qua-promax",
    "title": "Rota√ß√£o fatorial em R",
    "section": "Passo 4: Rota√ß√£o Obl√≠qua (Promax)",
    "text": "Passo 4: Rota√ß√£o Obl√≠qua (Promax)\nA rota√ß√£o Promax √© mais flex√≠vel, pois permite que os fatores sejam correlacionados. Isso costuma ser mais realista em psicologia.\n\n\nC√≥digo\n# An√°lise Fatorial com rota√ß√£o Promax (obl√≠qua)\nfa_promax &lt;- fa(bfi_complete, \n                nfactors = 5, \n                rotate = \"promax\", \n                fm = \"pa\")\n\n\nLoading required namespace: GPArotation\n\n\nC√≥digo\ncat(\"Cargas Fatoriais (Pattern Matrix) - Rota√ß√£o Promax:\\n\")\n\n\nCargas Fatoriais (Pattern Matrix) - Rota√ß√£o Promax:\n\n\nC√≥digo\nprint(fa_promax$loadings, cutoff = 0.3, sort = TRUE)\n\n\n\nLoadings:\n   PA2    PA1    PA3    PA5    PA4   \nN1  0.835                            \nN2  0.791                            \nN3  0.741                            \nN4  0.533 -0.311                     \nN5  0.529                            \nE1        -0.636                     \nE2        -0.711                     \nE3         0.545                     \nE4         0.660                     \nC1                0.567              \nC2                0.697              \nC3                0.597              \nC4               -0.652              \nC5               -0.561              \nA2                       0.611       \nA3                       0.620       \nO3                              0.576\nO5                             -0.543\nA1                      -0.463       \nA4                       0.411       \nA5         0.332         0.489       \nE5         0.498                     \nO1                              0.491\nO2                             -0.484\nO4                              0.370\n\n                 PA2   PA1   PA3   PA5   PA4\nSS loadings    2.704 2.486 2.050 1.638 1.461\nProportion Var 0.108 0.099 0.082 0.066 0.058\nCumulative Var 0.108 0.208 0.290 0.355 0.414\n\n\nA matriz de cargas √© similar √† da Varimax, mas a grande vantagem √© poder examinar a matriz de correla√ß√£o entre os fatores.\n\n\nC√≥digo\n# Matriz de correla√ß√£o entre os fatores\nfactor_correlations &lt;- fa_promax$Phi\n\ncat(\"Matriz de Correla√ß√£o entre os Fatores (Promax):\\n\")\n\n\nMatriz de Correla√ß√£o entre os Fatores (Promax):\n\n\nC√≥digo\nprint(round(factor_correlations, 2))\n\n\n      PA2   PA1   PA3   PA5  PA4\nPA2  1.00 -0.26 -0.22 -0.01 0.04\nPA1 -0.26  1.00  0.40  0.35 0.14\nPA3 -0.22  0.40  1.00  0.24 0.19\nPA5 -0.01  0.35  0.24  1.00 0.16\nPA4  0.04  0.14  0.19  0.16 1.00\n\n\nC√≥digo\n# Visualiza√ß√£o da matriz de correla√ß√£o\ncorrplot(factor_correlations, method = \"color\", type = \"upper\", \n         addCoef.col = \"black\", tl.col = \"black\", tl.srt = 45, diag = FALSE)\n\n\n\n\n\n\n\n\nFigura¬†4: Correla√ß√µes entre os fatores da solu√ß√£o Promax.\n\n\n\n\n\nA matriz de correla√ß√£o (Figura¬†4) mostra que Neuroticismo (ML1) se correlaciona negativamente com Conscienciosidade (ML3) (-0.33) e Extrovers√£o (ML2) (-0.24). Essas rela√ß√µes s√£o teoricamente plaus√≠veis e seriam perdidas em uma rota√ß√£o ortogonal.",
    "crumbs": [
      "Exemplos",
      "Rota√ß√£o fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#conclus√£o-qual-rota√ß√£o-escolher",
    "href": "src/exemplos/af_big_five.html#conclus√£o-qual-rota√ß√£o-escolher",
    "title": "Rota√ß√£o fatorial em R",
    "section": "Conclus√£o: Qual Rota√ß√£o Escolher?",
    "text": "Conclus√£o: Qual Rota√ß√£o Escolher?\n\nSolu√ß√£o N√£o Rotacionada: √â apenas um ponto de partida matem√°tico. Dificilmente √© poss√≠vel tirar interpreta√ß√µes √∫teis dela.\nRota√ß√£o Ortogonal (Varimax): √â a melhor escolha quando h√° fortes raz√µes te√≥ricas para acreditar que os fatores s√£o independentes. Oferece uma solu√ß√£o mais simples (parcimoniosa).\nRota√ß√£o Obl√≠qua (Promax): √â uma escolha mais realista nas ci√™ncias sociais. A decis√£o final deve ser baseada na matriz de correla√ß√£o dos fatores. Se as correla√ß√µes forem substanciais, a solu√ß√£o obl√≠qua √© superior.\n\nNeste exemplo, a solu√ß√£o Promax (obl√≠qua) √© a mais apropriada. Ela n√£o apenas recupera a estrutura dos Big Five, mas tamb√©m fornece insights sobre como esses tra√ßos se relacionam, oferecendo uma vis√£o mais rica e fiel da realidade psicol√≥gica.",
    "crumbs": [
      "Exemplos",
      "Rota√ß√£o fatorial em R"
    ]
  }
]