[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Multivariada 2",
    "section": "",
    "text": "Prefácio\nSeja Bem-vindo! Este livro aborda uma variedade de técnicas de análise multivariada, essenciais para a compreensão de dados complexos em diversas áreas do conhecimento.\nEste material foi elaborado especialmente para estudantes tendo um primeiro contato com técnicas estatísticas multivariadas. São cobertos os métodos a seguir:\n\nAnálise de Componentes Principais\nAnálise Fatorial\nAnálise de Agrupamento\nAnálise Discriminante\nAnálise de Correlação Canônica\nAnálise de Correspondência\n\nO objetivo é fornecer uma base sólida e prática para a aplicação dessas técnicas. antes, temos uma breve introdução dos conceitos fundamentais que norteiam a análise multivariada e algumas definições e resultados vetores e matrizes que são importantes para o acompanhamento do livro.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html",
    "href": "src/01_intro/01_introducao.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Por que usar Análise Multivariada?\nA análise multivariada é o campo da estatística dedicado a compreender conjuntos de dados com múltiplas variáveis inter-relacionadas. Em vez de analisar cada variável isoladamente, seu foco é examinar simultaneamente as relações entre três ou mais variáveis para extrair padrões e estruturas que de outra forma permaneceriam ocultos.\nCada observação em um estudo — seja um paciente descrito por indicadores de saúde, um consumidor por hábitos de compra, ou uma empresa por métricas financeiras — pode ser representada como um vetor de observações. A análise multivariada nos fornece as ferramentas para entender a estrutura de dependência e interdependência dentro desses vetores.\nA análise multivariada é motivada pela necessidade de extrair informações significativas de conjuntos de dados complexos. Ao invés de analisar variáveis de forma isolada, essas técnicas permitem uma compreensão mais profunda e realista dos dados. Os principais objetivos são:\nNos próximos capítulos, construiremos a base teórica para atingir esses objetivos, começando pelo conceito de vetor aleatório e seus parâmetros, para depois explorarmos como as amostras de dados nos permitem estimar e analisar essas estruturas.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html#por-que-usar-análise-multivariada",
    "href": "src/01_intro/01_introducao.html#por-que-usar-análise-multivariada",
    "title": "1  Introdução",
    "section": "",
    "text": "Simplificação Estrutural: Reduzir a dimensionalidade dos dados, identificando as principais fontes de variação e eliminando redundâncias. Isso facilita a visualização e a interpretação de dados complexos, revelando a estrutura subjacente de forma mais clara.\nAgrupamento e Classificação: Organizar as observações em grupos homogêneos (agrupamento) ou atribuir observações a categorias predefinidas (classificação). O objetivo é identificar padrões que permitam segmentar os dados de maneira significativa.\nInvestigação de Estruturas de Dependência: Explorar e quantificar as relações entre variáveis. Isso inclui desde a análise de correlações simples até a modelagem de interações complexas entre múltiplos conjuntos de variáveis.\nPredição: Construir modelos para prever o valor de uma ou mais variáveis com base em outras.\nInferência: Realizar testes de hipóteses e inferências estatísticas sobre as relações em um contexto multivariado.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html#visão-geral-das-técnicas-multivariadas",
    "href": "src/01_intro/01_introducao.html#visão-geral-das-técnicas-multivariadas",
    "title": "1  Introdução",
    "section": "1.2 Visão Geral das Técnicas Multivariadas",
    "text": "1.2 Visão Geral das Técnicas Multivariadas\nAs técnicas de análise multivariada podem ser classificadas com base em seus objetivos e na natureza das relações entre as variáveis. Uma distinção fundamental é entre técnicas de dependência, que analisam a relação entre variáveis dependentes e independentes, e técnicas de interdependência, que exploram as relações em um único conjunto de variáveis.\n\nTécnicas de Dependência: Analisam a relação entre uma ou mais variáveis dependentes e um conjunto de variáveis independentes. O objetivo é prever ou explicar o valor das variáveis dependentes.\nTécnicas de Interdependência: Exploram as relações entre todas as variáveis de um conjunto, sem fazer distinção entre dependentes e independentes. O foco é entender a estrutura geral dos dados.\n\nAlém disso a escolha de uma determinada técnica depende também dos tipos de variáveis em questão.\n\nVariáveis Categóricas (Qualitativas): Representam categorias ou grupos (e.g., gênero, tipo de produto).\nVariáveis Métricas (Quantitativas): Representam quantidades numéricas (e.g., idade, altura, renda, temperatura).\n\nCom o objetivo de classificar os métodos a serem apresentados nesse livro e posteriormente auxiliar na escolha da técnica mais adequada para o tratamento de um conjunto de dados, apresentamos a seguir uma tabela com algumas características de cada método e na sequência um fluxograma de decisão.\n\n\n\nTabela 1.1: Principais técnicas abordadas neste livro.\n\n\n\n\n\n\n\n\n\n\n\nTécnica\nObjetivo Principal\nTipo de Variável\nTipo de Análise\n\n\n\n\nComponentes Principais (PCA)\nRedução de dimensionalidade\nQuantitativas\nInterdependência\n\n\nAnálise Fatorial (FA)\nIdentificação de fatores latentes\nQuantitativas\nInterdependência\n\n\nAnálise de Agrupamento\nFormação de grupos homogêneos\nQuantitativas/Qualitativas\nInterdependência\n\n\nAnálise Discriminante\nClassificação de observações\nMista (Quali/Quanti)\nDependência\n\n\nCorrelação Canônica\nRelação entre conjuntos de variáveis\nQuantitativas\nDependência\n\n\nAnálise de Correspondência\nRelação entre variáveis categóricas\nQualitativas\nInterdependência\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\ncluster_blue_leaves\n\n\n\n\ncanonical_corr\n\nCorrelação Canônica\n\n\n\ndiscr_analysis\n\nAnálise Discriminante\n\n\n\nfactor_analysis\n\nAnálise Fatorial\n\n\n\npca\n\nComponentes Principais\n\n\n\ncluster_analysis\n\nAnálise de Agrupamento\n\n\n\ncorrespondence_analysis\n\nAnálise de Correspondência\n\n\n\nmulti_regression\n\nRegressão Múltipla\n\n\n\nmanova\n\nMANOVA\n\n\n\nlogistic_regression\n\nRegressão Logística\n\n\n\nrelation_type\n\nTipo de relação estudada\n\n\n\ndep_vs_indep\n\nDependência\n\n\n\nrelation_type-&gt;dep_vs_indep\n\n\n\n\n\ninterdep\n\nInterdependência\n\n\n\nrelation_type-&gt;interdep\n\n\n\n\n\ndep_var_type\n\nVariável Dependente?\n\n\n\ndep_vs_indep-&gt;dep_var_type\n\n\n\n\n\ngoal_interdep\n\nObjetivo?\n\n\n\ninterdep-&gt;goal_interdep\n\n\n\n\n\nnum_dependents\n\nQuantas Dependentes?\n\n\n\ndep_var_type-&gt;num_dependents\n\n\nMétrica\n\n\n\ngoal_cat_dep\n\nObjetivo?\n\n\n\ndep_var_type-&gt;goal_cat_dep\n\n\nCategórica\n\n\n\nnum_dependents-&gt;multi_regression\n\n\nUma\n\n\n\ngoal_multi_dep\n\nObjetivo?\n\n\n\nnum_dependents-&gt;goal_multi_dep\n\n\nMúltiplas\n\n\n\ngoal_multi_dep-&gt;canonical_corr\n\n\nRelacionar Conjuntos\n\n\n\ngoal_multi_dep-&gt;manova\n\n\nComparar Grupos\n\n\n\ngoal_cat_dep-&gt;discr_analysis\n\n\nClassificar\n\n\n\ngoal_cat_dep-&gt;logistic_regression\n\n\nPrever Probabilidade\n\n\n\ngoal_interdep-&gt;cluster_analysis\n\n\nAgrupar\n\n\n\nlatent_factors\n\nFatores Latentes?\n\n\n\ngoal_interdep-&gt;latent_factors\n\n\nReduzir Dimensão\n\n\n\ncategorical_vars\n\nVariáveis Categóricas?\n\n\n\ngoal_interdep-&gt;categorical_vars\n\n\nAssociar\n\n\n\nlatent_factors-&gt;factor_analysis\n\n\nSim\n\n\n\nlatent_factors-&gt;pca\n\n\nNão\n\n\n\ncategorical_vars-&gt;correspondence_analysis\n\n\nSim\n\n\n\n\n\n\nFigura 1.1: Diagrama de decisão para escolha de técnica de Análise Multivariada. Nós enfatizados fundo azul escuro indicam as técnicas de análise multivariada abordadas neste livro. Importante: Este diagrama é um guia simplificado para auxiliar na escolha da técnica mais adequada com base nas características dos dados e nos objetivos da análise. Ele não é exaustivo e serve apenas para posicionar as técnicas discutidas neste livro. A escolha final da técnica deve sempre considerar o contexto específico do problema e as características detalhadas dos dados.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html",
    "href": "src/01_intro/02_vetores_aleatorios.html",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "",
    "text": "2.1 O Vetor Aleatório\nNo capítulo anterior, estabelecemos a motivação para a análise multivariada: a necessidade de entender sistemas complexos onde múltiplas variáveis interagem. Para fazer isso de maneira formal e rigorosa, precisamos primeiro definir o objeto matemático central de nosso estudo. Em vez de começar com uma tabela de dados, começamos com o conceito que gera esses dados: o vetor aleatório.\nImagine que, para uma população de interesse (e.g., todos os estudantes de uma universidade), associamos a cada membro um conjunto de \\(p\\) características que nos interessam (e.g., nota em matemática, nota em história, horas de estudo). Antes de observarmos um membro específico, os valores dessas características são incertos. Podemos modelar essa incerteza tratando cada característica como uma variável aleatória.\nEste vetor é a representação matemática de uma “observação multivariada” em nível populacional. Toda a teoria da análise multivariada se baseia na compreensão das propriedades e da estrutura de distribuição deste vetor.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#o-vetor-aleatório",
    "href": "src/01_intro/02_vetores_aleatorios.html#o-vetor-aleatório",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "",
    "text": "Definição 2.1 Um vetor aleatório \\(\\mathbf{x}\\) é um vetor-coluna cujos componentes são \\(p\\) variáveis aleatórias, \\(X_1, X_2, \\ldots, X_p\\).\n\\[\n\\mathbf{x} = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\n\\vdots \\\\\nX_p\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\nCuidado com a Notação\n\n\n\n\nUma letra minúscula em negrito (e.g., \\(\\mathbf{x}\\)) denota um vetor aleatório.\nUma letra maiúscula comum (e.g., \\(X_j\\)) denota uma variável aleatória escalar, o \\(j\\)-ésimo componente do vetor.\nMais adiante, uma letra maiúscula em negrito (e.g., \\(\\mathbf{X}\\)) será usada para a matriz de dados (amostral).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#parâmetros-populacionais",
    "href": "src/01_intro/02_vetores_aleatorios.html#parâmetros-populacionais",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "2.2 Parâmetros Populacionais",
    "text": "2.2 Parâmetros Populacionais\nAssim como variáveis aleatórias escalares são caracterizadas por parâmetros como a média (expectativa) e a variância, os vetores aleatórios também o são. Esses parâmetros descrevem a tendência central, a dispersão e as inter-relações das variáveis que compõem o vetor.\n\nDefinição 2.2 O vetor de médias populacional, denotado por \\(\\boldsymbol{\\mu}\\), é o vetor das expectativas de cada uma de suas variáveis componentes.\n\\[\n\\boldsymbol{\\mu} = E[\\mathbf{x}] = \\begin{pmatrix}\nE[X_1] \\\\\nE[X_2] \\\\\n\\vdots \\\\\nE[X_p]\n\\end{pmatrix} = \\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{pmatrix}\n\\]\nGeometricamente, \\(\\boldsymbol{\\mu}\\) representa o centróide (centro de massa) da distribuição de probabilidade no espaço \\(p\\)-dimensional.\n\n\nDefinição 2.3 A matriz de covariâncias populacional, denotada por \\(\\boldsymbol{\\Sigma}\\), é uma matriz simétrica \\(p \\times p\\) cujo elemento \\((j, k)\\) é a covariância entre a \\(j\\)-ésima e a \\(k\\)-ésima variável aleatória, \\(\\sigma_{jk} = \\text{Cov}(X_j, X_k) = E[(X_j - \\mu_j)(X_k - \\mu_k)]\\).\n\\[\n\\boldsymbol{\\Sigma} = \\text{Cov}[\\mathbf{x}] = E[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})'] = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp}\n\\end{pmatrix}\n\\]\n\nDiagonal (\\(\\sigma_{jj}\\)): As variâncias, \\(\\text{Var}(X_j)\\), medem a dispersão de cada variável.\nFora da Diagonal (\\(\\sigma_{jk}\\)): As covariâncias, medem a tendência de associação linear entre as variáveis \\(X_j\\) e \\(X_k\\).\nSimetria: A matriz é simétrica, pois \\(\\text{Cov}(X_j, X_k) = \\text{Cov}(X_k, X_j)\\), o que implica \\(\\sigma_{jk} = \\sigma_{kj}\\).\n\n\n\nDefinição 2.4 A matriz de correlações populacional, denotada por \\(\\mathbf{P}\\), é uma versão reescalada da matriz de covariâncias, com elementos \\(\\rho_{jk} = \\frac{\\sigma_{jk}}{\\sqrt{\\sigma_{jj}}\\sqrt{\\sigma_{kk}}}\\).\n\\[\n\\mathbf{P} = \\begin{pmatrix}\n1 & \\rho_{12} & \\cdots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\cdots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nSeus elementos \\(\\rho_{jk}\\) variam de -1 a 1, fornecendo uma medida de associação linear livre de escala.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#propriedades-de-boldsymbolmu-e-boldsymbolsigma",
    "href": "src/01_intro/02_vetores_aleatorios.html#propriedades-de-boldsymbolmu-e-boldsymbolsigma",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "2.3 Propriedades de \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)",
    "text": "2.3 Propriedades de \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)\nAs propriedades de combinações lineares são generalizações diretas dos resultados univariados. Seja \\(\\mathbf{x}\\) um vetor aleatório \\(p\\)-dimensional com média \\(\\boldsymbol{\\mu}\\) e covariância \\(\\boldsymbol{\\Sigma}\\). Sejam \\(\\mathbf{c}\\) um vetor de constantes \\(p \\times 1\\) e \\(\\mathbf{A}\\) uma matriz de constantes \\(q \\times p\\).\n\nEsperança de uma Combinação Linear: \\[\nE[\\mathbf{A}\\mathbf{x} + \\mathbf{c}] = \\mathbf{A}E[\\mathbf{x}] + \\mathbf{c} = \\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{c}\n\\]\nCovariância de uma Combinação Linear: \\[\n\\text{Cov}[\\mathbf{A}\\mathbf{x} + \\mathbf{c}] = \\mathbf{A} \\text{Cov}[\\mathbf{x}] \\mathbf{A}' = \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}'\n\\]\n\nNo próximo capítulo, veremos como, na prática, não temos acesso a esses parâmetros populacionais (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\Sigma}\\), \\(\\mathbf{P}\\)), mas podemos usar dados observados para obter estimativas confiáveis deles.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/03_amostragem_e_estimacao.html",
    "href": "src/01_intro/03_amostragem_e_estimacao.html",
    "title": "3  Amostra e Estimação de Parâmetros",
    "section": "",
    "text": "3.1 Da População à Amostra\nNo capítulo anterior, introduzimos os conceitos teóricos que descrevem uma população multivariada: o vetor de médias \\(\\boldsymbol{\\mu}\\) e a matriz de covariâncias \\(\\boldsymbol{\\Sigma}\\). Esses parâmetros são construções ideais que existem no nível populacional. Na prática, quase nunca temos acesso a toda a população para calculá-los diretamente.\nO nosso trabalho como estatísticos e analistas de dados é fazer inferências sobre esses parâmetros desconhecidos com base em um conjunto limitado de dados. Fazemos isso através da amostragem.\nAssumimos que coletamos uma amostra aleatória de \\(n\\) observações da população. Cada observação, \\(\\mathbf{x}_i\\) (com \\(i=1, \\ldots, n\\)), é uma realização independente do vetor aleatório \\(\\mathbf{x}\\) que definimos no capítulo anterior.\nA coleção de todas essas observações forma o nosso conjunto de dados. É aqui que, finalmente, introduzimos a matriz de dados, \\(\\mathbf{X}\\), uma estrutura central em toda a análise multivariada aplicada.\nA matriz \\(\\mathbf{X}\\) é uma matriz de dimensão \\(n \\times p\\), onde cada linha é uma observação multivariada e cada coluna representa uma variável.\n\\[\n\\mathbf{X} = \\begin{pmatrix}\n\\mathbf{x}_1' \\\\\n\\mathbf{x}_2' \\\\\n\\vdots \\\\\n\\mathbf{x}_n'\n\\end{pmatrix} = \\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{pmatrix}\n\\tag{3.1}\\]\nO elemento \\(x_{ij}\\) representa o valor da \\(j\\)-ésima variável para a \\(i\\)-ésima observação. Com esta matriz em mãos, nosso objetivo é calcular quantidades que sirvam como boas estimativas para os parâmetros populacionais \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Amostra e Estimação de Parâmetros</span>"
    ]
  },
  {
    "objectID": "src/01_intro/03_amostragem_e_estimacao.html#estimadores-amostrais",
    "href": "src/01_intro/03_amostragem_e_estimacao.html#estimadores-amostrais",
    "title": "3  Amostra e Estimação de Parâmetros",
    "section": "3.2 Estimadores Amostrais",
    "text": "3.2 Estimadores Amostrais\nAs quantidades que calculamos a partir da amostra são chamadas de estatísticas amostrais ou estimadores, e são as contrapartes amostrais dos parâmetros populacionais.\n\nDefinição 3.1 O estimador de \\(\\boldsymbol{\\mu}\\) é o vetor de médias amostral, \\(\\bar{\\mathbf{x}}\\), cujos componentes \\(\\bar{x}_j\\) são a média das observações para a \\(j\\)-ésima variável.\n\\[\n\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}, \\quad \\text{resultando em} \\quad \\bar{\\mathbf{x}} = \\begin{pmatrix}\n\\bar{x}_1 \\\\\n\\vdots \\\\\n\\bar{x}_p\n\\end{pmatrix}\n\\]\n\n\nDefinição 3.2 O estimador de \\(\\boldsymbol{\\Sigma}\\) é a matriz de covariâncias amostral, \\(\\mathbf{S}\\). Seus elementos são a variância amostral (\\(s_{jj}\\)) e a covariância amostral (\\(s_{jk}\\)).\n\\(s_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)\\)\nA matriz resultante é:\n\\[\n\\mathbf{S} = \\begin{pmatrix}\ns_{11} & s_{12} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\cdots & s_{pp}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nPor que dividir por \\(n-1\\)?\n\n\n\nA divisão por \\(n-1\\) (graus de liberdade) em vez de \\(n\\) é feita para garantir que \\(s_{jk}\\) seja um estimador não-viesado de \\(\\sigma_{jk}\\), ou seja, \\(E[s_{jk}] = \\sigma_{jk}\\).\n\n\n\n\nDefinição 3.3 O estimador de \\(\\mathbf{P}\\) é a matriz de correlações amostral, \\(\\mathbf{R}\\), cujos elementos \\(r_{jk}\\) são obtidos padronizando a covariância amostral.\n\\(r_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}} = \\frac{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum_{i=1}^n (x_{ik} - \\bar{x}_k)^2}}\\)\nA matriz resultante é:\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nA matriz \\(\\mathbf{R}\\) é uma matriz simétrica com 1s na diagonal.\n\n\nEm resumo: Neste capítulo, fizemos a ponte crucial entre a teoria e a prática. - No nível populacional, temos parâmetros teóricos e não observáveis (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\Sigma}\\)). - No nível amostral, temos dados observáveis na matriz \\(\\mathbf{X}\\), a partir da qual calculamos estatísticas (\\(\\bar{\\mathbf{x}}\\), \\(\\mathbf{S}\\)) que estimam esses parâmetros.\nA maior parte das técnicas que veremos neste livro opera sobre as matrizes \\(\\mathbf{S}\\) ou \\(\\mathbf{R}\\) para fazer inferências sobre a estrutura da população.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Amostra e Estimação de Parâmetros</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html",
    "href": "src/01_intro/04_normal_multivariada.html",
    "title": "4  A Distribuição Normal Multivariada",
    "section": "",
    "text": "4.1 A Função de Densidade\nAté agora, discutimos os parâmetros de um vetor aleatório (\\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)) sem assumir uma forma específica para sua distribuição de probabilidade. No entanto, para desenvolvermos uma teoria de inferência estatística robusta e compreendermos o funcionamento de muitas técnicas clássicas, precisamos de um modelo de distribuição de referência. Na análise multivariada, esse papel é desempenhado pela distribuição Normal Multivariada (NMV).\nA NMV é uma generalização da distribuição normal (Gaussiana) para o caso de \\(p\\) variáveis. Ela é, de longe, a distribuição mais importante da análise multivariada, por várias razões: 1. Muitos fenômenos naturais podem ser aproximados pela NMV. 2. O Teorema do Limite Central, em sua forma multivariada, garante que a média de vetores aleatórios de (quase) qualquer distribuição tende a se comportar como uma NMV para amostras grandes. 3. Suas propriedades matemáticas são extremamente convenientes e bem compreendidas, o que facilita muito a derivação de resultados teóricos.\nUm vetor aleatório \\(\\mathbf{x}\\) de dimensão \\(p\\) segue uma distribuição Normal Multivariada com vetor de médias \\(\\boldsymbol{\\mu}\\) e matriz de covariâncias \\(\\boldsymbol{\\Sigma}\\) (positiva definida), denotado por \\(\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), se sua função de densidade de probabilidade (FDP) for dada por:\n\\[\nf(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right)\n\\]\nOnde: - \\(|\\boldsymbol{\\Sigma}|\\) é o determinante da matriz de covariâncias. - \\(\\boldsymbol{\\Sigma}^{-1}\\) é a inversa da matriz de covariâncias.\nApesar de parecer intimidante, a estrutura da FDP é bastante lógica. O termo no expoente, \\((\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\), é uma forma quadrática que mede a “distância” do ponto \\(\\mathbf{x}\\) ao centro \\(\\boldsymbol{\\mu}\\), ponderada pela estrutura de covariância \\(\\boldsymbol{\\Sigma}\\). Essa distância é chamada de distância de Mahalanobis. Quanto maior essa distância, menor o valor da função de densidade, o que faz todo o sentido.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Distribuição Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html#propriedades-da-distribuição-normal-multivariada",
    "href": "src/01_intro/04_normal_multivariada.html#propriedades-da-distribuição-normal-multivariada",
    "title": "4  A Distribuição Normal Multivariada",
    "section": "4.2 Propriedades da Distribuição Normal Multivariada",
    "text": "4.2 Propriedades da Distribuição Normal Multivariada\nA popularidade da NMV vem de suas propriedades elegantes:\n\nCombinações Lineares: Se \\(\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), então qualquer combinação linear de suas componentes, \\(\\mathbf{a}'\\mathbf{x} = a_1X_1 + \\dots + a_pX_p\\), segue uma distribuição normal univariada. Mais geralmente, se \\(\\mathbf{A}\\) é uma matriz de constantes, então \\(\\mathbf{Ax}\\) segue uma distribuição Normal Multivariada.\nDistribuições Marginais: Qualquer subconjunto de variáveis de um vetor Normal Multivariado também segue uma distribuição Normal Multivariada. Por exemplo, se \\(\\mathbf{x} = [X_1, X_2, X_3]'\\) é NMV, então o vetor \\([X_1, X_3]'\\) também é NMV.\nIndependência e Covariância Zero: Para a maioria das distribuições, covariância zero não implica independência. No entanto, para a NMV, essa implicação é verdadeira. Se um subconjunto de variáveis em um vetor NMV tem covariância zero com outro subconjunto, então esses dois subconjuntos são independentes. Esta é uma propriedade extremamente poderosa.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Distribuição Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html#visualizando-a-normal-bivariada",
    "href": "src/01_intro/04_normal_multivariada.html#visualizando-a-normal-bivariada",
    "title": "4  A Distribuição Normal Multivariada",
    "section": "4.3 Visualizando a Normal Bivariada",
    "text": "4.3 Visualizando a Normal Bivariada\nPara ganhar intuição, é útil visualizar o caso bivariado (\\(p=2\\)). A função de densidade forma uma superfície em forma de sino no espaço 3D. Os contornos de densidade constante, quando projetados no plano \\((x_1, x_2)\\), formam elipses.\n\\[\n(\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) = c^2\n\\]\nA forma e a orientação dessas elipses são inteiramente determinadas pela matriz de covariâncias \\(\\boldsymbol{\\Sigma}\\).\n\nSe \\(\\sigma_{12} = 0\\) e \\(\\sigma_{11} = \\sigma_{22}\\): As variáveis são não correlacionadas (e, portanto, independentes) e têm a mesma variância. Os contornos são círculos.\nSe \\(\\sigma_{12} = 0\\) e \\(\\sigma_{11} \\neq \\sigma_{22}\\): As variáveis são não correlacionadas, mas com variâncias diferentes. Os contornos são elipses alinhadas com os eixos de coordenadas.\nSe \\(\\sigma_{12} \\neq 0\\): As variáveis são correlacionadas. Os contornos são elipses rotacionadas. A direção do eixo principal da elipse é determinada pelos autovetores de \\(\\boldsymbol{\\Sigma}\\), e o comprimento dos eixos é determinado pelos autovalores.\n\n\n\n\n\n\n\n\n\nG\n\ncluster_A\n\nσ₁₂ = 0, σ₁₁ = σ₂₂\n\n\ncluster_B\n\nσ₁₂ = 0, σ₁₁ &gt; σ₂₂\n\n\ncluster_C\n\nσ₁₂ &gt; 0\n\n\n\nA\n\n\n\n\nB\n\n\n\n\n\nC\n\nrotacionada\n\n\n\n\n\n\n\nFigura 4.1: Contornos de densidade para uma distribuição Normal Bivariada, ilustrando o efeito da matriz de covariância.\n\n\n\n\n\nEssa conexão entre a álgebra da matriz \\(\\boldsymbol{\\Sigma}\\) e a geometria da distribuição de dados é um dos temas mais importantes da análise multivariada e será a base para a técnica de Componentes Principais, que exploraremos mais adiante.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Distribuição Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html",
    "href": "src/01_intro/05_algebra_matricial.html",
    "title": "5  Fundamentos Matemáticos: Álgebra Matricial",
    "section": "",
    "text": "5.1 Formas Quadráticas\nCom os conceitos estatísticos fundamentais estabelecidos, voltamos nossa atenção para as ferramentas matemáticas necessárias para manipular esses objetos. A linguagem da análise multivariada é a álgebra linear.\nNeste capítulo, revisaremos conceitos-chave — formas quadráticas, matrizes positiva-definidas e a decomposição espectral — que são a base para muitas das técnicas que veremos, como a Análise de Componentes Principais (PCA).\nUma forma quadrática é uma função polinomial de várias variáveis que contém apenas termos de grau dois. Para um vetor \\(\\mathbf{x}\\) de dimensão \\(p \\times 1\\) e uma matriz simétrica \\(\\mathbf{A}\\) de dimensão \\(p \\times p\\), a forma quadrática é expressa como:\n\\[\nQ(\\mathbf{x}) = \\mathbf{x}' \\mathbf{A} \\mathbf{x} = \\sum_{i=1}^p \\sum_{j=1}^p a_{ij} x_i x_j\n\\]\nUm exemplo fundamental que já encontramos é a distância de Mahalanobis ao quadrado, \\((\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\), que aparece no expoente da distribuição normal multivariada. Esta forma quadrática define as elipses de contorno de densidade constante da distribuição.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentos Matemáticos: Álgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html#matrizes-positiva-definidas",
    "href": "src/01_intro/05_algebra_matricial.html#matrizes-positiva-definidas",
    "title": "5  Fundamentos Matemáticos: Álgebra Matricial",
    "section": "5.2 Matrizes positiva-Definidas",
    "text": "5.2 Matrizes positiva-Definidas\nO conceito de positividade para um número escalar é estendido para matrizes através das formas quadráticas. Uma matriz simétrica \\(\\mathbf{A}\\) é dita:\n\npositiva-definida se \\(\\mathbf{x}' \\mathbf{A} \\mathbf{x} &gt; 0\\) para todos os vetores não-nulos \\(\\mathbf{x}\\).\npositiva-semidefinida se \\(\\mathbf{x}' \\mathbf{A} \\mathbf{x} \\geq 0\\) para todos os vetores não-nulos \\(\\mathbf{x}\\).\n\nPropriedades de uma matriz positiva-definida: - Todos os seus autovalores são estritamente positivos (\\(\\lambda_i &gt; 0\\)). - A matriz é invertível (não-singular). - Seu determinante é positivo.\nMatrizes de covariância (\\(\\boldsymbol{\\Sigma}\\)) e correlação (\\(\\mathbf{R}\\)) são, por natureza, positiva-semidefinidas. Para que a função de densidade da normal multivariada seja bem definida e a matriz \\(\\boldsymbol{\\Sigma}\\) seja invertível, exigimos que ela seja positiva-definida. Isso implica que nenhuma variável no vetor aleatório é uma combinação linear perfeita de outras (ou seja, não há redundância linear total nos dados).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentos Matemáticos: Álgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html#sec-espectral",
    "href": "src/01_intro/05_algebra_matricial.html#sec-espectral",
    "title": "5  Fundamentos Matemáticos: Álgebra Matricial",
    "section": "5.3 Decomposição Espectral",
    "text": "5.3 Decomposição Espectral\nA decomposição espectral (ou de autovalores) é uma fatoração de uma matriz simétrica em seus autovalores e autovetores. Ela revela a estrutura fundamental da transformação linear representada pela matriz.\nToda matriz simétrica \\(\\mathbf{A}\\) de dimensão \\(p \\times p\\) pode ser reescrita como:\n\\[\n\\mathbf{A} = \\mathbf{E}\\Lambda\\mathbf{E}'\n\\]\nOnde:\n\n\\(\\lambda_1, \\dots, \\lambda_p\\) são os autovalores de \\(\\mathbf{A}\\).\n\\(\\mathbf{e}_1, \\dots, \\mathbf{e}_p\\) são os autovetores ortonormais correspondentes.\n\\(\\Lambda\\) é a matriz diagonal com os autovalores \\(\\lambda_i\\) na diagonal.\n\\(\\mathbf{E}\\) é a matriz ortogonal cujas colunas são os autovetores \\(\\mathbf{e}_i\\).\n\n\nExemplo 5.1 Vamos decompor a seguinte matriz de covariâncias \\(\\mathbf{S}\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2\n\\end{pmatrix}\n\\]\n\nAutovalores: Resolvendo a equação característica \\(\\det(\\mathbf{S} - \\lambda\\mathbf{I}) = 0\\), encontramos \\(\\lambda_1 = 3\\) e \\(\\lambda_2 = 1\\).\nAutovetores:\n\nPara \\(\\lambda_1 = 3\\): O autovetor correspondente é \\(\\mathbf{e}_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2}\n\\end{pmatrix}\\).\nPara \\(\\lambda_2 = 1\\): O autovetor correspondente é \\(\\mathbf{e}_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2}\n\\end{pmatrix}\\).\n\n\nA decomposição é \\(\\mathbf{S} = \\mathbf{E}\\Lambda\\mathbf{E}'\\), com: \\[\n\\mathbf{E} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2}\n\\end{pmatrix}, \\quad\n\\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1\n\\end{pmatrix}\n\\]\nIsso nos diz que a maior variância dos dados (igual a 3) está na direção do vetor \\((1, 1)\\), enquanto a variância na direção ortogonal \\((1, -1)\\) é menor (igual a 1).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentos Matemáticos: Álgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html",
    "href": "src/01_intro/06_distancias.html",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "",
    "text": "6.1 Distâncias vs. Dissimilaridades\nUm conceito fundamental que permeia quase todas as técnicas de análise multivariada é a medição da “proximidade” ou “distância” entre observações. Seja para agrupar dados semelhantes, classificar uma nova observação ou entender a estrutura de um conjunto de dados, essas medidas determinam uma forma quantitativa para expressar o quão perto ou longe duas observações estão uma da outra no espaço p-dimensional.\nFormalmente, uma função \\(d(\\cdot, \\cdot)\\) é considerada uma métrica de distância se satisfaz as seguintes propriedades para quaisquer pontos \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}\\):\nNo entanto, em muitos contextos práticos, utilizamos medidas que não satisfazem todas essas propriedades, mas que ainda são extremamente úteis para quantificar o quão diferentes dois objetos são. Usamos o termo mais geral medida de dissimilaridade para nos referirmos a qualquer função que indique o grau de diferença entre dois pontos, onde valores pequenos indicam semelhança e valores grandes indicam diferença.\nUm exemplo clássico de uma medida de dissimilaridade que não é uma métrica de distância estrita é a distância Euclidiana quadrática, \\(d^2(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})'(\\mathbf{x} - \\mathbf{y})\\). Ela viola a propriedade da desigualdade triangular, mas pode ser usada em algoritmos como o K-médias e o método de Ward por suas convenientes propriedades computacionais (evitar o cálculo da raiz quadrada economiza tempo).\nNas seções a seguir, apresentamos algumas das medidas de dissimilaridade e distância mais populares. A escolha da medida ideal é um campo vasto e depende fundamentalmente da natureza dos dados e do objetivo da análise.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#distâncias-vs.-dissimilaridades",
    "href": "src/01_intro/06_distancias.html#distâncias-vs.-dissimilaridades",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "",
    "text": "Não-negatividade: \\(d(\\mathbf{x}, \\mathbf{y}) \\ge 0\\)\nIdentidade: \\(d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}\\)\nSimetria: \\(d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})\\)\nDesigualdade Triangular: \\(d(\\mathbf{x}, \\mathbf{z}) \\le d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z})\\)",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#medidas-para-dados-contínuos",
    "href": "src/01_intro/06_distancias.html#medidas-para-dados-contínuos",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "6.2 Medidas para Dados Contínuos",
    "text": "6.2 Medidas para Dados Contínuos\n\nDefinição 6.1 A Distância Euclidiana é a métrica de distância mais comum e corresponde à noção intuitiva de distância em linha reta entre dois pontos.\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2} = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_j)'(\\mathbf{x}_i - \\mathbf{x}_j)}\n\\]\n\n\nDefinição 6.2 A Distância de Manhattan (ou City-Block) calcula a distância como a soma das diferenças absolutas entre as coordenadas dos pontos. É como se deslocar entre dois pontos em uma cidade, movendo-se apenas ao longo das ruas (horizontais e verticais).\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sum_{k=1}^{p} |x_{ik} - x_{jk}|\n\\]\nEsta medida é, em geral, mais robusta a outliers do que a distância Euclidiana.\n\n\nDefinição 6.3 A Distância de Minkowski é uma generalização tanto da Euclidiana quanto da de Manhattan.\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\left( \\sum_{k=1}^{p} |x_{ik} - x_{jk}|^m \\right)^{1/m}\n\\]\n\nSe \\(m=2\\), temos a distância Euclidiana.\nSe \\(m=1\\), temos a distância de Manhattan.\n\nQuanto maior o valor de \\(m\\), mais peso é dado às maiores diferenças entre as coordenadas.\n\n\n\n\n\n\n\nLimitação das Distâncias Comuns\n\n\n\nAs distâncias Euclidiana, de Manhattan e de Minkowski são sensíveis às escalas das variáveis. Se uma variável tiver uma magnitude muito maior que as outras, ela dominará o cálculo da distância. Por isso, é prática comum padronizar as variáveis (subtrair a média e dividir pelo desvio padrão) antes de calcular a matriz de distâncias.\n\n\n\nA Distância de Mahalanobis é uma medida de distância estatística que leva em conta a correlação entre as variáveis e é invariante à escala.\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_j)' \\mathbf{S}^{-1} (\\mathbf{x}_i - \\mathbf{x}_j)}\n\\]\nOnde \\(\\mathbf{S}^{-1}\\) é a inversa da matriz de covariâncias amostral. Ela mede a distância entre os pontos em unidades de desvio padrão, ajustando a contribuição de cada variável pela estrutura de covariância dos dados. Já encontramos essa forma quadrática no expoente da distribuição Normal Multivariada (Capítulo 4).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#medidas-para-variáveis-binárias",
    "href": "src/01_intro/06_distancias.html#medidas-para-variáveis-binárias",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "6.3 Medidas para Variáveis Binárias",
    "text": "6.3 Medidas para Variáveis Binárias\nQuando os dados são binários (0 ou 1), a interpretação da distância muda. A distância Euclidiana quadrática, por exemplo, simplesmente conta o número de posições em que os dois vetores discordam.\n\\[\n(x_{ij} - x_{kj})^2 =\n\\begin{cases}\n0, & \\text{se } x_{ij} = x_{kj} \\\\\n1, & \\text{se } x_{ij} \\neq x_{kj}\n\\end{cases}\n\\]\nO problema é que essa abordagem dá o mesmo peso para uma concordância de 1-1 e uma concordância de 0-0. Em muitos contextos, a ausência conjunta de uma característica (concordância 0-0) é menos informativa do que a presença conjunta (concordância 1-1).\nPara lidar com isso, podemos construir uma tabela de contingência para duas observações \\(\\mathbf{x}_i\\) e \\(\\mathbf{x}_j\\):\n\n\n\n\nObservação j = 1\nObservação j = 0\nTotal\n\n\n\n\nObs i = 1\na\nb\na+b\n\n\nObs i = 0\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\np\n\n\n\nOnde: - a: número de variáveis onde \\(x_{ik}=1\\) e \\(x_{jk}=1\\). - d: número de variáveis onde \\(x_{ik}=0\\) e \\(x_{jk}=0\\). - b e c: número de variáveis onde há discordância.\nA distância Euclidiana quadrática corresponde a \\(b+c\\).\n\nDefinição 6.4 O Coeficiente de Jaccard é uma medida de similaridade para dados binários que ignora as concordâncias 0-0.\n\\[\nJ(\\mathbf{x}_i, \\mathbf{x}_j) = \\frac{a}{a+b+c}\n\\]\nA Distância de Jaccard é a sua contraparte de dissimilaridade, definida como \\(1 - J(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\n\nDefinição 6.5 O Coeficiente de Correspondência Simples (Simple Matching Coefficient, SMC) considera tanto as presenças (1-1) quanto as ausências (0-0) como concordâncias. É útil quando a ausência de uma característica é tão informativa quanto a sua presença.\n\\[\nSMC = \\frac{a+d}{a+b+c+d}\n\\]\n\n\nDefinição 6.6 O Coeficiente de Dice (ou Sørensen-Dice) é outra medida de similaridade que, assim como Jaccard, ignora as concordâncias 0-0. No entanto, ele dá um peso maior às concordâncias 1-1.\n\\[\nDice = \\frac{2a}{2a+b+c}\n\\]\n\n\nDefinição 6.7 O Coeficiente de Russell-Rao é uma medida mais simples que calcula a proporção de presenças conjuntas em relação ao total de variáveis.\n\\[\nRR = \\frac{a}{a+b+c+d}\n\\]",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#matriz-de-distâncias",
    "href": "src/01_intro/06_distancias.html#matriz-de-distâncias",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "6.4 Matriz de Distâncias",
    "text": "6.4 Matriz de Distâncias\nUma vez escolhida a medida de dissimilaridade, é comum pré-calcular todas as distâncias entre os pares de observações e organizá-las em uma matriz de distâncias \\(\\mathbf{D}\\), de dimensão \\(n \\times n\\).\n\\[\n\\mathbf{D} =\n\\begin{pmatrix}\n0 & d(\\mathbf{x}_1, \\mathbf{x}_2) & \\cdots & d(\\mathbf{x}_1, \\mathbf{x}_n) \\\\\nd(\\mathbf{x}_2, \\mathbf{x}_1) & 0 & \\cdots & d(\\mathbf{x}_2, \\mathbf{x}_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nd(\\mathbf{x}_n, \\mathbf{x}_1) & d(\\mathbf{x}_n, \\mathbf{x}_2) & \\cdots & 0\n\\end{pmatrix}\n\\]\nEsta matriz é simétrica, ou seja \\(d(\\mathbf{x}_i, \\mathbf{x}_j) = d(\\mathbf{x}_j, \\mathbf{x}_i)\\), e possui zeros na diagonal principal. Ela serve como a entrada para muitos algoritmos de agrupamento, especialmente os hierárquicos.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html",
    "href": "src/02_tec_mult/06_acp.html",
    "title": "7  Análise de Componentes Principais",
    "section": "",
    "text": "7.1 Variância como Medida de Informação\nA Análise de Componentes Principais (ACP ou PCA do acrônimo em inglês) é uma técnica estatística multivariada que transforma um conjunto de variáveis possivelmente correlacionadas em um novo conjunto de variáveis não correlacionadas, chamadas de componentes principais. O objetivo primário da ACP é a redução de dimensionalidade: representar a variabilidade presente nos dados originais com um número menor de variáveis, minimizando a perda de informação.\nCada componente principal é uma combinação linear das variáveis originais. O primeiro componente principal é construído para capturar a maior variabilidade possível nos dados. O segundo componente principal, ortogonal ao primeiro, captura a maior parte da variabilidade restante, e assim por diante. Ao final, o número de componentes principais é igual ao número de variáveis originais, mas a expectativa é que os primeiros componentes concentrem a maior parte da informação relevante.\nGeometricamente, a ACP é uma projeção do espaço original de variáveis para um outro espaço com características mais interessantes: A variância dos dados é concentrada em direções especificas (os componentes principais) e não existem correlações entre os novos eixos.\nPara construir a intuição geométrica, vamos começar com um exemplo simples, em duas dimensões.\nA essa altura, você deve estar se perguntando: por que a direção do “maior alongamento” é a mais importante? Em estatística, a variância é frequentemente usada como uma medida de informação. Uma variável com alta variância indica que seus valores são bem espalhados, o que nos ajuda a diferenciar as observações. Se a variância fosse zero, todos os pontos seriam idênticos, não nos fornecendo nenhuma informação sobre suas diferenças.\nA ACP utiliza essa ideia para encontrar os eixos mais informativos. Ao rotacionar o sistema de coordenadas, ela não altera a variabilidade total dos dados, mas a redistribui de forma inteligente.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#variância-como-medida-de-informação",
    "href": "src/02_tec_mult/06_acp.html#variância-como-medida-de-informação",
    "title": "7  Análise de Componentes Principais",
    "section": "",
    "text": "Definição 7.1 A variância total de um conjunto de dados com \\(p\\) variáveis é a soma das variâncias de cada variável individual. Matematicamente, se \\(\\mathbf{x} = (X_1, \\dots, X_p)'\\) é o vetor de variáveis aleatórias com matriz de covariâncias \\(\\mathbf{\\Sigma}\\), a variância total é definida como:\n\\[\n\\text{Variância Total} = \\sum_{j=1}^{p} \\text{Var}(X_j) = \\sum_{j=1}^{p} \\sigma_{jj} = tr(\\mathbf{\\Sigma})\n\\]\nOnde \\(\\sigma_{jj}\\) é a variância da \\(j\\)-ésima variável e \\(tr(\\mathbf{\\Sigma})\\) é o traço da matriz de covariâncias (a soma dos elementos da diagonal principal). Essa medida representa a dispersão total na nuvem de pontos, somando a variabilidade em cada uma das direções dos eixos originais.\n\n\nExemplo 7.2 Voltando ao exemplo Exemplo 7.1, as variâncias das variáveis originais são:\n\nVariância do Peso: 59.88\nVariância da Altura: 66.71\nVariância Total Original: 126.59\n\nApós a rotação, as variâncias ao longo dos novos eixos (os componentes principais) são:\n\nVariância de \\(CP_1\\): 123.55\nVariância de \\(CP_2\\): 3.04\nVariância Total dos Componentes: 126.59\n\nDois fatos cruciais se destacam:\n\nA variância total é conservada. A soma das variâncias é a mesma nos dois sistemas de eixos. Nenhuma informação foi perdida; o ponto de vista foi apenas alterado.\nA variância foi eficientemente redistribuída. O primeiro componente, \\(CP_1\\), agora concentra 97.60% da variância total. Isso significa que, se quiséssemos reduzir nossos dados de 2D para 1D, poderíamos manter apenas o \\(CP_1\\) e ainda reter a maior parte da informação original. Essa é a essência da redução de dimensionalidade com ACP.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#a-formalização-matemática",
    "href": "src/02_tec_mult/06_acp.html#a-formalização-matemática",
    "title": "7  Análise de Componentes Principais",
    "section": "7.2 A Formalização Matemática",
    "text": "7.2 A Formalização Matemática\nCom a intuição geométrica estabelecida, podemos formalizar a Análise de Componentes Principais. O objetivo é transformar um conjunto de variáveis correlacionadas \\(\\mathbf{x} = (X_1, \\dots, X_p)'\\) em um novo conjunto de variáveis não correlacionadas, os componentes principais \\(\\mathbf{y} = (Y_1, \\dots, Y_p)'\\). Cada componente é uma combinação linear das variáveis originais:\n\\[\n\\begin{aligned}\nY_1 &= e_{11}X_1 + e_{12}X_2 + \\dots + e_{1p}X_p = \\mathbf{e}_1' \\mathbf{x} \\\\\nY_2 &= e_{21}X_1 + e_{22}X_2 + \\dots + e_{2p}X_p = \\mathbf{e}_2' \\mathbf{x} \\\\\n&\\vdots \\\\\nY_p &= e_{p1}X_1 + e_{p2}X_2 + \\dots + e_{pp}X_p = \\mathbf{e}_p' \\mathbf{x}\n\\end{aligned}\n\\]\nEm notação matricial, a transformação pode ser escrita de forma compacta:\n\\[\n\\mathbf{Y} = \\mathbf{E}' \\mathbf{X}\n\\tag{7.1}\\]\nOnde \\(\\mathbf{Y}\\) é o vetor \\(p \\times 1\\) de autovalores e \\(\\mathbf{E}\\) é a matriz \\(p \\times p\\) cujas colunas são os vetores de coeficientes \\(\\mathbf{e}_k\\).\nEsses componentes são construídos para satisfazer duas condições fundamentais:\n\nVariâncias Ordenadas: A variância do primeiro componente é a maior possível, a do segundo é a maior possível entre as direções não correlacionadas com o primeiro, e assim por diante. Ou seja, \\(\\text{Var}(Y_1) \\ge \\text{Var}(Y_2) \\ge \\dots \\ge \\text{Var}(Y_p)\\).\nNão Correlacionados: Os componentes são ortogonais entre si, o que significa que \\(\\text{Cov}[Y_i, Y_k] = 0\\) para todo \\(i \\neq k\\).\n\n\n7.2.1 O Problema de Maximização\nO primeiro componente principal, \\(Y_1 = \\mathbf{e}_1'\\mathbf{x}\\), é a combinação linear com variância máxima. A variância de \\(Y_1\\) é dada por:\n\\[\n\\text{Var}(Y_1) = \\text{Var}(\\mathbf{e}_1'\\mathbf{x}) = \\mathbf{e}_1' \\text{Var}(\\mathbf{x}) \\mathbf{e}_1 = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1\n\\]\nOnde \\(\\mathbf{\\Sigma}\\) é a matriz de covariâncias de \\(\\mathbf{x}\\). Para evitar que a variância seja aumentada simplesmente inflando os coeficientes em \\(\\mathbf{e}_1\\), impomos a restrição de que seu comprimento seja unitário, \\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\). Formalmente, o problema de maximização para o primeiro componente principal se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_1} \\quad & \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 \\\\\n    \\text{sujeito a} \\quad & \\mathbf{e}_1' \\mathbf{e}_1 = 1\n\\end{aligned}\n\\]\nPara maximizar a variância sujeita à restrição, utilizamos o método dos multiplicadores de Lagrange. A função a ser maximizada é:\n\\[\nL(\\mathbf{e}_1, \\lambda_1) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 - \\lambda_1 (\\mathbf{e}_1' \\mathbf{e}_1 - 1)\n\\]\nDerivando em relação a \\(\\mathbf{e}_1\\) e igualando a zero, obtemos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_1} = 2 \\mathbf{\\Sigma} \\mathbf{e}_1 - 2 \\lambda_1 \\mathbf{e}_1 = 0\n\\]\nO que nos leva à equação fundamental de autovalores e autovetores:\n\\[\n\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\n\\]\nEsta equação mostra que o vetor de coeficientes \\(\\mathbf{e}_1\\) deve ser um autovetor da matriz de covariâncias \\(\\mathbf{\\Sigma}\\). Para encontrar a variância, pré-multiplicamos a equação por \\(\\mathbf{e}_1'\\):\n\\[\n\\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1' \\mathbf{e}_1\n\\]\nComo \\(\\text{Var}(Y_1) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1\\) e a restrição é \\(\\mathbf{e}_1' \\mathbf{e}_1 = 1\\), temos:\n\\[\n\\text{Var}(Y_1) = \\lambda_1\n\\]\nPara maximizar a variância de \\(Y_1\\), devemos escolher o maior autovalor possível. Portanto, \\(\\lambda_1\\) é o maior autovalor de \\(\\mathbf{\\Sigma}\\), e \\(\\mathbf{e}_1\\) é o autovetor correspondente.\n\n\n\n\n\n\nNota\n\n\n\nA matriz de covariâncias \\(\\mathbf{\\Sigma}\\) é, por construção, uma matriz simétrica e positiva semi-definida. Conforme discutido em Seção 5.3, o Teorema Espectral garante que os autovalores de tal matriz são reais e não-negativos, e que seus autovetores correspondentes a autovalores distintos são ortogonais. Esta propriedade é fundamental para a existência e unicidade dos componentes principais.\n\n\n\n\n\n\n\n\nNota\n\n\n\nA demonstração acima, utilizando multiplicadores de Lagrange, é uma maneira moderna e elegante de conduzir a derivação do problema de máximização. Uma abordagem clássica restringe a norma de \\(e\\) através do quociente,\n\\[\n\\text{Var}(Y_1) = \\max_{e_1} \\frac{\\mathbf{e}_1' \\Sigma \\mathbf{e}_1}{\\mathbf{e}_1' \\mathbf{e_1}}\n\\]\nEste é um problema clássico na álgebra linear. Um teorema fundamental afirma que para qualquer matriz simétrica \\(A\\), o máximo da forma quadrática \\(\\mathbf{x}' A \\mathbf{x}\\), sujeito à restrição \\(\\mathbf{x}' \\mathbf{x} = 1\\), é o maior autovalor de \\(A\\). O vetor \\(\\mathbf{x}\\) que atinge esse máximo é o autovetor correspondente. Como a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) é simétrica, este teorema se aplica diretamente ao nosso problema.\n\n\n\n\n7.2.2 Componentes Subsequentes\nUma vez encontrada a primeira direção de máxima variância, o segundo componente principal, \\(Y_2 = \\mathbf{e}_2'\\mathbf{x}\\), busca capturar o máximo da variabilidade restante, sob a condição de ser não correlacionado com \\(Y_1\\). A condição de componentes não correlacionados garante que a informação presente no segundo componente principal não é redundante com relação aquela já presente no primeiro. Formalmente, temos:\n\\[\nCov(Y_1, Y_2) = Cov(\\mathbf{e}'_1 \\mathbf{x}, \\mathbf{e}'_2 \\mathbf{x}) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_2\n\\]\nComo \\(\\mathbf{e}_1\\) é o primeiro autovetor, temos \\(\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\\), Assim:\n\\[\n\\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_2 = (\\lambda_1 \\mathbf{e}_1)' \\mathbf{e}_2 = \\lambda_1 \\mathbf{e}_1' \\mathbf{e}_2\n\\]\nLogo:\n\\[\nCov(Y_1, Y_2) = 0 \\iff \\mathbf{e}_1' \\mathbf{e}_2 = 0\n\\]\nCom essa condição bem definida, o problema para o segundo componente se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_2} \\quad & \\mathbf{e}_2' \\mathbf{\\Sigma} \\mathbf{e}_2 \\\\\n    \\text{sujeito a} \\quad & \\begin{cases}\n        \\mathbf{e}_2' \\mathbf{e}_2 = 1 \\\\\n        \\mathbf{e}_1' \\mathbf{e}_2 = 0\n    \\end{cases}\n\\end{aligned}\n\\]\nA função Lagrangiana agora inclui dois multiplicadores, \\(\\lambda_2\\) e \\(\\phi\\):\n\\[\nL(\\mathbf{e}_2, \\lambda_2, \\phi) = \\mathbf{e}_2' \\mathbf{\\Sigma} \\mathbf{e}_2 - \\lambda_2(\\mathbf{e}_2' \\mathbf{e}_2 - 1) - \\phi(\\mathbf{e}_1' \\mathbf{e}_2 - 0)\n\\]\nDerivando em relação a \\(\\mathbf{e}_2\\) e igualando a zero, temos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_2} = 2\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_2 - \\phi\\mathbf{e}_1 = \\mathbf{0}\n\\]\nPré-multiplicando por \\(\\mathbf{e}_1'\\):\n\\[\n2\\mathbf{e}_1'\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_1'\\mathbf{e}_2 - \\phi\\mathbf{e}_1'\\mathbf{e}_1 = 0\n\\]\nSabendo que:\n\n\\(\\mathbf{e}_1'\\mathbf{\\Sigma} = \\lambda_1\\mathbf{e}_1'\\)\n\\(\\mathbf{e}_1'\\mathbf{e}_2 = 0\\)\n\\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\)\n\nA equação se simplifica a \\(\\phi = 0\\). Substituindo \\(\\phi=0\\) de volta na derivada, a equação se torna:\n\\[\n\\mathbf{\\Sigma}\\mathbf{e}_2 = \\lambda_2\\mathbf{e}_2\n\\]\nAssim, \\(\\mathbf{e}_2\\) é o autovetor de \\(\\mathbf{\\Sigma}\\) correspondente ao autovalor \\(\\lambda_2\\). Como \\(\\lambda_1\\) foi o maior autovalor, para maximizar a variância de \\(Y_2\\), \\(\\lambda_2\\) deve ser o segundo maior autovalor. Este processo se generaliza para os componentes subsequentes.\nEste processo continua: o \\(k\\)-ésimo componente principal (\\(Y_k\\)) é definido pelo autovetor \\(\\mathbf{e}_k\\) associado ao \\(k\\)-ésimo maior autovalor \\(\\lambda_k\\), garantindo que \\(\\text{Var}(Y_k) = \\lambda_k\\) e que todos os componentes sejam mutuamente não correlacionados.\nÉ neste ponto que a conexão com a Seção 5.3 se torna explícita. A matriz \\(\\mathbf{P}\\) (Equação 7.1), cujas colunas são os autovetores da matriz de covariâncias \\(\\mathbf{\\Sigma}\\), é exatamente a mesma matriz \\(\\mathbf{P}\\) da decomposição espectral \\(\\mathbf{\\Sigma} = \\mathbf{P}\\Lambda\\mathbf{P}'\\). Além disso, \\(\\Lambda\\) é uma matriz diagonal contendo a variância de cada componente principal. Logo, podemos obter todos os componentes principais de maneira prática e simultânea através da decomposição espectral.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#a-importância-do-pré-processamento-dos-dados",
    "href": "src/02_tec_mult/06_acp.html#a-importância-do-pré-processamento-dos-dados",
    "title": "7  Análise de Componentes Principais",
    "section": "7.3 A Importância do Pré-processamento dos Dados",
    "text": "7.3 A Importância do Pré-processamento dos Dados\nA Análise de Componentes Principais é, em sua essência, uma análise de variabilidade. A forma como medimos essa variabilidade impacta diretamente o resultado. Dois pré-processamentos são cruciais: a centralização e o escalonamento.\n\n7.3.1 Centralização\nNa Análise de Componentes Principais (ACP), a centralização dos dados — ou seja, a subtração da média de cada variável — é uma etapa fundamental não apenas para o cálculo da matriz de covariâncias, mas também para a projeção dos dados nos componentes principais.\nAo projetar os dados em um componente \\(\\mathbf{e}_i\\), é imprescindível que a projeção seja feita a partir dos dados centralizados, ou seja:\n\\[\nY_i = \\mathbf{e}_i^𝑇(\\mathbf{x} − \\bar{\\mathbf{{x}}})\n\\]\nEsse detalhe é essencial porque os autovetores da ACP são obtidos com base na matriz de covariâncias, a qual descreve a dispersão dos dados em torno da média, e não em torno da origem. Se aplicarmos a projeção diretamente sobre \\(\\mathbf{x}\\), sem subtrair a média, os componentes resultantes não representarão adequadamente as direções de maior variabilidade — e sim uma combinação da dispersão com a posição média dos dados.\nPortanto, para que os componentes principais preservem a interpretação correta como combinações lineares que explicam a variância dos dados em torno do centro da nuvem de pontos, é indispensável que tanto o cálculo da matriz de covariâncias quanto a projeção dos dados utilizem os dados centralizados.\n\n\n\n\n\n\nImportante\n\n\n\nNo contexto de ACP, é comum e prático denotar por \\(\\mathbf{x}\\) o vetor de variáveis já centralizado. Utilizamos esse abuso de notação durante esse capítulo para simplificação do texto sem perda de generalidade.\n\n\n\n\n7.3.2 Por que Escalonar? O Dilema da Covariância vs. Correlação\nA Análise de Componentes Principais (ACP) é sensível à escala das variáveis. Se uma variável tiver uma variância numericamente muito maior que as outras — mesmo que apenas por causa da sua unidade de medida — ela poderá dominar os primeiros componentes principais.\nImagine incluir uma terceira variável no conjunto Altura/Peso: a renda mensal, medida em Reais. As variâncias poderiam ser aproximadamente:\n\nAltura: 80 cm²\n\nPeso: 60 kg²\n\nRenda: 4.000.000 (RS)²\n\nNesse cenário, a variância da Renda é milhares de vezes maior que a das outras variáveis. Se aplicarmos a ACP diretamente na matriz de covariâncias, o primeiro componente principal será fortemente direcionado pela Renda, mesmo que sua correlação com as demais variáveis seja baixa. Isso ocorre porque a ACP estará apenas “seguindo” a direção da variável com maior variância — não necessariamente a mais informativa.\nPara evitar esse viés, escalonamos as variáveis: cada uma é dividida por seu desvio padrão. Isso padroniza todas para variância igual a 1. Ao fazer isso, estamos na prática realizando a ACP sobre a matriz de correlação \\((\\mathbf{R})\\) em vez da matriz de covariâncias \\((\\mathbf{\\Sigma})\\).\nVantagem do escalonamento:\nUsar a matriz de correlação “democratiza” a análise. Todas as variáveis começam com a mesma importância inicial (variância 1), e a ACP passa a capturar a estrutura de correlações, ao invés de ser enviesada pelas diferenças de escala.\nQuando usar a matriz de covariâncias?\nSomente quando todas as variáveis estão na mesma unidade de medida e possuem uma interpretação comparável. Por exemplo, comparar a temperatura em Celsius em diferentes regiões pode fazer sentido sem escalonamento. Fora isso, a matriz de correlação é geralmente a escolha mais robusta e segura.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "href": "src/02_tec_mult/06_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "title": "7  Análise de Componentes Principais",
    "section": "7.4 Componentes Principais Populacionais vs. Amostrais",
    "text": "7.4 Componentes Principais Populacionais vs. Amostrais\nAté este ponto, discutimos os componentes principais em um contexto populacional, onde a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) (ou correlação \\(\\mathbf{R}\\)) e seus autovalores \\(\\lambda_k\\) e autovetores \\(\\mathbf{e}_k\\) são conhecidos. Na prática, quase sempre trabalhamos com uma amostra de dados. Nesse caso, não conhecemos os verdadeiros parâmetros populacionais e devemos estimá-los.\nOs componentes principais amostrais são obtidos da mesma maneira, mas usando a matriz de covariâncias amostral \\(\\mathbf{S}\\) (ou a matriz de correlação amostral \\(\\mathbf{R}\\)). As quantidades resultantes são estimativas dos seus análogos populacionais:\n\nO \\(k\\)-ésimo autovalor amostral, \\(\\hat{\\lambda}_k\\), é uma estimativa de \\(\\lambda_k\\).\nO \\(k\\)-ésimo autovetor amostral, \\(\\hat{\\mathbf{e}}_k\\), é uma estimativa de \\(\\mathbf{e}_k\\).\nO \\(k\\)-ésimo componente principal amostral, \\(\\hat{Y}_k = \\hat{\\mathbf{e}}_k' \\mathbf{x}\\), é uma estimativa de \\(Y_k\\).\n\nA teoria e a interpretação permanecem as mesmas. Para simplificar a notação, ao longo deste capítulo, omitimos o acento circunflexo (\\(\\hat{\\phantom{a}}\\)), mas é importante lembrar que, na aplicação prática, estamos sempre lidando com estimativas amostrais.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#escolhendo-o-número-de-componentes",
    "href": "src/02_tec_mult/06_acp.html#escolhendo-o-número-de-componentes",
    "title": "7  Análise de Componentes Principais",
    "section": "7.5 Escolhendo o Número de Componentes",
    "text": "7.5 Escolhendo o Número de Componentes\nA principal vantagem da ACP é a redução de dimensionalidade. Mas como decidimos quantos componentes (\\(q &lt; p\\)) reter? A escolha de \\(q\\) envolve um trade-off entre a simplicidade (poucos componentes) e a fidelidade aos dados originais (muitos componentes). Não existe uma regra única, mas sim um conjunto de critérios que devem ser avaliados em conjunto.\n\n7.5.1 Critério da Variância Explicada Acumulada\nEste é o critério mais comum. Calculamos a proporção da variância total explicada por cada componente e acumulamos essa proporção.\n\\[\n\\text{Proporção da Variância por } CP_k = \\frac{\\lambda_k}{\\sum_{j=1}^{p} \\lambda_j}\n\\]\nEm seguida, escolhemos o menor número de componentes \\(q\\) cuja variância explicada acumulada atinja um limiar satisfatório, geralmente entre 70% e 90%. A escolha do limiar depende do contexto da análise.\n\n\n7.5.2 Critério do Autovalor (Critério de Kaiser)\nProposto por Henry Kaiser, este critério sugere reter apenas os componentes cujos autovalores (\\(\\lambda_k\\)) são maiores que 1. A intuição por trás dessa regra é mais clara quando a ACP é aplicada sobre a matriz de correlação. Nesse caso, as variáveis originais são padronizadas para ter variância 1. Um componente com autovalor (variância) menor que 1 está, portanto, explicando menos variabilidade do que uma única variável original. Reter tal componente não traria uma “economia” de informação”, tornando-o um candidato à exclusão.\n\n\n7.5.3 Scree Plot (Gráfico de Cotovelo)\nO Scree Plot, proposto por Raymond Cattell, é uma ferramenta visual que nos ajuda a identificar o número ideal de componentes. Ele é um gráfico de linha dos autovalores (variâncias dos componentes) em ordem decrescente.\nTipicamente, o gráfico mostra uma queda acentuada nos primeiros autovalores, seguida por um nivelamento gradual para os autovalores restantes. O ponto onde a curva “dobra” ou forma um “cotovelo” (elbow) é considerado o ponto de corte. A ideia é reter os componentes que aparecem antes do cotovelo, pois eles são os que contribuem mais significativamente para a variância total. Os componentes após o cotovelo formam o “cascalho” (scree) na base de uma montanha e são considerados “ruído”.\n\n\n\n\n\n\n\n\nFigura 7.3: Exemplo de um Scree Plot. O ‘cotovelo’ em k=3 sugere a retenção de 3 componentes.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#interpretando-os-componentes-principais",
    "href": "src/02_tec_mult/06_acp.html#interpretando-os-componentes-principais",
    "title": "7  Análise de Componentes Principais",
    "section": "7.6 Interpretando os Componentes Principais",
    "text": "7.6 Interpretando os Componentes Principais\nUma vez que selecionamos o número de componentes a reter, o passo final é a interpretação. O que esses novos eixos, que são combinações de nossas variáveis originais, realmente significam?\nOs coeficientes \\(e_{kj}\\) do autovetor \\(\\mathbf{e}_k\\) são chamados de cargas (loadings) e representam o peso da variável original \\(X_j\\) na formação do componente \\(Y_k\\). Embora as cargas sejam importantes, a sua interpretação pode ser complicada, pois sua magnitude depende das unidades das variáveis originais.\nUma medida mais interpretável é a correlação entre os componentes principais e as variáveis originais, \\(Cor(Y_k, X_j)\\). Ela nos diz o quão “alinhado” um componente está com cada variável original, numa escala padronizada de -1 a 1. A fórmula para essa correlação é:\n\\[\nCor(Y_k, X_j) = \\frac{e_{kj} \\sqrt{\\lambda_k}}{\\sqrt{s_{jj}}}\n\\]\nOnde:\n\n\\(e_{kj}\\) é a carga da variável \\(j\\) no componente \\(k\\).\n\\(\\lambda_k\\) é o autovalor (variância) do componente \\(k\\).\n\\(s_{jj}\\) é a variância da variável original \\(j\\).\n\nQuando a ACP é realizada sobre a matriz de correlação (ou seja, com dados padronizados), as variâncias \\(s_{jj}\\) são todas iguais a 1. Nesse caso, a fórmula simplifica para \\(Cor(Y_k, X_j) = e_{kj} \\sqrt{\\lambda_k}\\). As correlações se tornam proporcionais às cargas, facilitando a interpretação.\nAlém disso, quando a ACP é realizada sobre a matriz de correlações, as variáveis são padronizadas. Nesse caso, uma opção comum e direta é avaliar os próprios loadings (os autovetores da matriz de correlação) para entender a contribuição de cada variável. Um loading alto (próximo de 1 ou -1) indica que a variável tem uma forte influência na construção daquele componente.\nA etapa mais crucial da ACP é transformar os eixos matemáticos (os componentes) em descobertas práticos. A ferramenta visual mais adequada para essa tarefa é o biplot. O termo “biplot” significa “dois plots” (plot duplo), pois ele sobrepõe duas informações em um único gráfico:\n\nOs scores: As coordenadas das observações no novo espaço dos componentes principais.\nOs loadings: As contribuições das variáveis originais para a criação desses componentes.\n\nO resultado é um mapa rico que mostra não apenas como as observações se agrupam, mas por que elas se agrupam daquela maneira. A interpretação de um biplot segue uma lógica visual. Vamos quebrar em partes:\n\nEixos (Componentes Principais): O eixo horizontal é o CP1 e o vertical é o CP2. Eles são as “réguas” do nosso novo mapa e representam as direções de maior variabilidade nos dados. A porcentagem de variância que cada um explica é mostrada nos seus rótulos.\nPontos (Observações): Cada ponto no gráfico é uma observação.\n\nProximidade: Pontos próximos uns dos outros representam observações com perfis semelhantes (conforme capturado pelos dois primeiros CPs).\nAgrupamentos: Grupos de pontos (clusters) indicam subpopulações nos dados.\n\nVetores (Variáveis Originais): Cada seta (vetor) representa uma das variáveis originais.\n\nDireção: A direção da seta indica como a variável contribui para os dois componentes. Uma seta que aponta para a direita indica uma forte contribuição positiva para o CP1. Uma que aponta para cima, uma forte contribuição positiva para o CP2.\nComprimento: O comprimento da seta é proporcional a quão bem a variável é representada no espaço 2D do biplot. Setas mais longas significam que a variável tem uma forte influência nos componentes mostrados e é bem representada no gráfico. Setas curtas são menos importantes para os dois primeiros CPs ou sua variabilidade está melhor explicada em outros componentes (CP3, CP4, etc.).\nRelações entre Variáveis: O ângulo entre os vetores nos informa sobre a correlação entre as variáveis originais.\n\nÂngulo pequeno (&lt; 90°): As variáveis são positivamente correlacionadas.\nÂngulo de ~90°: As variáveis não são correlacionadas.\nÂngulo obtuso (&gt; 90°): As variáveis são negativamente correlacionadas.\n\n\nRelação entre Pontos e Vetores: Para entender o perfil de um ponto (ou grupo de pontos), projete-o ortogonalmente sobre os vetores das variáveis. Se a projeção de um ponto cai na direção de um vetor, aquela observação tem um valor alto para aquela variável. Se cai na direção oposta, tem um valor baixo.\n\nCom essas regras em mente, vamos analisar um biplot genérico.\n\n\n\n\n\n\n\n\nFigura 7.4: Exemplo de um biplot genérico para ilustrar a interpretação dos seus elementos. Os pontos representam as observações e as setas, as variáveis originais.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html",
    "href": "src/02_tec_mult/07_af.html",
    "title": "8  Análise Fatorial",
    "section": "",
    "text": "8.1 O Modelo Fatorial Ortogonal\nA Análise Fatorial é uma técnica estatística utilizada para descrever a estrutura de covariância entre um conjunto de variáveis observadas. A hipótese central é que essa estrutura é gerada por um número menor de variáveis latentes não observáveis, denominadas fatores comuns.\nComeçamos com uma intuição. Suponha que temos as seguintes variáveis de gastos para diferentes famílias:\nÉ razoável supor que essas variáveis sejam correlacionadas. Mais do que isso, pode existir um fator latente, como a renda familiar (\\(F_1\\)), que influencia todos esses gastos. A Análise Fatorial busca formalizar e quantificar essa relação.\nO modelo supõe que cada variável observada é linearmente dependente de um conjunto de fatores comuns, somado a um termo de variância individual, ou específico.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#o-modelo-fatorial-ortogonal",
    "href": "src/02_tec_mult/07_af.html#o-modelo-fatorial-ortogonal",
    "title": "8  Análise Fatorial",
    "section": "",
    "text": "Definição 8.1 Seja \\(\\mathbf{x}\\) um vetor aleatório de p variáveis observadas com vetor de médias \\(\\boldsymbol{\\mu}\\) e matriz de covariâncias \\(\\mathbf{\\Sigma}\\). O modelo fatorial com m fatores comuns (\\(m &lt; p\\)) postula que \\(\\mathbf{x}\\) é linearmente dependente de m fatores comuns \\(F_1, F_2, \\dots, F_m\\) e p termos de erro \\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p\\). Em notação matricial, o modelo é:\n\\[\n\\mathbf{x}_{(p \\times 1)} - \\boldsymbol{\\mu}_{(p \\times 1)} = \\mathbf{L}_{(p \\times m)}\\mathbf{F}_{(m \\times 1)} + \\boldsymbol{\\epsilon}_{(p \\times 1)}\n\\tag{8.1}\\]\nOnde:\n\n\\(\\mathbf{L}\\) é a matriz de cargas fatoriais: Uma matriz de pesos responsável por quantificar as relações entre as p variáveis e os m fatores.\n\\(\\mathbf{F}\\) é o vetor de fatores comuns, ou seja \\(\\mathbf{F} = [F_1, F_2, \\dots, F_m]'\\).\n\\(\\boldsymbol{\\epsilon}\\) é o vetor de erros, ou variâncias específicas, ou seja, \\(\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p]'\\)\n\nPara que o ajuste desse modelo seja factível, as seguintes suposições são feitas para o modelo ortogonal:\n\n\\(E[\\mathbf{F}] = \\mathbf{0}\\) e \\(Cov(\\mathbf{F}) = E[\\mathbf{F}\\mathbf{F}'] = \\mathbf{I}\\).\n\\(E[\\boldsymbol{\\epsilon}] = \\mathbf{0}\\) e \\(Cov(\\boldsymbol{\\epsilon}) = E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] = \\mathbf{\\Psi}\\), onde \\(\\mathbf{\\Psi}\\) é uma matriz diagonal.\n\\(Cov(\\mathbf{F}, \\boldsymbol{\\epsilon}) = E[\\mathbf{F}\\boldsymbol{\\epsilon}'] = \\mathbf{0}\\).",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#a-estrutura-de-covariância-implícita",
    "href": "src/02_tec_mult/07_af.html#a-estrutura-de-covariância-implícita",
    "title": "8  Análise Fatorial",
    "section": "8.2 A Estrutura de Covariância Implícita",
    "text": "8.2 A Estrutura de Covariância Implícita\nAs suposições do modelo implicam uma estrutura específica para a matriz de covariâncias \\(\\mathbf{\\Sigma}\\).\n\nTeorema 8.1 Sob as premissas do modelo fatorial ortogonal (Definição 8.1), a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) do vetor \\(\\mathbf{x}\\) é dada por:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\n\\tag{8.2}\\]\n\n\nComprovação. A partir do modelo fatorial em Equação 8.1, temos que \\(\\mathbf{x} - \\boldsymbol{\\mu} = \\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon}\\). A matriz de covariâncias de \\(\\mathbf{x}\\) é, por definição, \\(\\mathbf{\\Sigma} = E[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})']\\). Substituindo a expressão do modelo, obtemos:\n\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= E[(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})'] \\\\\n&= E[(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})(\\mathbf{F}'\\mathbf{L}' + \\boldsymbol{\\epsilon}')] \\\\\n&= E[\\mathbf{L}\\mathbf{F}\\mathbf{F}'\\mathbf{L}' + \\mathbf{L}\\mathbf{F}\\boldsymbol{\\epsilon}' + \\boldsymbol{\\epsilon}\\mathbf{F}'\\mathbf{L}' + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] \\\\\n&= \\mathbf{L}E[\\mathbf{F}\\mathbf{F}']\\mathbf{L}' + \\mathbf{L}E[\\mathbf{F}\\boldsymbol{\\epsilon}'] + E[\\boldsymbol{\\epsilon}\\mathbf{F}']\\mathbf{L}' + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}']\n\\end{aligned}\n\\]\nPelas suposições do modelo ortogonal (Definição 8.1):\n\n\\(E[\\mathbf{F}\\mathbf{F}'] = \\text{Cov}(\\mathbf{F}) = \\mathbf{I}\\) (os fatores são não correlacionados e têm variância unitária).\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] = \\text{Cov}(\\boldsymbol{\\epsilon}) = \\mathbf{\\Psi}\\) (os erros são não correlacionados entre si).\n\\(E[\\mathbf{F}\\boldsymbol{\\epsilon}'] = \\text{Cov}(\\mathbf{F}, \\boldsymbol{\\epsilon}) = \\mathbf{0}\\) (os fatores e os erros são não correlacionados).\n\nSubstituindo essas esperanças na equação de \\(\\mathbf{\\Sigma}\\), temos:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{I}\\mathbf{L}' + \\mathbf{L}\\mathbf{0} + \\mathbf{0}\\mathbf{L}' + \\mathbf{\\Psi} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\n\\]\nIsso completa a prova.\n\nEsta equação decompõe a variância de cada variável \\(X_i\\) em:\n\nComunalidade (\\(h_i^2\\)): A porção da variância de \\(X_i\\) explicada pelos m fatores comuns (\\(h_i^2 = \\sum_{j=1}^{m} l_{ij}^2\\)).\nVariância Específica (\\(\\psi_i\\)): A porção da variância de \\(X_i\\) não explicada pelos fatores comuns (\\(Var(X_i) = \\sigma_{ii} = h_i^2 + \\psi_i\\)).\n\n\nExemplo 8.1 Suponha que a matriz de covariâncias de um vetor aleatório \\(\\mathbf{x}\\) com \\(p=4\\) variáveis seja:\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n19 & 30 & 2 & 12 \\\\\n30 & 57 & 5 & 23 \\\\\n2 & 5 & 37 & 47 \\\\\n12 & 23 & 47 & 68\n\\end{pmatrix}\n\\]\nÉ possível mostrar que um modelo fatorial com \\(m=2\\) fatores comuns pode gerar essa estrutura de covariância. Uma solução possível para \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) é dada por:\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n4 & 1 \\\\\n7 & 2 \\\\\n-1 & 6 \\\\\n1 & 8\n\\end{pmatrix}\n,\n\\quad\n\\mathbf{\\Psi} =\n\\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n0 & 4 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{pmatrix}\n\\]\nO leitor pode verificar que \\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\). O modelo decompõe a variância de cada variável.\n\nPara \\(X_1\\), a comunalidade é \\(h_1^2 = 4^2 + 1^2 = 17\\), e sua variância total é \\(Var(X_1) = \\sigma_{11} = h_1^2 + \\psi_1 = 17 + 2 = 19\\).\nPara \\(X_2\\), a comunalidade é \\(h_2^2 = 7^2 + 2^2 = 53\\), e sua variância total é \\(Var(X_2) = \\sigma_{22} = h_2^2 + \\psi_2 = 53 + 4 = 57\\).",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#problemas-no-modelo-fatorial",
    "href": "src/02_tec_mult/07_af.html#problemas-no-modelo-fatorial",
    "title": "8  Análise Fatorial",
    "section": "8.3 Problemas no Modelo Fatorial",
    "text": "8.3 Problemas no Modelo Fatorial\n\nExistência da Solução: Nem sempre existe uma solução factível para o modelo fatorial com m fatores. A estimação dos parâmetros, especialmente com um número inadequado de fatores, pode levar a soluções impróprias, como uma variância específica negativa (\\(\\hat{\\psi}_i &lt; 0\\)), conhecida como caso de Heywood. Isso viola a premissa de que \\(\\psi_i\\) é uma variância e, portanto, deve ser não-negativa. Geralmente, uma solução imprópria indica que o modelo é inadequado para os dados.\n\n\nExemplo 8.2 Considere um modelo de um fator (\\(m=1\\)) para \\(p=3\\) variáveis, com a seguinte matriz de correlação populacional:\n\\[\n\\mathbf{R} =\n\\begin{pmatrix}\n1.0 & 0.4 & 0.9 \\\\\n0.4 & 1.0 & 0.7 \\\\\n0.9 & 0.7 & 1.0\n\\end{pmatrix}\n\\]\nO modelo fatorial para a matriz de correlação é \\(\\mathbf{P} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\). Para \\(m=1\\), as cargas são um vetor \\(\\mathbf{L} = [l_{11}, l_{21}, l_{31}]'\\). As covariâncias (correlações) são dadas por \\(\\rho_{ij} = l_{i1}l_{j1}\\). Temos o sistema:\n\n\\(\\rho_{12} = l_{11}l_{21} = 0.4\\)\n\\(\\rho_{13} = l_{11}l_{31} = 0.9\\)\n\\(\\rho_{23} = l_{21}l_{31} = 0.7\\)\n\nMultiplicando as três equações, obtemos \\((l_{11}l_{21}l_{31})^2 = 0.4 \\times 0.9 \\times 0.7 = 0.252\\). Isso nos permite resolver para as cargas:\n\n\\(l_{11}^2 = (l_{11}l_{21})(l_{11}l_{31}) / (l_{21}l_{31}) = (0.4 \\times 0.9) / 0.7 \\approx 0.514\\)\n\\(l_{21}^2 = (l_{11}l_{21})(l_{21}l_{31}) / (l_{11}l_{31}) = (0.4 \\times 0.7) / 0.9 \\approx 0.311\\)\n\\(l_{31}^2 = (l_{11}l_{31})(l_{21}l_{31}) / (l_{11}l_{21}) = (0.9 \\times 0.7) / 0.4 = 1.575\\)\n\nA comunalidade da terceira variável é \\(h_3^2 = l_{31}^2 = 1.575\\). Como estamos modelando uma matriz de correlação, a variância total de cada variável é 1. A variância específica seria \\(\\psi_3 = 1 - h_3^2 = 1 - 1.575 = -0.575\\). Uma variância negativa é impossível, indicando que o modelo de um fator não é apropriado para descrever a estrutura de correlação dada.\n\n\nIndeterminação da Solução (Rotação Fatorial): A solução para a matriz de cargas \\(\\mathbf{L}\\) não é única. Para qualquer matriz ortogonal \\(\\mathbf{T}\\) de dimensão \\(m \\times m\\) (ou seja, uma matriz tal que \\(\\mathbf{T}\\mathbf{T}' = \\mathbf{T}'\\mathbf{T} = \\mathbf{I}\\)), podemos definir uma nova matriz de cargas \\(\\mathbf{L}^* = \\mathbf{L}\\mathbf{T}\\) que resulta na mesma matriz de covariâncias.\n\nIsso ocorre porque a parte da covariância explicada pelos fatores, \\(\\mathbf{L}\\mathbf{L}'\\), permanece inalterada:\n\\[\n\\mathbf{L}^*(\\mathbf{L}^*)' = (\\mathbf{L}\\mathbf{T})(\\mathbf{L}\\mathbf{T})' = \\mathbf{L}\\mathbf{T}\\mathbf{T}'\\mathbf{L}' = \\mathbf{L}(\\mathbf{T}\\mathbf{T}')\\mathbf{L}' = \\mathbf{L}\\mathbf{I}\\mathbf{L}' = \\mathbf{L}\\mathbf{L}'\n\\]\nPortanto, o modelo \\(\\mathbf{\\Sigma} = \\mathbf{L}^*(\\mathbf{L}^*)' + \\mathbf{\\Psi}\\) é equivalente ao modelo original. Essa propriedade é a base para a rotação fatorial, um procedimento que busca a solução \\(\\mathbf{L}^*\\) mais simples e interpretável, sem alterar o ajuste do modelo.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#adequabilidade-do-modelo-fatorial",
    "href": "src/02_tec_mult/07_af.html#adequabilidade-do-modelo-fatorial",
    "title": "8  Análise Fatorial",
    "section": "8.4 Adequabilidade do Modelo Fatorial",
    "text": "8.4 Adequabilidade do Modelo Fatorial\nAntes de aplicar os métodos de estimação, pode-se avaliar se os dados são adequados para a Análise Fatorial. A lema fundamental da AF é que as variáveis observadas são correlacionadas e que essa correlação pode ser explicada por fatores latentes. Se as variáveis são ortogonais ou se a correlação entre elas é espúria, o modelo fatorial não é apropriado.\nDois dos principais diagnósticos para verificar a adequabilidade dos dados são o Teste de Esfericidade de Bartlett e a medida de adequação da amostra de Kaiser-Meyer-Olkin (KMO).\n\n8.4.1 Teste de Esfericidade de Bartlett\nO Teste de Esfericidade de Bartlett avalia a hipótese nula (\\(H_0\\)) de que a matriz de correlação populacional \\(\\mathbf{P}\\) é uma matriz identidade (\\(H_0: \\mathbf{P} = \\mathbf{I}\\)). Se essa hipótese for verdadeira, as variáveis são não correlacionadas, e não há estrutura latente para ser extraída.\nA estatística de teste é baseada no determinante da matriz de correlação amostral \\(\\mathbf{R}\\) e, sob \\(H_0\\), segue aproximadamente uma distribuição Qui-quadrado. Para uma amostra de tamanho n e p variáveis, a estatística é:\n\\[\n\\chi^2 = -\\left[(n - 1) - \\frac{2p + 5}{6}\\right] \\ln(|\\mathbf{R}|)\n\\]\nEsta estatística tem, aproximadamente, uma distribuição \\(\\chi^2\\) com \\(p(p-1)/2\\) graus de liberdade. Um p-valor baixo (e.g., &lt; 0.05) leva à rejeição de \\(H_0\\), indicando que existe correlação suficiente entre as variáveis para justificar a aplicação da Análise Fatorial.\n\n\n8.4.2 Medida de Adequação da Amostra (KMO)\nEnquanto o teste de Bartlett avalia se a matriz de correlação como um todo se desvia significativamente da identidade, a medida de Kaiser-Meyer-Olkin (KMO) quantifica o quão adequados os dados são para a fatorização. O KMO compara a magnitude dos coeficientes de correlação observados com a magnitude dos coeficientes de correlação parcial.\nA lógica é que, se as variáveis compartilham fatores comuns, as correlações parciais entre pares de variáveis (controlando pelas outras variáveis) devem ser pequenas. A estatística KMO é calculada como:\n\\[\n\\text{KMO} = \\frac{\\sum_{i \\neq j} r_{ij}^2}{\\sum_{i \\neq j} r_{ij}^2 + \\sum_{i \\neq j} a_{ij}^2}\n\\]\nOnde \\(r_{ij}\\) é o coeficiente de correlação simples entre as variáveis \\(X_i\\) e \\(X_j\\), e \\(a_{ij}\\) é o coeficiente de correlação parcial.\nO valor do KMO varia de 0 a 1. Valores mais altos indicam que a Análise Fatorial é mais apropriada. Uma regra prática para a interpretação do KMO é:\n\n&gt; 0.9: Maravilhoso\n0.8 - 0.9: Meritório\n0.7 - 0.8: Razoável\n0.6 - 0.7: Medíocre\n0.5 - 0.6: Ruim\n&lt; 0.5: Inaceitável\n\nValores abaixo de 0.5 sugerem que a Análise Fatorial pode não ser uma boa ideia.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#métodos-de-estimação",
    "href": "src/02_tec_mult/07_af.html#métodos-de-estimação",
    "title": "8  Análise Fatorial",
    "section": "8.5 Métodos de Estimação",
    "text": "8.5 Métodos de Estimação\nAssumindo uma amostra aleatória \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\) de uma população com matriz de covariâncias \\(\\mathbf{\\Sigma}\\), o desafio é estimar \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) usando a matriz de covariâncias amostral \\(\\mathbf{S}\\) ou a matriz de correlação amostral \\(\\mathbf{R}\\).\nExistem diversos métodos para estimar os parâmetros do modelo fatorial, cada um com suas próprias premissas e propriedades. Alguns dos mais conhecidos incluem:\n\nMétodo de Componentes Principais (MCP)\nMétodo da Máxima Verossimilhança (MMV)\nMétodo dos Fatores Principais (Principal Axis Factoring)\nMínimos Quadrados Ponderados\nMínimos Quadrados Generalizados\n\nNeste capítulo, focaremos nos dois métodos mais amplamente utilizados na prática: o Método de Componentes Principais, por sua simplicidade computacional, e o Método da Máxima Verossimilhança, por sua fundamentação estatística robusta.\n\n8.5.1 A Solução por Componentes Principais\nO método de componentes principais (MCP) provê uma solução para \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) a partir da decomposição espectral da matriz de covariâncias amostral \\(\\mathbf{S}\\) (ou da matriz de correlações \\(\\mathbf{R}\\)).\nA ideia é que a matriz \\(\\mathbf{S}\\) pode ser decomposta em termos de seus pares de autovalor-autovetor \\((\\hat{\\lambda}_i, \\hat{\\mathbf{e}}_i)\\):\n\\[\n\\mathbf{S} = \\hat{\\lambda}_1\\hat{\\mathbf{e}}_1\\hat{\\mathbf{e}}_1' + \\hat{\\lambda}_2\\hat{\\mathbf{e}}_2\\hat{\\mathbf{e}}_2' + \\dots + \\hat{\\lambda}_p\\hat{\\mathbf{e}}_p\\hat{\\mathbf{e}}_p'\n\\]\nA estrutura do modelo fatorial é \\(\\mathbf{S} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\). O MCP busca uma aproximação para \\(\\mathbf{S}\\) retendo apenas os m primeiros componentes, que explicam a maior parte da variabilidade total. A matriz \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\) é construída para igualar a contribuição desses componentes:\n\\[\n\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' = \\hat{\\lambda}_1\\hat{\\mathbf{e}}_1\\hat{\\mathbf{e}}_1' + \\hat{\\lambda}_2\\hat{\\mathbf{e}}_2\\hat{\\mathbf{e}}_2' + \\dots + \\hat{\\lambda}_m\\hat{\\mathbf{e}}_m\\hat{\\mathbf{e}}_m'\n\\]\nUma solução explícita para \\(\\hat{\\mathbf{L}}\\) que satisfaz essa equação é uma matriz \\(p \\times m\\) cujas colunas são os autovetores reescalados pelos respectivos autovalores. A matriz \\(\\hat{\\mathbf{\\Psi}}\\) é então definida para garantir que as variâncias do modelo (\\(\\text{diag}(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}})\\)) sejam iguais às variâncias amostrais (\\(\\text{diag}(\\mathbf{S})\\)).\nIsso nos leva à seguinte definição formal.\n\nDefinição 8.2 Seja \\(\\mathbf{S}\\) a matriz de covariância amostral com pares de autovalor-autovetor \\((\\hat{\\lambda}_i, \\hat{\\mathbf{e}}_i)\\). A solução de componentes principais com m fatores é definida por:\n\nMatriz de Cargas Estimada (\\(\\hat{\\mathbf{L}}\\)): \\[\n\\hat{\\mathbf{L}} = [\\sqrt{\\hat{\\lambda}_1}\\hat{\\mathbf{e}}_1 | \\sqrt{\\hat{\\lambda}_2}\\hat{\\mathbf{e}}_2 | \\dots | \\sqrt{\\hat{\\lambda}_m}\\hat{\\mathbf{e}}_m]\n\\]\nMatriz de Variâncias Específicas Estimada (\\(\\hat{\\mathbf{\\Psi}}\\)): \\[\n\\hat{\\mathbf{\\Psi}} = \\text{diag}(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}')\n\\]\n\nonde \\(\\hat{\\psi}_i = s_{ii} - \\sum_{j=1}^{m} \\hat{l}_{ij}^2\\).\nSe a matriz de correlações \\(\\mathbf{R}\\) for utilizada, as cargas \\(\\hat{\\mathbf{L}}\\) são calculadas a partir dos autovalores e autovetores de \\(\\mathbf{R}\\), e as variâncias específicas são \\(\\hat{\\psi}_i = 1 - \\sum_{j=1}^{m} \\hat{l}_{ij}^2\\).\n\nPor construção, este método força a diagonal da matriz de covariâncias do modelo, \\(\\hat{\\mathbf{\\Sigma}} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\), a ser idêntica à diagonal de \\(\\mathbf{S}\\). O ajuste do modelo é então avaliado pela magnitude dos resíduos fora da diagonal. A matriz de resíduos é:\n\\[\n\\mathbf{S} - \\hat{\\mathbf{\\Sigma}} = \\mathbf{S} - (\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}})\n\\]\nComo \\(\\hat{\\mathbf{\\Psi}} = \\text{diag}(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}')\\), os elementos da diagonal da matriz de resíduos são zero. Os resíduos fora da diagonal são os elementos de \\(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\). Pode-se demonstrar que a soma dos quadrados de todos os elementos da matriz \\(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\) (incluindo a diagonal) é:\n\\[\n\\sum_{i=1}^p \\sum_{j=1}^p (s_{ij} - \\sum_{k=1}^m \\hat{l}_{ik}\\hat{l}_{jk})^2 = \\sum_{k=m+1}^p \\hat{\\lambda}_k^2\n\\]\nIsso mostra que, para que o ajuste seja bom, a soma dos autovalores descartados (\\(\\hat{\\lambda}_{m+1}, \\dots, \\hat{\\lambda}_p\\)) deve ser pequena.\n\n\n8.5.2 Método da Máxima Verossimilhança (MMV)\nO método da máxima verossimilhança (MMV) é uma abordagem mais rigorosa para a estimação, baseada em suposições sobre a distribuição dos dados.\nSuposições Adicionais:\n\nO vetor de fatores comuns \\(\\mathbf{F}\\) e o vetor de erros \\(\\boldsymbol{\\epsilon}\\) seguem uma distribuição normal multivariada:\n\n\\(\\mathbf{F} \\sim N_m(\\mathbf{0}, \\mathbf{I})\\)\n\\(\\boldsymbol{\\epsilon} \\sim N_p(\\mathbf{0}, \\mathbf{\\Psi})\\)\n\n\\(\\mathbf{F}\\) e \\(\\boldsymbol{\\epsilon}\\) são independentes.\n\nSob essas condições, o vetor de variáveis observáveis \\(\\mathbf{x}\\) segue uma distribuição normal multivariada \\(N_p(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})\\), onde \\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\).\nDada uma amostra aleatória \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\), a função de log-verossimilhança (ignorando constantes) para os parâmetros \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) é:\n\\[\n\\log L(\\mathbf{L}, \\mathbf{\\Psi}) = -\\frac{n}{2} \\ln |\\mathbf{\\Sigma}| - \\frac{n}{2} \\text{tr}(\\mathbf{\\Sigma}^{-1}\\mathbf{S})\n\\]\nonde \\(\\mathbf{S}\\) é a matriz de covariâncias amostral (versão ML, com divisor n). O objetivo é encontrar as estimativas \\(\\hat{\\mathbf{L}}\\) e \\(\\hat{\\mathbf{\\Psi}}\\) que maximizam essa função, sujeito à restrição de que \\(\\hat{\\mathbf{L}}'\\hat{\\mathbf{\\Psi}}^{-1}\\hat{\\mathbf{L}}\\) seja uma matriz diagonal para garantir a unicidade da solução.\nA maximização é realizada por meio de algoritmos numéricos (como o de Newton-Raphson), pois não há uma solução analítica fechada. As estimativas resultantes, \\(\\hat{\\mathbf{L}}\\) e \\(\\hat{\\mathbf{\\Psi}}\\), satisfazem um conjunto complexo de equações.\nA principal vantagem do MMV é que ele permite um teste de hipóteses para a adequação do número de fatores m, comparando a matriz de covariâncias do modelo, \\(\\hat{\\mathbf{\\Sigma}} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\), com a matriz amostral \\(\\mathbf{S}\\). Isso é fundamental na Análise Fatorial Confirmatória (AFC)",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#a-escolha-do-número-de-fatores-m",
    "href": "src/02_tec_mult/07_af.html#a-escolha-do-número-de-fatores-m",
    "title": "8  Análise Fatorial",
    "section": "8.6 A Escolha do Número de Fatores (m)",
    "text": "8.6 A Escolha do Número de Fatores (m)\nA determinação do número de fatores, m, é uma das decisões mais importantes na Análise Fatorial. Um número muito baixo de fatores pode não capturar a estrutura de covariância subjacente, enquanto um número muito alto pode levar a um modelo superajustado e de difícil interpretação, violando o princípio da parcimônia.\nA escolha de m geralmente envolve uma combinação de critérios estatísticos e julgamento prático. Vários dos métodos utilizados são análogos aos empregados na Análise de Componentes Principais (Capítulo 7). Os mais comuns são:\n\nProporção da Variância Total Explicada: Um critério comum é reter fatores suficientes para explicar uma proporção substancial (e.g., 70-90%) da variância total. No contexto do método de componentes principais para AF, a proporção da variância explicada pelo fator j é \\(\\hat{\\lambda}_j / \\text{tr}(\\mathbf{S})\\).\nCritério de Kaiser (Autovalores &gt; 1): Ao trabalhar com a matriz de correlação \\(\\mathbf{R}\\), o critério de Kaiser sugere reter apenas os fatores correspondentes a autovalores maiores que 1. A lógica é que um fator deve explicar pelo menos a variância de uma variável original.\nGráfico de cotovelo (Scree Plot): Este é um gráfico dos autovalores ordenados (\\(\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots\\)). Procura-se por um “cotovelo” no gráfico, um ponto onde a magnitude dos autovalores começa a diminuir drasticamente. O número de fatores a reter seria o número de pontos antes do início do platô.\nTeste de Hipóteses (para MMV): Quando o método da máxima verossimilhança é utilizado, é possível realizar um teste de razão de verossimilhanças para testar a hipótese nula de que m fatores são suficientes para descrever a estrutura de covariância.\n\nNa prática, é recomendável utilizar uma combinação desses critérios. A interpretabilidade da solução fatorial resultante é, em última análise, o guia mais importante.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#rotação-fatorial",
    "href": "src/02_tec_mult/07_af.html#rotação-fatorial",
    "title": "8  Análise Fatorial",
    "section": "8.7 Rotação Fatorial",
    "text": "8.7 Rotação Fatorial\nComo visto anteriormente, a solução para a matriz de cargas fatoriais \\(\\hat{\\mathbf{L}}\\) não é única. Qualquer rotação ortogonal dos fatores resulta em uma nova matriz de cargas \\(\\hat{\\mathbf{L}}^* = \\hat{\\mathbf{L}}\\mathbf{T}\\) que explica a estrutura de covariâncias dos dados exatamente da mesma forma, pois \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' = \\hat{\\mathbf{L}}^* (\\hat{\\mathbf{L}}^*)'\\).\nEssa indeterminação, que a princípio parece um problema, é na verdade uma das ferramentas mais poderosas da Análise Fatorial. Ela nos permite girar a estrutura fatorial para uma posição que seja mais simples e interpretável, sem sacrificar o ajuste do modelo. O objetivo é alcançar o que o psicólogo Louis Thurstone chamou de estrutura simples.\nA estrutura simples ideal teria as seguintes propriedades:\n\nCada variável deve ter pelo menos uma carga fatorial próxima de zero.\nCada fator deve ter várias cargas próximas de zero e algumas cargas altas.\nPara cada par de fatores, deve haver variáveis com cargas altas em um fator, mas não no outro.\n\nEm suma, busca-se uma matriz de cargas onde cada variável esteja fortemente associada a apenas um ou poucos fatores, e cada fator represente claramente um subconjunto de variáveis. Os métodos de rotação são algoritmos que buscam, de forma objetiva, uma matriz \\(\\mathbf{T}\\) que aproxime a matriz de cargas rotacionada \\(\\hat{\\mathbf{L}}^*\\) a essa estrutura ideal.\nAs rotações dividem-se em duas categorias principais.\n\n8.7.1 Rotações Ortogonais\nNeste tipo de rotação, a matriz de transformação \\(\\mathbf{T}\\) é ortogonal, o que significa que os eixos dos fatores são girados, mas mantidos em um ângulo de 90 graus entre si. A consequência fundamental é que os fatores rotacionados permanecem não correlacionados.\nOs métodos mais comuns de rotação ortogonal são:\n\nVarimax: É o método de rotação ortogonal mais popular. O objetivo do Varimax é simplificar as colunas da matriz de cargas fatoriais. Para cada fator, ele busca maximizar a variância das cargas ao quadrado, efetivamente empurrando as cargas para perto de 0 ou \\(\\pm 1\\). Isso facilita a identificação de quais variáveis estão associadas a cada fator. O critério Varimax maximiza a seguinte função:\n\\[\nV = \\sum_{j=1}^{m} \\left[ \\frac{1}{p} \\sum_{i=1}^{p} (\\hat{l}_{ij}^* / h_i)^4 - \\left( \\frac{1}{p} \\sum_{i=1}^{p} (\\hat{l}_{ij}^* / h_i)^2 \\right)^2 \\right]\n\\]\nOnde \\(\\hat{l}_{ij}^*\\) são as cargas rotacionadas e \\(h_i^2\\) são as comunalidades (que permanecem invariantes sob rotação).\nQuartimax: Este método foca em simplificar as linhas da matriz de cargas. Ele tenta fazer com que cada variável tenha carga alta em apenas um fator. O Quartimax foi o primeiro método analítico proposto, mas tende a criar um fator geral com cargas altas para muitas variáveis, o que pode dificultar a interpretação.\nEquimax: É um meio termo entre o Varimax e o Quartimax. Ele tenta simplificar tanto as linhas quanto as colunas da matriz de cargas simultaneamente.\n\n\n\n8.7.2 Rotações Oblíquas\nEm muitos campos, especialmente nas ciências sociais, é teoricamente razoável esperar que os fatores latentes sejam correlacionados. Por exemplo, os fatores “habilidade verbal” e “habilidade matemática” são distintos, mas é provável que sejam positivamente correlacionados.\nAs rotações oblíquas permitem que os fatores se tornem correlacionados. A matriz de transformação \\(\\mathbf{T}\\) não é mais ortogonal, e os eixos dos fatores podem ter ângulos diferentes de 90 graus. A vantagem é a capacidade de encontrar uma estrutura mais simples e teoricamente mais realista, ao custo de uma complexidade maior na interpretação, pois é preciso analisar tanto a matriz de cargas quanto a matriz de correlação entre os fatores.\nOs métodos mais comuns incluem:\n\nPromax: É um método muito utilizado que funciona em duas etapas. Primeiro, ele realiza uma rotação ortogonal (geralmente Varimax). Em seguida, ele relaxa a restrição de ortogonalidade, permitindo que os fatores se correlacionem para buscar uma estrutura ainda mais simples (com mais cargas próximas de zero).\nOblimin Direto: É um método mais geral que busca minimizar a covariância das cargas ao quadrado para pares de fatores. Ele possui um parâmetro (delta) que controla o grau de correlação permitido entre os fatores.\n\nA escolha entre uma rotação ortogonal e oblíqua depende de considerações teóricas. Se não há uma razão forte para acreditar que os fatores são correlacionados, a rotação ortogonal (como a Varimax) é geralmente preferida por sua simplicidade. Se a correlação entre os fatores é esperada, uma rotação oblíqua pode fornecer uma representação mais fiel da realidade.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html",
    "href": "src/02_tec_mult/08_cluster.html",
    "title": "9  Análise de Agrupamentos",
    "section": "",
    "text": "9.1 Decomposição da Variabilidade\nA Análise de Agrupamentos, ou Análise de Clusters, é uma técnica exploratória multivariada cujo objetivo é particionar um conjunto de observações em subgrupos (os clusters). A partição é feita de tal forma que as observações dentro de um mesmo grupo sejam semelhantes entre si, enquanto observações em grupos diferentes sejam o mais distintas possível.\nDiferentemente de outras técnicas como a análise de regressão ou a análise discriminante, a análise de agrupamentos é um método de aprendizagem não supervisionada. Isso significa que não temos uma variável resposta ou rótulos pré-definidos para os grupos; o objetivo é descobrir a estrutura de agrupamentos inerente aos próprios dados.\nO princípio fundamental é a maximização da homogeneidade intra-grupo e, ao mesmo tempo, a maximização da heterogeneidade entre grupos.\nDiversas técnicas apresentadas nesse capítulo dependem da definição de uma medida para quantificar o quão semelhantes ou diferentes as observações são. Essa medida é formalizada como uma medida de dissimilaridade ou distância. Uma discussão detalhada sobre as diferentes métricas de distância pode ser encontrada na Capítulo 6.\nPodemos formalizar o critério de “boa separação” dos grupos através de uma decomposição da variabilidade total dos dados, análoga à Análise de Variância (ANOVA).\nO objetivo da análise de agrupamentos pode ser visto como encontrar a partição que minimiza a SQI (grupos coesos) e maximiza a SQE (grupos separados).\nA prova deste teorema mostra que a variabilidade total é conservada e apenas particionada, de forma que minimizar a SQI é equivalente a maximizar a SQE.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#decomposição-da-variabilidade",
    "href": "src/02_tec_mult/08_cluster.html#decomposição-da-variabilidade",
    "title": "9  Análise de Agrupamentos",
    "section": "",
    "text": "Definição 9.2 Dado um conjunto de \\(n\\) observações e uma partição em \\(K\\) grupos \\(C_1, \\dots, C_K\\):\n\nSoma de Quadrados Total (SQT): Mede a dispersão total dos dados em torno da média geral \\(\\bar{\\mathbf{x}}\\). \\[\nSQT = \\sum_{i=1}^{n} (\\mathbf{x}_i - \\bar{\\mathbf{x}})'(\\mathbf{x}_i - \\bar{\\mathbf{x}})\n\\]\nSoma de Quadrados Intra-grupos (SQI): Mede a dispersão dentro dos grupos. É a soma das dispersões de cada observação em relação ao centroide do seu próprio grupo, \\(\\bar{\\mathbf{x}}_k\\). Também é conhecida como Within-Cluster Sum of Squares (WCSS). \\[\nSQI = \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)\n\\]\nSoma de Quadrados Entre-grupos (SQE): Mede a dispersão entre os centroides dos grupos em relação à média geral. \\[\nSQE = \\sum_{k=1}^{K} n_k (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\] Onde \\(n_k\\) é o número de observações no grupo \\(C_k\\).\n\n\n\n\nTeorema 9.1 A soma de quadrados total pode ser decomposta como a soma da variabilidade dentro dos grupos e entre os grupos.\n\\[\nSQT = SQI + SQE\n\\]\n\nComprovação. A prova parte da decomposição do desvio de uma observação \\(\\mathbf{x}_i \\in C_k\\) em relação à média geral \\(\\bar{\\mathbf{x}}\\):\n\\[\n(\\mathbf{x}_i - \\bar{\\mathbf{x}}) = (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\]\nElevando ao quadrado (no sentido de produto vetorial), temos:\n\\[\n(\\mathbf{x}_i - \\bar{\\mathbf{x}})'(\\mathbf{x}_i - \\bar{\\mathbf{x}}) = [(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})]'[(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})]\n\\]\n\\[\n= (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}}) + 2(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\]\nAgora, somamos sobre todas as observações \\(i=1, \\dots, n\\). Para fazer isso, somamos primeiro dentro de cada grupo \\(k\\) e depois somamos os resultados sobre todos os grupos:\n\\[\nSQT = \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}})'(\\mathbf{x}_i - \\bar{\\mathbf{x}})\n\\]\n\\[\n= \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}}) + \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} 2(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\]\nAnalisando cada termo:\n\nO primeiro termo é, por definição, a Soma de Quadrados Intra-grupos (SQI).\nNo segundo termo, a expressão \\((\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\\) é constante para todas as \\(n_k\\) observações no grupo \\(C_k\\). Portanto, a soma interna resulta em \\(n_k (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\\). Somar sobre \\(k\\) nos dá a Soma de Quadrados Entre-grupos (SQE).\nPara o terceiro termo (o termo cruzado), podemos reescrevê-lo como: \\[\n2 \\sum_{k=1}^{K} \\left[ \\left( \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) \\right)' (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}}) \\right]\n\\] Pela definição do centroide \\(\\bar{\\mathbf{x}}_k\\), a soma dos desvios em torno dele dentro de um grupo é zero: \\(\\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) = \\mathbf{0}\\). Portanto, todo o terceiro termo é igual a zero.\n\nJuntando os resultados, obtemos \\(SQT = SQI + SQE\\).",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#agrupamentos-hierárquicos",
    "href": "src/02_tec_mult/08_cluster.html#agrupamentos-hierárquicos",
    "title": "9  Análise de Agrupamentos",
    "section": "9.2 Agrupamentos Hierárquicos",
    "text": "9.2 Agrupamentos Hierárquicos\nOs métodos de agrupamento hierárquico criam uma sequência de partições aninhadas, que pode ser representada visualmente por uma árvore chamada dendrograma. Existem duas abordagens principais:\n\nAglomerativa (Bottom-Up): Começa com cada observação em seu próprio grupo e, a cada passo, funde os dois grupos mais próximos até que reste apenas um único grupo contendo todas as observações.\nDivisiva (Top-Down): Começa com todas as observações em um único grupo e, a cada passo, divide um grupo em dois até que cada observação esteja em seu próprio grupo.\n\n\n\n\n\n\n\nNota\n\n\n\nDevido a dificuldades de implementação, agrupamentos divisivos raramente são utilizados na prática. Por esse motivo, os exemplos presentes nesta seção consideram apenas agrupamentos aglomerativos.\n\n\n\n9.2.1 Métodos de Ligação (Linkage)\nEnquanto as medidas de dissimilaridade retratam a distância entre observações, precisamos de uma regra que define a distância entre dois grupos. Para isso definimos alguns métodos de ligação populares a seguir.\n\nLigação Simples (Single Linkage): A distância entre dois grupos é a distância mínima entre quaisquer dois pontos dos grupos. Tende a produzir grupos “alongados” e é sensível a ruído. \\[ d(A, B) = \\min_{\\mathbf{a} \\in A, \\mathbf{b} \\in B} d(\\mathbf{a}, \\mathbf{b}) \\]\nLigação Completa (Complete Linkage): A distância é o máximo da distância entre quaisquer dois pontos. Produz grupos mais compactos e esféricos. \\[ d(A, B) = \\max_{\\mathbf{a} \\in A, \\mathbf{b} \\in B} d(\\mathbf{a}, \\mathbf{b}) \\]\nLigação Média (Average Linkage): A distância é a média de todas as distâncias entre os pares de pontos dos dois grupos. É um meio-termo entre a simples e a completa. \\[ d(A, B) = \\frac{1}{n_A n_B} \\sum_{\\mathbf{a} \\in A} \\sum_{\\mathbf{b} \\in B} d(\\mathbf{a}, \\mathbf{b}) \\]\nMétodo de Ward: Este método se baseia em um critério de minimização da variância. A cada passo do algoritmo aglomerativo, ele funde o par de grupos que leva ao menor aumento possível na Soma de Quadrados Intra-grupos (SQI). O objetivo é encontrar, a cada passo, a fusão mais “econômica” em termos de perda de coesão interna.\nSuponha que estejamos considerando fundir dois grupos, \\(C_i\\) e \\(C_j\\). O aumento na SQI, que denotamos por \\(\\Delta(C_i, C_j)\\), é a diferença entre a SQI do novo grupo fundido (\\(C_{ij}\\)) e a soma das SQIs dos grupos individuais antes da fusão.\n\\[\n\\Delta(C_i, C_j) = \\text{SQI}(C_{ij}) - (\\text{SQI}(C_i) + \\text{SQI}(C_j))\n\\]\nA SQI para o novo grupo \\(C_{ij} = C_i \\cup C_j\\) é \\(\\sum_{\\mathbf{x} \\in C_{ij}} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})\\), onde \\(\\mathbf{x}_{ij}\\) é o centroide do novo grupo. Podemos reescrever essa soma como: \\[\n\\sum_{\\mathbf{x} \\in C_i} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij}) + \\sum_{\\mathbf{x} \\in C_j} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})\n\\] Usando a mesma lógica da decomposição da variância, podemos mostrar que \\(\\sum_{\\mathbf{x} \\in C_i} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij}) = \\text{SQI}(C_i) + n_i (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})\\).\nAplicando o resultado para ambos os termos, a SQI do novo grupo é: \\[\n\\text{SQI}(C_{ij}) = \\text{SQI}(C_i) + \\text{SQI}(C_j) + n_i (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij}) + n_j (\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})\n\\] Portanto, o aumento na SQI é: \\[\n\\Delta(C_i, C_j) = n_i (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij}) + n_j (\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})\n\\] Substituindo \\(\\bar{\\mathbf{x}}_{ij} = \\frac{n_i\\bar{\\mathbf{x}}_i + n_j\\bar{\\mathbf{x}}_j}{n_i+n_j}\\) e simplificando a álgebra, chegamos a: \\[\n\\Delta(C_i, C_j) = \\frac{n_i n_j}{n_i + n_j} (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_j)'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_j)\n\\]\nA fórmula final nos dá uma maneira eficiente de calcular o critério de Ward. A cada passo, o algoritmo calcula \\(\\Delta(C_i, C_j)\\) para todos os pares de grupos e realiza a fusão para o par que tiver o menor valor. Note que a fórmula depende da distância euclidiana entre centroides, por esse motivo, o método de Ward tende a produzir grupos de tamanho semelhante e formato esférico.\n\n\n\n9.2.2 O Dendrograma\nO dendrograma é a principal ferramenta de visualização para o agrupamento hierárquico. Ele mostra como, passo a passo, as observações são fundidas em grupos. O eixo Y representa a distância ou dissimilaridade em que as fusões ocorrem. Quanto mais alta a “ponte” que une dois grupos, mais diferentes eles são.\nO gráfico abaixo, gerado a partir de um exemplo simples de 4 observações, mostra a anatomia de um dendrograma.\n\n\n\n\n\n\n\n\nFigura 9.1: Anatomia de um dendrograma simples.\n\n\n\n\n\nO exemplo abaixo apresenta os dendogramas de maneira prática, além de exemplificar também como a função de ligação escolhida impacta no agrupamento formado.\n\nExemplo 9.1 Vamos analisar o comportamento dos métodos de ligação com a matriz de 5x5 abaixo.\n\\[\n\\mathbf{D} =\n\\left(\n\\begin{array}{c|ccccc}\n     & \\mathbf{1} & \\mathbf{2} & \\mathbf{3} & \\mathbf{4} & \\mathbf{5} \\\\ \\hline\n\\mathbf{1} & 0     &       &        &        &        \\\\\n\\mathbf{2} & 1     & 0     &        &        &        \\\\\n\\mathbf{3} & 8     & 2     & 0      &        &        \\\\\n\\mathbf{4} & 6     & 4     & 11     & 0      &        \\\\\n\\mathbf{5} & 7     & 9     & 8      & 7      & 0      \\\\\n\\end{array}\n\\right)\n\\]\n1. Usando a Ligação Simples (Single Linkage)\n\nPasso 1: A menor distância na matriz é d(1,2) = 1. Fundimos {1,2} na altura 1.\nPasso 2: A distância do novo grupo {1,2} para {3} é min(d(1,3), d(2,3)) = min(8, 2) = 2. Todas as outras distâncias entre grupos ou pontos restantes são maiores que 2. Assim, fundimos {1,2} com {3} na altura 2.\nPasso 3: A distância de {1,2,3} para {4} é min(d(1,4), d(2,4), d(3,4)) = min(6, 4, 11) = 4. Esta é a próxima menor distância, então fundimos {1,2,3} com {4} na altura 4.\nPasso 4: A distância de {1,2,3,4} para {5} é min(d(1,5), d(2,5), d(3,5), d(4,5)) = min(7, 9, 8, 7) = 7. Fundimos o último ponto na altura 7.\nResultado: O método cria uma longa cadeia: ((({1,2},3),4),5).\n\n2. Usando a Ligação Completa (Complete Linkage)\n\nPasso 1: A fusão inicial é a mesma: {1,2} (altura 1).\nPasso 2: A distância de {1,2} para {4} é max(d(1,4), d(2,4)) = max(6, 4) = 6. Já a distância para {3} é max(d(1,3), d(2,3)) = max(8, 2) = 8. A menor distância entre os grupos existentes é 6, então fundimos {1,2} com {4}.\nPasso 3: Temos os grupos {1,2,4}, {3} e {5}. A próxima menor distância entre os grupos restantes é d(3,5) = 8. Fundimos {3,5}.\nPasso 4: A distância entre {1,2,4} e {3,5} é max(d(1,3), d(1,5), d(2,3), d(2,5), d(4,3), d(4,5)) = max(8, 7, 2, 9, 11, 7) = 11. A fusão final ocorre na altura 11.\nResultado: O método cria dois grupos distintos, ({1,2,4}, {3,5}), antes da fusão final.\n\nOs resultados são estruturalmente diferentes, como mostram os dendrogramas.\n\n\nCódigo\nimport numpy as np\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\nimport matplotlib.pyplot as plt\n\n# Matriz de distância 5x5 do exemplo\nD = np.array([\n    [0, 1, 8, 6, 7],\n    [1, 0, 2, 4, 9],\n    [8, 2, 0, 11, 8],\n    [6, 4, 11, 0, 7],\n    [7, 9, 8, 7, 0]\n])\n\ncondensed_D = squareform(D)\nlabels = ['1', '2', '3', '4', '5']\n\nfig, axes = plt.subplots(1, 2, figsize=(7, 5))\nfig.suptitle('Dendrogramas Puros (Sem Definição de grupos)')\n\n# Ligação Simples\nlinked_single = linkage(condensed_D, 'single')\ndendrogram(linked_single, orientation='top', labels=labels, ax=axes[0], color_threshold=0, above_threshold_color='k')\naxes[0].set_title('Ligação Simples')\naxes[0].set_xlabel('Observação')\naxes[0].set_ylabel('Distância')\n\n# Ligação Completa\nlinked_complete = linkage(condensed_D, 'complete')\ndendrogram(linked_complete, orientation='top', labels=labels, ax=axes[1], color_threshold=0, above_threshold_color='k')\naxes[1].set_title('Ligação Completa')\naxes[1].set_xlabel('Observação')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nFigura 9.2: Comparação de dendrogramas para a matriz 5x5. As cores foram removidas para mostrar a estrutura pura.\n\n\n\n\n\n\nO dendrograma não apenas mostra a hierarquia das fusões, mas também é a principal ferramenta para decidir o número final de grupos. A estratégia consiste em “cortar” a árvore em uma determinada altura. Todas as ramificações que estão abaixo da linha de corte constituem os grupos.\nA regra geral é procurar por um corte que cruze as conexões mais longas. Uma conexão longa representa uma fusão que ocorreu a uma distância (ou aumento de SQI, no caso de Ward) muito maior do que as fusões anteriores. Isso sugere que estamos unindo grupos que são naturalmente muito diferentes entre si. Portanto, cortar a árvore logo acima dessa grande “distância de fusão” é uma escolha sensata.\nA Figura 9.3 demonstra como a escolha de diferentes alturas de corte leva a diferentes números de grupos.\n\n\nCódigo\nimport numpy as np\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\nimport matplotlib.pyplot as plt\n\n# Matriz de distância 5x5 do exemplo\nD = np.array([\n    [0, 1, 8, 6, 7],\n    [1, 0, 2, 4, 9],\n    [8, 2, 0, 11, 8],\n    [6, 4, 11, 0, 7],\n    [7, 9, 8, 7, 0]\n])\n\ncondensed_D = squareform(D)\nlabels = ['1', '2', '3', '4', '5']\nlinked_complete = linkage(condensed_D, 'complete')\n\nfig, axes = plt.subplots(1, 3, figsize=(7, 5))\nfig.suptitle('Visualização dos Cortes no Dendrograma (Ligação Completa)')\n\n# --- Corte para K=2 ---\ncut_height_k2 = 9\ndendrogram(\n    linked_complete,\n    orientation='top',\n    labels=labels,\n    ax=axes[0],\n    color_threshold=0,\n    above_threshold_color='k'\n)\naxes[0].axhline(y=cut_height_k2, color='r', linestyle='--')\naxes[0].set_title('Corte para K=2 grupos')\naxes[0].set_xlabel('Observação')\naxes[0].set_ylabel('Distância')\n\n# --- Corte para K=3 ---\ncut_height_k3 = 7\ndendrogram(\n    linked_complete,\n    orientation='top',\n    labels=labels,\n    ax=axes[1],\n    color_threshold=0,\n    above_threshold_color='k'\n)\naxes[1].axhline(y=cut_height_k3, color='r', linestyle='--')\naxes[1].set_title('Corte para K=3 grupos')\naxes[1].set_xlabel('Observação')\n\n# --- Corte para K=4 ---\ncut_height_k4 = 3\ndendrogram(\n    linked_complete,\n    orientation='top',\n    labels=labels,\n    ax=axes[2],\n    color_threshold=0,\n    above_threshold_color='k'\n)\naxes[2].axhline(y=cut_height_k4, color='r', linestyle='--')\naxes[2].set_title('Corte para K=4 grupos')\naxes[2].set_xlabel('Observação')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nFigura 9.3: Ilustração do corte no dendrograma de Ligação Completa para obter K=2 (esquerda), K=3 (centro) e K=4 (direita) grupos.\n\n\n\n\n\nA interpretação dos resultados para cada corte é a seguinte:\n\nK=2: Ao cortar o dendrograma na altura 9, obtemos dois grupos. O primeiro é {1, 2, 4} e o segundo é {3, 5}. Esta partição representa a estrutura de mais alto nível nos dados, separando os dois grupos mais distintos.\nK=3: Abaixando a linha de corte para a altura 7, o grupo {3, 5} (que era formado na altura 8) é quebrado. O resultado são três grupos: {1, 2, 4}, {3} e {5}.\nK=4: Com um corte ainda mais baixo, na altura 3, quebramos o grupo {1, 2, 4} (formado na altura 6). Os grupos resultantes são {1, 2}, {4}, {3} e {5}.\n\nObserve que o primeiro grupo {1, 2} se forma em uma altura de uma unidade de distância. Já a segunda ligação, {1, 2} com {4} ocorre na altura d({1, 2}, {4})=6. Isso indica que já existe um salto na distância da ligação logo no segundo passo. Por isso, uma escolha sensível é \\(K=4\\).\n\n\n\n\n\n\nO Eixo Vertical no Dendrograma\n\n\n\nPara os métodos de ligação simples, completa e média o eixo vertical do dendrograma represente diretamente a distância da fusão. Já para o Método de Ward, a altura da fusão representa o aumento na Soma de Quadrados Intra-grupos (SQI) resultante da união dos dois grupos. Esse valor não é uma distância, mas sim uma medida de perda de homogeneidade.\n\n\n\n\n9.2.3 Limitações do Agrupamento Hierárquico\nApesar de sua simplicidade e elegância, os métodos hierárquicos possuem limitações importantes. Uma delas é sua complexidade computacional, que cresce rapidamente para dados com muitas observações devido a grande quantidade de comparações.\nAlém disso, a decisão de fusão é final e não pode ser desfeita. Se um grupo inicial for inadequada, o erro se propagará por toda a hierarquia. Ou seja, o agrupamento é muito sensível à estrutura inicial. Finalmente, todos os métodos de ligação carregam suas vantagens e problemas.\nEssas limitações motivam o uso de métodos não hierárquicos, como o K-médias, que abordaremos a seguir.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#agrupamento-não-hierárquico",
    "href": "src/02_tec_mult/08_cluster.html#agrupamento-não-hierárquico",
    "title": "9  Análise de Agrupamentos",
    "section": "9.3 Agrupamento Não-Hierárquico",
    "text": "9.3 Agrupamento Não-Hierárquico\nDiferente dos métodos hierárquicos, os métodos particionais, como o K-Médias, dividem os dados em um número \\(K\\) de grupos pré-especificado. O K-Médias é um dos algoritmos de agrupamento mais populares e eficientes.\nO objetivo do K-Médias é particionar as \\(n\\) observações em \\(K\\) grupos de modo a minimizar a Soma de Quadrados Intra-grupos (SQI), também chamada de inércia.\n\\[\n\\text{SQI} = \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)\n\\]\nOnde \\(\\bar{\\mathbf{x}}_k\\) é o centroide (média) do grupo \\(C_k\\).\n\n9.3.1 O Algoritmo K-Médias (K-Means)\nO algoritmo K-médias é um processo iterativo que busca minimizar a SQI. Seus passos podem ser definidos matematicamente da seguinte forma:\n\nInicialização: Escolha \\(K\\) centroides iniciais \\(\\boldsymbol{\\mu}_1^{(0)}, \\boldsymbol{\\mu}_2^{(0)}, \\dots, \\boldsymbol{\\mu}_K^{(0)}\\). Esta escolha pode ser feita selecionando \\(K\\) observações aleatórias do conjunto de dados.\nAtribuição: Em cada iteração \\(t\\), atribua cada observação \\(\\mathbf{x}_i\\) ao grupo cujo centroide é o mais próximo. Matematicamente, para cada observação \\(i\\), encontramos o índice do grupo \\(c_i\\) que minimiza a distância Euclidiana quadrada: \\[\nc_i^{(t)} = \\arg \\min_{k} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k^{(t-1)})' (\\mathbf{x}_i - \\boldsymbol{\\mu}_k^{(t-1)})\n\\] Isso particiona os dados nos conjuntos \\(C_1^{(t)}, \\dots, C_K^{(t)}\\).\nAtualização: Recalcule o centroide de cada grupo como a média de todas as observações atribuídas a ele na iteração atual: \\[\n\\boldsymbol{\\mu}_k^{(t)} = \\frac{1}{\\# C_k^{(t)}} \\sum_{\\mathbf{x}_i \\in C_k^{(t)}} \\mathbf{x}_i\n\\]\nRepetição: Repita os passos 2 e 3 até que as atribuições dos grupos não mudem mais, ou seja, \\(c_i^{(t)} = c_i^{(t-1)}\\) para todas as observações \\(i\\).\n\nO grande problema do método K-médias é a escolha dos centroides iniciais. O algoritmo tem a garantia de convergir, mas dependendo das condições iniciais pode chegar a um mínimo local, e não necessariamente o mínimo global da SQI. Uma opção comum é executar o algoritmo várias vezes com diferentes inicializações e escolher o resultado com a menor SQI.\nA necessidade de pré-especificar o número de grupos, \\(K\\), também pode ser uma desvantagem. Para contornar o problema, é comum executar o algoritmo para uma gama de valores de \\(K\\) e calcular a Soma de Quadrados Intra-grupos (SQI) em cada caso. A ideia é escolher \\(K\\) tal que a SQI seja baixa, mas sendo parcimonioso com o número de grupos. Vale lembrar que a SQI é inversamente proporcional a K – se \\(K=1\\), a SQI é máxima e se \\(K=n\\) a SQI é zero. Na prática, aumentamos \\(K\\) progressivamente observando os decréscimo na SQI, seguimos aumentando \\(K\\) enquanto esse decréscimo for grande.\nA seguir, definimos um procedimento mais robusto, que soluciona os problemas mencionados combinando diferentes métodos de agrupamento.\n\n\n9.3.2 Procedimento sugerido para análise de agrupamentos\nLevando em consideração as vantagens e desvantagens dos agrupamentos hierárquicos e de K-médias, podemos definir um procedimento que simplifica as escolhas durante um problema prático de análise de agrupamentos.\n\nInicie com um agrupamento hierárquico. O método de Ward costuma ser uma boa primeira opção, no entanto, experimente também outros métodos de ligação conforme necessário.\nObserve o dendograma para escolher o corte que seja mais plausível. Esse corte determina um número de grupos ótimo \\(K*\\).\nCalcule o centroide para cada um dos grupos obtidos via agrupamento hierárquico \\(\\mu_1, \\cdots, \\mu_K\\).\nUtilize os centoides obtidos como centroides iniciais para o método K-médias com \\(K*\\) grupos.\n\nExemplificamos esse procedimento com dados reais no Capítulo 12.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#agrupamento-baseado-em-modelos",
    "href": "src/02_tec_mult/08_cluster.html#agrupamento-baseado-em-modelos",
    "title": "9  Análise de Agrupamentos",
    "section": "9.4 Agrupamento Baseado em Modelos",
    "text": "9.4 Agrupamento Baseado em Modelos\nUma abordagem mais avançada e flexível é o agrupamento baseado em modelos. A ideia central é assumir que os dados são gerados a partir de uma mistura de distribuições de probabilidade, onde cada componente da mistura corresponde a um grupo.\nO modelo mais comum é o Modelo de Mistura Gaussiana (Gaussian Mixture Model, GMM). Ele assume que cada grupo segue uma distribuição normal multivariada. A densidade de probabilidade de todo o conjunto de dados é uma soma ponderada de \\(K\\) densidades Gaussianas:\n\\[\np(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k)\n\\]\nOnde, para cada grupo \\(k\\):\n\n\\(\\pi_k\\): é o peso da mistura (a probabilidade de uma observação pertencer ao grupo \\(k\\)).\n\\(\\boldsymbol{\\mu}_k\\): é o vetor de médias (o centroide do grupo).\n\\(\\mathbf{\\Sigma}_k\\): é a matriz de covariâncias (descreve a forma e orientação do grupo).\n\n\n9.4.1 O Algoritmo de Expectation-Maximization (EM)\nOs parâmetros do GMM (\\(\\pi_k, \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k\\)) são estimados usando o algoritmo de Expectation-Maximization (EM).\nO algoritmo é um processo iterativo que alterna entre dois passos:\n\nInicialização: Inicialize os parâmetros do modelo (\\(\\pi_k, \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k\\)) para cada grupo \\(k=1, \\dots, K\\). Isso pode ser feito de forma aleatória ou usando o resultado de um algoritmo mais simples, como o K-médias.\nPasso-E (Expectation): Nesta etapa, calculamos a “responsabilidade” de cada grupo \\(k\\) por cada observação \\(\\mathbf{x}_i\\). A responsabilidade, denotada por \\(p_{ik}\\), é a probabilidade posterior de que a observação \\(\\mathbf{x}_i\\) tenha sido gerada pela componente \\(k\\) da mistura, dados os parâmetros atuais. Usando a regra de Bayes, a responsabilidade é calculada como: \\[\np_{ik} = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_j, \\mathbf{\\Sigma}_j)}\n\\] Essencialmente, para cada ponto, calculamos a probabilidade de ele pertencer a cada um dos \\(K\\) grupos.\nPasso-M (Maximization): Nesta etapa, usamos as responsabilidades calculadas para reestimar os parâmetros do modelo, maximizando a verossimilhança esperada. As atualizações são:\n\nNovos pesos da mistura: O novo peso \\(\\pi_k^{\\text{novo}}\\) é a proporção média de responsabilidade do grupo \\(k\\) sobre todas as observações. \\[\n\\pi_k^{\\text{novo}} = \\frac{N_k}{n}, \\quad \\text{onde } N_k = \\sum_{i=1}^{n} p_{ik}\n\\]\nNovas médias: A nova média \\(\\boldsymbol{\\mu}_k^{\\text{novo}}\\) é uma média ponderada de todas as observações, onde os pesos são as responsabilidades. \\[\n\\boldsymbol{\\mu}_k^{\\text{novo}} = \\frac{1}{N_k} \\sum_{i=1}^{n} p_{ik} \\mathbf{x}_i\n\\]\nNovas covariâncias: A nova matriz de covariâncias \\(\\mathbf{\\Sigma}_k^{\\text{novo}}\\) é uma média ponderada das covariâncias, centrada na nova média. \\[\n\\mathbf{\\Sigma}_k^{\\text{novo}} = \\frac{1}{N_k} \\sum_{i=1}^{n} p_{ik} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k^{\\text{novo}})(\\mathbf{x}_i - \\boldsymbol{\\mu}_k^{\\text{novo}})'\n\\]\n\nRepetição: Os passos E e M são repetidos até que a log-verossimilhança do modelo, \\(\\ln p(\\mathbf{X} | \\pi, \\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = \\sum_{i=1}^{n} \\ln \\left( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right)\\), convirja para um valor estável.\n\n\n\n9.4.2 Vantagens do Agrupamento Baseado em Modelos\n\nAgrupamento “Suave”: O GMM fornece uma probabilidade de pertencimento a cada grupo para cada observação, em vez de uma atribuição “dura” (tudo ou nada) como o K-Médias.\nFlexibilidade de Formato: Ao permitir que cada grupo tenha sua própria matriz de covariâncias \\(\\mathbf{\\Sigma}_k\\), os GMMs podem identificar grupos com diferentes formas (elípticas) e orientações, enquanto o K-Médias assume implicitamente que os grupos são esféricos.\nSeleção de Modelo Criteriosa: A natureza estatística do método permite o uso de critérios de informação, como o Critério de Informação Bayesiano (BIC) ou o Critério de Informação de Akaike (AIC), para selecionar o número ideal de grupos \\(K\\) e a estrutura da matriz de covariâncias de forma mais objetiva. O modelo com o menor valor de BIC é geralmente preferido.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#índices-de-v1alidação-de-agrupamentos",
    "href": "src/02_tec_mult/08_cluster.html#índices-de-v1alidação-de-agrupamentos",
    "title": "9  Análise de Agrupamentos",
    "section": "9.5 Índices de v1alidação de agrupamentos",
    "text": "9.5 Índices de v1alidação de agrupamentos\nUma das perguntas mais importantes na análise de grupo é “qual o número ideal de grupos?”. Embora o procedimento que combina o método hierárquico com o K-médias ajude a guiar essa escolha, existem diversas métricas quantitativas que avaliam a qualidade de uma partição de grupos, independentemente do método utilizado para obtê-la. Esses índices nos permitem comparar os resultados para diferentes valores de \\(K\\) e escolher o que for estatisticamente mais robusto.\n\n9.5.1 Método da Silhueta\nA análise de silhueta é uma das técnicas mais populares. Ela mede o quão bem cada observação se encaixa em seu grupo em comparação com outros grupos. O coeficiente de silhueta para uma única observação \\(i\\) é:\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n\\]\nOnde: - \\(a(i)\\): A distância média de \\(i\\) para todas as outras observações no mesmo grupo (coesão). - \\(b(i)\\): A distância média de \\(i\\) para todas as observações no grupo vizinho mais próximo (separação).\nO valor de \\(s(i)\\) varia de -1 a 1. Para um dado \\(K\\), calculamos o coeficiente de silhueta médio para todas as observações. O valor de \\(K\\) que maximiza a pontuação média da silhueta é considerado o ideal.\n\n\n9.5.2 Índice de Calinski-Harabasz\nTambém conhecido como Critério da Razão de Variâncias, este índice avalia a qualidade do agrupamento pela razão entre a dispersão entre os grupos e a dispersão intra-grupo. A fórmula é:\n\\[\nCH(K) = \\frac{SQE / (K-1)}{SQI / (n-K)}\n\\]\nOnde \\(SQE\\) é a Soma de Quadrados Entre-grupos, \\(SQI\\) é a Soma de Quadrados Intra-grupos, \\(n\\) é o número total de observações e \\(K\\) é o número de grupos.\nIntuitivamente, um bom agrupamento tem uma alta dispersão entre os grupos (SQE alta) e uma baixa dispersão dentro dos grupos (SQI baixa). Portanto, procuramos o valor de \\(K\\) que maximiza o índice de Calinski-Harabasz.\n\n\n9.5.3 Índice de Davies-Bouldin\nEste índice mede a “similaridade” média de cada grupo com seu grupo mais semelhante. A similaridade entre dois grupos \\(C_i\\) e \\(C_j\\) é definida como:\n\\[\nR_{ij} = \\frac{s_i + s_j}{d_{ij}}\n\\]\nOnde \\(s_i\\) é a dispersão média dentro do grupo \\(i\\) (por exemplo, a distância média de cada ponto ao centroide) e \\(d_{ij}\\) é a distância entre os centroides dos grupos \\(i\\) e \\(j\\).\nPara cada grupo \\(i\\), encontramos o grupo \\(j\\) que maximiza \\(R_{ij}\\). O índice de Davies-Bouldin é a média desses valores máximos sobre todos os grupos:\n\\[\nDB(K) = \\frac{1}{K} \\sum_{i=1}^{K} \\max_{j \\neq i} (R_{ij})\n\\]\nUm valor baixo de \\(DB(K)\\) indica que os grupos estão bem separados e compactos. Portanto, procuramos o valor de \\(K\\) que minimiza o índice de Davies-Bouldin.\nEm resumo, a escolha de \\(K\\) deve ser guiada por uma combinação dessas ferramentas, a análise do dendrograma e, mais importante, o contexto do problema. Se os grupos resultantes não forem interpretáveis ou úteis, o valor de \\(K\\) deve ser reconsiderado.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#interpretações-em-análises-de-agrupamentos",
    "href": "src/02_tec_mult/08_cluster.html#interpretações-em-análises-de-agrupamentos",
    "title": "9  Análise de Agrupamentos",
    "section": "9.6 Interpretações em análises de agrupamentos",
    "text": "9.6 Interpretações em análises de agrupamentos\nUma vez que os grupos são formados, o passo final e mais importante é a interpretação. Um agrupamento matematicamente sólido é inútil se não pudermos extrair dele insights práticos. A interpretação envolve a caracterização e a validação dos grupos.\n\n9.6.1 Perfil dos grupos\nO objetivo aqui é responder à pergunta: “Quem são os membros de cada grupo?”. Para isso, descrevemos cada grupo em termos das variáveis usadas na análise (e também de outras variáveis descritivas, se disponíveis). O método mais comum é calcular estatísticas descritivas para cada variável, segmentadas por grupo.\n\nVariáveis Contínuas: Calcule a média e o desvio padrão de cada variável para cada grupo. Em seguida, compare a média de um grupo com a média geral da amostra. Por exemplo: “O grupo 2 tem uma renda média 30% acima da média geral, enquanto o grupo 1 tem uma renda 20% abaixo”.\nVariáveis Categóricas: Calcule a distribuição de frequência (ou porcentagens) de cada categoria para cada grupo. Por exemplo: “O grupo 3 é composto por 80% de clientes do sexo feminino, enquanto na amostra total essa proporção é de 55%”.\n\nA criação de uma tabela de perfis, com os grupos nas linhas e as estatísticas das variáveis nas colunas, é uma ferramenta extremamente eficaz para resumir e comunicar os resultados.\n\n\n9.6.2 Visualização do agrupamento\nA visualização é pode auxiliar na validação da separação dos grupos.\n\nDados de Baixa Dimensão (2D ou 3D): Um simples gráfico de dispersão, com os pontos identificados de acordo com seu grupo designado, é suficiente.\nDados de Alta Dimensão: Quando temos mais de três variáveis, precisamos primeiro reduzir a dimensionalidade para poder visualizar. A Análise de Componentes Principais (ACP) é a técnica mais comum para isso. Podemos plotar as observações em um gráfico de dispersão usando os dois primeiros componentes principais como eixos e identificar os pontos por grupo. Isso nos dá uma visão da separação dos grupos no espaço que captura a maior parte da variabilidade dos dados.\n\n\n\n9.6.3 Nomeação dos grupos\nDepois de entender o perfil de cada grupo, é uma boa prática dar a eles nomes ou “personas” descritivas. Nomes como “Jovens Urbanos Conectados”, “Famílias Rurais Conservadoras” ou “Clientes de Alto Risco” são muito mais fáceis de comunicar e lembrar do que “Grupo 1”, “Grupo 2”, etc. O nome deve capturar a essência do que torna aquele grupo único.\nA interpretação é um processo iterativo. Às vezes, os perfis dos grupos podem sugerir que o número de grupos escolhido não foi o ideal, levando o analista a revisitar as etapas anteriores.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html",
    "href": "src/exemplos/acp_manual.html",
    "title": "10  Exemplo manual: ACP",
    "section": "",
    "text": "10.1 O Cenário\nNeste exemplo, vamos detalhar passo a passo a aplicação da Análise de Componentes Principais (ACP) em um pequeno conjunto de dados. O objetivo é demonstrar manualmente todos os cálculos, desde a preparação dos dados até a interpretação dos resultados, seguindo a metodologia apresentada no Capítulo 3.\nVamos expandir o exemplo da intuição geométrica, que usava Peso e Altura. Adicionaremos uma terceira variável, Renda (em milhares de R$), para um grupo de 5 indivíduos. A ideia é que Peso e Altura sejam correlacionados, mas a Renda não tenha uma correlação forte com eles.\nNosso conjunto de dados inicial é:",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#o-cenário",
    "href": "src/exemplos/acp_manual.html#o-cenário",
    "title": "10  Exemplo manual: ACP",
    "section": "",
    "text": "Indivíduo\nPeso (kg)\nAltura (cm)\nRenda (R$ 1000)\n\n\n\n\n1\n65\n170\n5.5\n\n\n2\n72\n182\n4.0\n\n\n3\n58\n165\n7.0\n\n\n4\n81\n190\n3.5\n\n\n5\n75\n178\n5.0",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-1-preparação-dos-dados",
    "href": "src/exemplos/acp_manual.html#passo-1-preparação-dos-dados",
    "title": "10  Exemplo manual: ACP",
    "section": "10.2 Passo 1: Preparação dos Dados",
    "text": "10.2 Passo 1: Preparação dos Dados\nConforme discutido no Capítulo 3, a ACP é sensível à escala das variáveis. Portanto, o primeiro passo é padronizar os dados. Isso envolve duas etapas: centralizar (subtrair a média) e escalonar (dividir pelo desvio padrão).\n\n10.2.1 1.1. Calcular a Média e o Desvio Padrão\nPrimeiro, calculamos a média e o desvio padrão para cada variável.\n\\[\n\\bar{x}_{peso} = \\frac{65+72+58+81+75}{5} = 70.2 \\, \\text{kg}\n\\] \\[\n\\bar{x}_{altura} = \\frac{170+182+165+190+178}{5} = 177.0 \\, \\text{cm}\n\\] \\[\n\\bar{x}_{renda} = \\frac{5.5+4.0+7.0+3.5+5.0}{5} = 5.0 \\, (RS 1000)\n\\]\nAgora, os desvios padrão (usando a fórmula com denominador \\(n-1\\)):\n\\[\ns_{peso} = \\sqrt{\\frac{(65-70.2)^2 + ... + (75-70.2)^2}{4}} = 8.64 \\, \\text{kg}\n\\] \\[\ns_{altura} = \\sqrt{\\frac{(170-177)^2 + ... + (178-177)^2}{4}} = 9.67 \\, \\text{cm}\n\\] \\[\ns_{renda} = \\sqrt{\\frac{(5.5-5.0)^2 + ... + (5.0-5.0)^2}{4}} = 1.35 \\, (RS 1000)\n\\]\n\n\n10.2.2 1.2. Padronizar os Dados\nCom as médias e desvios padrão, podemos padronizar cada observação \\(x_{ij}\\) usando a fórmula \\(z_{ij} = (x_{ij} - \\bar{x}_j) / s_j\\).\nPor exemplo, para o Indivíduo 1: \\[\nz_{1, peso} = \\frac{65 - 70.2}{8.64} = -0.60\n\\] \\[\nz_{1, altura} = \\frac{170 - 177}{9.67} = -0.72\n\\] \\[\nz_{1, renda} = \\frac{5.5 - 5.0}{1.35} = 0.37\n\\]\nAplicando isso a todos os dados, obtemos a matriz de dados padronizados \\(\\mathbf{Z}\\):\n\\[\n\\mathbf{Z} = \\begin{pmatrix}\n-0.60 & -0.72 & 0.37 \\\\\n0.21 & 0.52 & -0.74 \\\\\n-1.41 & -1.24 & 1.48 \\\\\n1.25 & 1.34 & -1.11 \\\\\n0.56 & 0.10 & 0.00\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-2-calcular-a-matriz-de-correlação",
    "href": "src/exemplos/acp_manual.html#passo-2-calcular-a-matriz-de-correlação",
    "title": "10  Exemplo manual: ACP",
    "section": "10.3 Passo 2: Calcular a Matriz de Correlação",
    "text": "10.3 Passo 2: Calcular a Matriz de Correlação\nComo estamos trabalhando com dados padronizados, a ACP será realizada sobre a matriz de correlação \\(\\mathbf{R}\\). A matriz de correlação pode ser calculada como:\n\\[\n\\mathbf{R} = \\frac{1}{n-1} \\mathbf{Z}' \\mathbf{Z}\n\\]\nCalculando \\(\\mathbf{Z}' \\mathbf{Z}\\): \\[\n\\mathbf{Z}' \\mathbf{Z} = \\begin{pmatrix}\n4.00 & 3.85 & -0.81 \\\\\n3.85 & 4.00 & -1.18 \\\\\n-0.81 & -1.18 & 4.00\n\\end{pmatrix}\n\\]\nDividindo por \\(n-1 = 4\\), obtemos a matriz de correlação \\(\\mathbf{R}\\):\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1.00 & 0.96 & -0.20 \\\\\n0.96 & 1.00 & -0.29 \\\\\n-0.20 & -0.29 & 1.00\n\\end{pmatrix}\n\\]\nComo esperado, a correlação entre Peso e Altura (0.96) é muito alta, enquanto a Renda tem uma correlação fraca e negativa com as outras duas variáveis.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-3-decomposição-espectral-da-matriz-de-correlação",
    "href": "src/exemplos/acp_manual.html#passo-3-decomposição-espectral-da-matriz-de-correlação",
    "title": "10  Exemplo manual: ACP",
    "section": "10.4 Passo 3: Decomposição Espectral da Matriz de Correlação",
    "text": "10.4 Passo 3: Decomposição Espectral da Matriz de Correlação\nO próximo passo é encontrar os autovalores (\\(\\lambda\\)) e autovetores (\\(\\mathbf{e}\\)) da matriz de correlação \\(\\mathbf{R}\\). Eles são a solução da equação \\(\\mathbf{R}\\mathbf{e} = \\lambda\\mathbf{e}\\), que é equivalente a resolver \\((\\mathbf{R} - \\lambda\\mathbf{I})\\mathbf{e} = \\mathbf{0}\\).\nIsso requer encontrar as raízes do polinômio característico \\(det(\\mathbf{R} - \\lambda\\mathbf{I}) = 0\\).\n\\[\ndet \\begin{pmatrix}\n1.00 - \\lambda & 0.96 & -0.20 \\\\\n0.96 & 1.00 - \\lambda & -0.29 \\\\\n-0.20 & -0.29 & 1.00 - \\lambda\n\\end{pmatrix} = 0\n\\]\nResolver este determinante cúbico manualmente é trabalhoso. Usando uma calculadora ou software, encontramos os seguintes autovalores:\n\\[\n\\lambda_1 = 1.98 \\quad \\lambda_2 = 1.00 \\quad \\lambda_3 = 0.02\n\\]\n\n10.4.1 Interpretação dos Autovalores\nA variância total no sistema é a soma dos autovalores (que é igual ao traço da matriz \\(\\mathbf{R}\\), ou seja, 3). - Variância Total = \\(1.98 + 1.00 + 0.02 = 3.00\\)\nA proporção da variância explicada por cada componente é: - CP1: \\(\\frac{1.98}{3.00} = 66.0\\%\\) - CP2: \\(\\frac{1.00}{3.00} = 33.3\\%\\) - CP3: \\(\\frac{0.02}{3.00} = 0.7\\%\\)\nOs dois primeiros componentes juntos explicam \\(66.0\\% + 33.3\\% = 99.3\\%\\) da variância total. Isso indica que podemos reduzir a dimensionalidade de 3 para 2 com uma perda mínima de informação.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-4-calcular-os-autovetores",
    "href": "src/exemplos/acp_manual.html#passo-4-calcular-os-autovetores",
    "title": "10  Exemplo manual: ACP",
    "section": "10.5 Passo 4: Calcular os Autovetores",
    "text": "10.5 Passo 4: Calcular os Autovetores\nAgora, para cada autovalor, resolvemos o sistema \\((\\mathbf{R} - \\lambda_i\\mathbf{I})\\mathbf{e}_i = \\mathbf{0}\\) para encontrar o autovetor correspondente \\(\\mathbf{e}_i\\).\n\nPara \\(\\lambda_1 = 1.98\\): \\[\n\\begin{pmatrix}\n-0.98 & 0.96 & -0.20 \\\\\n0.96 & -0.98 & -0.29 \\\\\n-0.20 & -0.29 & -0.98\n\\end{pmatrix}\n\\begin{pmatrix} e_{11} \\\\ e_{12} \\\\ e_{13} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solução, após normalização (para que \\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\)), é: \\[\n\\mathbf{e}_1 = \\begin{pmatrix} 0.69 \\\\ 0.71 \\\\ -0.15 \\end{pmatrix}\n\\]\nPara \\(\\lambda_2 = 1.00\\): \\[\n\\begin{pmatrix}\n0.00 & 0.96 & -0.20 \\\\\n0.96 & 0.00 & -0.29 \\\\\n-0.20 & -0.29 & 0.00\n\\end{pmatrix}\n\\begin{pmatrix} e_{21} \\\\ e_{22} \\\\ e_{23} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solução normalizada é: \\[\n\\mathbf{e}_2 = \\begin{pmatrix} 0.18 \\\\ -0.22 \\\\ -0.96 \\end{pmatrix}\n\\]\nPara \\(\\lambda_3 = 0.02\\): \\[\n\\begin{pmatrix}\n0.98 & 0.96 & -0.20 \\\\\n0.96 & 0.98 & -0.29 \\\\\n-0.20 & -0.29 & 0.98\n\\end{pmatrix}\n\\begin{pmatrix} e_{31} \\\\ e_{32} \\\\ e_{33} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solução normalizada é: \\[\n\\mathbf{e}_3 = \\begin{pmatrix} -0.70 \\\\ 0.67 \\\\ -0.24 \\end{pmatrix}\n\\]",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-5-interpretação-dos-componentes",
    "href": "src/exemplos/acp_manual.html#passo-5-interpretação-dos-componentes",
    "title": "10  Exemplo manual: ACP",
    "section": "10.6 Passo 5: Interpretação dos Componentes",
    "text": "10.6 Passo 5: Interpretação dos Componentes\nOs autovetores (ou loadings) nos dizem como as variáveis originais se combinam para formar cada componente.\n\nComponente Principal 1 (\\(CP_1\\)): \\[\nY_1 = 0.69 \\cdot Z_{peso} + 0.71 \\cdot Z_{altura} - 0.15 \\cdot Z_{renda}\n\\] Este componente é basicamente uma média ponderada de Peso e Altura, com uma pequena contribuição negativa da Renda. Podemos interpretá-lo como um índice de “Tamanho Corporal”. As cargas altas e positivas para Peso e Altura confirmam a alta correlação entre essas variáveis.\nComponente Principal 2 (\\(CP_2\\)): \\[\nY_2 = 0.18 \\cdot Z_{peso} - 0.22 \\cdot Z_{altura} - 0.96 \\cdot Z_{renda}\n\\] Este componente é dominado pela Renda, com uma carga muito alta e negativa. As cargas para Peso e Altura são pequenas. Podemos interpretar o \\(CP_2\\) como um índice de “Status Socioeconômico Inverso”, já que ele é quase que inteiramente uma representação da Renda (com sinal trocado).",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-6-calcular-os-scores-dos-componentes",
    "href": "src/exemplos/acp_manual.html#passo-6-calcular-os-scores-dos-componentes",
    "title": "10  Exemplo manual: ACP",
    "section": "10.7 Passo 6: Calcular os Scores dos Componentes",
    "text": "10.7 Passo 6: Calcular os Scores dos Componentes\nFinalmente, podemos calcular os valores (scores) dos componentes principais para cada indivíduo. Usamos a fórmula \\(\\mathbf{Y} = \\mathbf{Z} \\mathbf{P}\\), onde \\(\\mathbf{P}\\) é a matriz cujas colunas são os autovetores.\n\\[\n\\mathbf{P} = \\begin{pmatrix}\n0.69 & 0.18 & -0.70 \\\\\n0.71 & -0.22 & 0.67 \\\\\n-0.15 & -0.96 & -0.24\n\\end{pmatrix}\n\\]\nPara o Indivíduo 1, com dados padronizados \\((-0.60, -0.72, 0.37)\\): \\[\ny_{11} = (-0.60)(0.69) + (-0.72)(0.71) + (0.37)(-0.15) = -0.98\n\\] \\[\ny_{12} = (-0.60)(0.18) + (-0.72)(-0.22) + (0.37)(-0.96) = -0.31\n\\]\nCalculando para todos os indivíduos, obtemos a matriz de scores \\(\\mathbf{Y}\\):\n\n\n\nIndivíduo\nCP1 (Tamanho)\nCP2 (Renda Inversa)\n\n\n\n\n1\n-0.98\n-0.31\n\n\n2\n0.57\n0.85\n\n\n3\n-2.19\n-1.18\n\n\n4\n2.09\n1.35\n\n\n5\n0.46\n0.08",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#conclusão",
    "href": "src/exemplos/acp_manual.html#conclusão",
    "title": "10  Exemplo manual: ACP",
    "section": "10.8 Conclusão",
    "text": "10.8 Conclusão\nEste exemplo demonstra o poder da ACP. Começamos com três variáveis e, através de uma derivação passo a passo, conseguimos: 1. Reduzir a dimensionalidade: Mostramos que 99.3% da informação está contida em dois componentes. 2. Criar variáveis não correlacionadas: O \\(CP_1\\) e o \\(CP_2\\) são, por construção, ortogonais. 3. Interpretar a estrutura latente: Identificamos que a principal fonte de variação nos dados é o “Tamanho Corporal” (uma combinação de Peso e Altura), seguida pelo “Status Socioeconômico” (representado pela Renda).\nA análise manual, embora trabalhosa, revela a mecânica exata da técnica, solidificando a compreensão teórica apresentada no capítulo principal.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exemplo manual: ACP</span>"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html",
    "href": "src/exemplos/af_big_five.html",
    "title": "11  Rotação fatorial em R",
    "section": "",
    "text": "11.1 Passo 1: Análise Descritiva e Adequação dos Dados\nA Análise Fatorial (AF) é uma técnica estatística poderosa usada para identificar estruturas latentes (fatores) subjacentes a um conjunto de variáveis observadas. No entanto, a solução matemática inicial de uma AF raramente é interpretável. É aqui que a rotação fatorial se torna a etapa mais crítica do processo. A rotação transforma a matriz de cargas fatoriais inicial em uma solução mais simples e teoricamente mais significativa, sem alterar as propriedades matemáticas fundamentais da solução.\nEste documento oferece um exemplo prático e didático, totalmente focado em demonstrar o impacto das diferentes estratégias de rotação. Usaremos o software R e o clássico conjunto de dados bfi (Big Five Inventory) do pacote psych.\nObjetivos:\nPrimeiro, carregamos os pacotes necessários e o conjunto de dados bfi. Este dataset contém respostas de 2800 indivíduos a 25 itens de personalidade.\nCódigo\n# Carregar pacotes\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(corrplot)\n\n# Carregar os dados do Big Five Inventory\ndata(bfi, package = \"psych\")\n\n# Selecionar apenas as 25 variáveis de itens de personalidade\nbfi_items &lt;- bfi[, 1:25]\n\n# Remover linhas com dados ausentes para simplificar\nbfi_complete &lt;- na.omit(bfi_items)\n\nknitr::kable(head(bfi_complete))\n\n\n\n\nTabela 11.1: Exemplo de respostas no banco de dados Big Five\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1\nA2\nA3\nA4\nA5\nC1\nC2\nC3\nC4\nC5\nE1\nE2\nE3\nE4\nE5\nN1\nN2\nN3\nN4\nN5\nO1\nO2\nO3\nO4\nO5\n\n\n\n\n61617\n2\n4\n3\n4\n4\n2\n3\n3\n4\n4\n3\n3\n3\n4\n4\n3\n4\n2\n2\n3\n3\n6\n3\n4\n3\n\n\n61618\n2\n4\n5\n2\n5\n5\n4\n4\n3\n4\n1\n1\n6\n4\n3\n3\n3\n3\n5\n5\n4\n2\n4\n3\n3\n\n\n61620\n5\n4\n5\n4\n4\n4\n5\n4\n2\n5\n2\n4\n4\n4\n5\n4\n5\n4\n2\n3\n4\n2\n5\n5\n2\n\n\n61621\n4\n4\n6\n5\n5\n4\n4\n3\n5\n5\n5\n3\n4\n4\n4\n2\n5\n2\n4\n1\n3\n3\n4\n3\n5\n\n\n61622\n2\n3\n3\n4\n5\n4\n4\n5\n3\n2\n2\n2\n5\n4\n5\n2\n3\n4\n4\n3\n3\n3\n4\n3\n3\n\n\n61623\n6\n6\n5\n6\n5\n6\n6\n6\n1\n3\n2\n1\n6\n5\n6\n3\n5\n2\n2\n3\n4\n3\n5\n6\n1\nAs 25 variáveis correspondem a 5 itens para cada um dos traços do “Big Five”:\nA hipótese teórica é que os 5 itens que medem o mesmo traço (e.g., N1 a N5) estarão altamente correlacionados entre si e se agruparão em um único fator latente (Neuroticismo).\nUma boa prática é verificar se os dados são fatorizáveis. Para isso, podemos usar o teste de Bartlett e a medida KMO.\nCódigo\n# Teste de Bartlett\nbartlett_test &lt;- cortest.bartlett(bfi_complete)\n\n\nR was not square, finding R from data\n\n\nCódigo\n# Teste KMO\nkmo_test &lt;- KMO(bfi_complete)\n\n# Exibindo os resultados de forma concisa\ncat(\"Teste de Bartlett: p-valor =\", bartlett_test$p.value, \"\\n\")\n\n\nTeste de Bartlett: p-valor = 0 \n\n\nCódigo\ncat(\"Medida KMO Geral (Overall MSA):\", round(kmo_test$MSA, 2), \"\\n\")\n\n\nMedida KMO Geral (Overall MSA): 0.85\nO p-valor de Bartlett próximo de zero e o KMO de 0.85 (“meritório”) sugerem que os dados têm correlações suficientes para justificar uma análise fatorial.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rotação fatorial em R</span>"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-1-análise-descritiva-e-adequação-dos-dados",
    "href": "src/exemplos/af_big_five.html#passo-1-análise-descritiva-e-adequação-dos-dados",
    "title": "11  Rotação fatorial em R",
    "section": "",
    "text": "A1-A5: Amabilidade (Agreeableness)\nC1-C5: Conscienciosidade (Conscientiousness)\nE1-E5: Extroversão (Extraversion)\nN1-N5: Neuroticismo (Neuroticism)\nO1-O5: Abertura à Experiência (Openness)",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rotação fatorial em R</span>"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-2-extração-inicial-dos-fatores-e-escolha-do-número-de-fatores",
    "href": "src/exemplos/af_big_five.html#passo-2-extração-inicial-dos-fatores-e-escolha-do-número-de-fatores",
    "title": "11  Rotação fatorial em R",
    "section": "11.2 Passo 2: Extração Inicial dos Fatores e Escolha do Número de Fatores",
    "text": "11.2 Passo 2: Extração Inicial dos Fatores e Escolha do Número de Fatores\nAntes de rotacionar, precisamos extrair os fatores. Dois métodos comuns são a Fatoração do Eixo Principal (ou “componentes principais” para o modelo fatorial) e a Máxima Verossimilhança. Vamos extrair 5 fatores usando ambos os métodos (sem rotação) para ver a solução inicial.\n\n\nCódigo\n# Extração via Fatoração do Eixo Principal (Principal Axis Factoring)\nfa_pa &lt;- fa(bfi_complete, nfactors = 5, rotate = \"none\", fm = \"pa\")\ncat(\"Cargas - Fatoração do Eixo Principal (PA):\\n\")\n\n\nCargas - Fatoração do Eixo Principal (PA):\n\n\nCódigo\nprint(fa_pa$loadings, cutoff = 0.3)\n\n\n\nLoadings:\n   PA1    PA2    PA3    PA4    PA5   \nA1                             -0.371\nA2  0.467                       0.340\nA3  0.534  0.302                     \nA4  0.417                            \nA5  0.581                            \nC1  0.343         0.446              \nC2  0.336         0.477              \nC3  0.319         0.351  0.310       \nC4 -0.465        -0.452              \nC5 -0.493                            \nE1 -0.408                            \nE2 -0.619                       0.323\nE3  0.527  0.328                     \nE4  0.599        -0.329              \nE5  0.513                            \nN1 -0.441  0.636                     \nN2 -0.423  0.616                     \nN3 -0.407  0.611                     \nN4 -0.528  0.416                     \nN5 -0.345  0.413                     \nO1  0.328               -0.360       \nO2                       0.370       \nO3  0.407               -0.446       \nO4                                   \nO5                       0.412       \n\n                 PA1   PA2   PA3   PA4   PA5\nSS loadings    4.600 2.268 1.549 1.218 0.956\nProportion Var 0.184 0.091 0.062 0.049 0.038\nCumulative Var 0.184 0.275 0.337 0.385 0.424\n\n\nCódigo\n# Extração via Máxima Verossimilhança (Maximum Likelihood)\nfa_ml &lt;- fa(bfi_complete, nfactors = 5, rotate = \"none\", fm = \"ml\")\ncat(\"\\nCargas - Máxima Verossimilhança (ML):\\n\")\n\n\n\nCargas - Máxima Verossimilhança (ML):\n\n\nCódigo\nprint(fa_ml$loadings, cutoff = 0.3)\n\n\n\nLoadings:\n   ML1    ML2    ML3    ML4    ML5   \nA1                             -0.322\nA2 -0.396  0.354                0.334\nA3 -0.462  0.401                0.321\nA4 -0.386                            \nA5 -0.546                            \nC1                0.465              \nC2                0.511              \nC3                0.404              \nC4  0.441        -0.512              \nC5  0.485        -0.358              \nE1  0.355 -0.309                     \nE2  0.585                       0.336\nE3 -0.446  0.436                     \nE4 -0.552  0.333                     \nE5 -0.409  0.429                     \nN1  0.609  0.566                     \nN2  0.587  0.543                     \nN3  0.533  0.479                     \nN4  0.591                            \nN5  0.421                            \nO1                      -0.409       \nO2                       0.388       \nO3 -0.329  0.349        -0.491       \nO4                      -0.307  0.311\nO5                       0.433       \n\n                 ML1   ML2   ML3   ML4   ML5\nSS loadings    4.451 2.379 1.546 1.221 0.977\nProportion Var 0.178 0.095 0.062 0.049 0.039\nCumulative Var 0.178 0.273 0.335 0.384 0.423\n\n\nAs duas soluções iniciais são numericamente diferentes, mas conceitualmente iguais: são ininterpretáveis. Um primeiro fator geral domina, e as variáveis se distribuem de forma confusa nos demais. Isso reforça a necessidade da rotação.\nPara determinar o número de fatores a extrair de forma mais objetiva, usamos a Análise Paralela.\n\n\nCódigo\nfa.parallel(bfi_complete, fa = \"fa\", fm = \"pa\")\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  NA \n\n\n\n\n\n\n\n\nFigura 11.1: Análise Paralela sugerindo a extração de 6 fatores.\n\n\n\n\n\nA Análise Paralela (Figura 11.1) sugere 6 fatores. No entanto, como nosso objetivo é testar a teoria dos Big Five, prosseguiremos com a extração de 5 fatores, uma decisão comum quando a teoria é forte.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rotação fatorial em R</span>"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-3-rotação-ortogonal-varimax",
    "href": "src/exemplos/af_big_five.html#passo-3-rotação-ortogonal-varimax",
    "title": "11  Rotação fatorial em R",
    "section": "11.3 Passo 3: Rotação Ortogonal (Varimax)",
    "text": "11.3 Passo 3: Rotação Ortogonal (Varimax)\nA rotação Varimax “limpa” a estrutura sob a suposição de que os fatores não são correlacionados entre si.\n\n\nCódigo\n# Análise Fatorial com rotação Varimax\nfa_varimax &lt;- factanal(bfi_complete, \n                       factors = 5, \n                       rotation = \"varimax\")\n\ncat(\"Cargas Fatoriais (AF) - Rotação Varimax:\\n\")\n\n\nCargas Fatoriais (AF) - Rotação Varimax:\n\n\nCódigo\nprint(fa_varimax$loadings, cutoff = 0.3, sort = TRUE)\n\n\n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5\nN1  0.816                                 \nN2  0.787                                 \nN3  0.714                                 \nN4  0.562  -0.367                         \nN5  0.518                                 \nE1         -0.587                         \nE2         -0.674                         \nE4          0.613           0.363         \nC1                  0.533                 \nC2                  0.624                 \nC3                  0.554                 \nC4                 -0.653                 \nC5                 -0.573                 \nA2                          0.601         \nA3                          0.662         \nA5          0.351           0.580         \nO1                                  0.524 \nO3                                  0.614 \nO5                                 -0.512 \nA1                         -0.393         \nA4                          0.454         \nE3          0.490           0.315   0.313 \nE5          0.491   0.310                 \nO2                                 -0.454 \nO4                                  0.368 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      2.687   2.320   2.034   1.978   1.557\nProportion Var   0.107   0.093   0.081   0.079   0.062\nCumulative Var   0.107   0.200   0.282   0.361   0.423\n\n\nA estrutura agora é muito mais “simples” e alinhada com a teoria. Podemos visualizar essa transformação de forma clara comparando o círculo de correlações antes e depois da rotação.\nPrimeiro, a solução não rotacionada (Figura 11.2). Note como as variáveis (vetores) se espalham pelo espaço fatorial sem um padrão claro. É difícil traçar os eixos (fatores) de forma que representem grupos distintos de variáveis.\n\n\nCódigo\nlibrary(ggrepel)\n\n# Extrair cargas da solução NÃO ROTACIONADA (ml) para um dataframe\nloadings_unrotated_df &lt;- as.data.frame(unclass(fa_ml$loadings))\nloadings_unrotated_df$Variable &lt;- rownames(loadings_unrotated_df)\n\n# Selecionar algumas variáveis para anotar e evitar poluição\nvars_to_label &lt;- c(\"N1\", \"N3\", \"E2\", \"E4\", \"A1\", \"C1\", \"O1\")\nannotations_df_unrotated &lt;- loadings_unrotated_df %&gt;% filter(Variable %in% vars_to_label)\n\n# Criar dados para o círculo unitário\ncircle &lt;- data.frame(\n  angle = seq(-pi, pi, length = 100),\n  x = sin(seq(-pi, pi, length = 100)),\n  y = cos(seq(-pi, pi, length = 100))\n)\n\n# Gerar o gráfico com ggplot2\nggplot(data = loadings_unrotated_df, aes(x = ML1, y = ML2)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_path(data = circle, aes(x = x, y = y), inherit.aes = FALSE, color = \"gray60\") +\n  geom_segment(aes(x = 0, y = 0, xend = ML1, yend = ML2),\n               arrow = arrow(length = unit(0.1, \"inches\")),\n               color = \"steelblue\") +\n  geom_text_repel(data = annotations_df_unrotated, aes(label = Variable), min.segment.length = 0) +\n  coord_fixed(ratio = 1, xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  labs(title = \"Círculo de Correlações - Solução Não Rotacionada\",\n       x = \"Fator 1\",\n       y = \"Fator 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 11.2: Círculo de correlações da solução não rotacionada. Os vetores não estão alinhados com os eixos.\n\n\n\n\n\nAgora, veja o resultado após a rotação Varimax (Figura 11.3). A rotação funcionou como um ajuste dos eixos, alinhando-os com os agrupamentos de variáveis.\n\n\nCódigo\n# Extrair cargas da solução ROTACIONADA (Varimax) para um dataframe\nloadings_rotated_df &lt;- as.data.frame(unclass(fa_varimax$loadings))\nloadings_rotated_df$Variable &lt;- rownames(loadings_rotated_df)\n\n# Selecionar as mesmas variáveis para anotar\nannotations_df_rotated &lt;- loadings_rotated_df %&gt;% filter(Variable %in% vars_to_label)\n\n# Calcular a variância explicada para os eixos\nss_loadings &lt;- colSums(fa_varimax$loadings^2)\nprop_variance &lt;- ss_loadings / ncol(bfi_complete)\nxlab_text &lt;- sprintf(\"Fator 1 (%.2f%% da variância)\", prop_variance[1] * 100)\nylab_text &lt;- sprintf(\"Fator 2 (%.2f%% da variância)\", prop_variance[2] * 100)\n\n# Gerar o gráfico com ggplot2\nggplot(data = loadings_rotated_df, aes(x = Factor1, y = Factor2)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_path(data = circle, aes(x = x, y = y), inherit.aes = FALSE, color = \"gray60\") +\n  geom_segment(aes(x = 0, y = 0, xend = Factor1, yend = Factor2),\n               arrow = arrow(length = unit(0.1, \"inches\")),\n               color = \"steelblue\") +\n  geom_text_repel(data = annotations_df_rotated, aes(label = Variable), min.segment.length = 0) +\n  coord_fixed(ratio = 1, xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  labs(title = \"Círculo de Correlações - Rotação Varimax\",\n       x = xlab_text,\n       y = ylab_text) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 11.3: Círculo de correlações após a rotação Varimax. A rotação alinhou os vetores com os eixos, revelando uma estrutura simples.\n\n\n\n\n\nO resultado é uma “estrutura simples”, onde os itens de Neuroticismo (como N1 e N3) carregam quase exclusivamente no Fator 1 (correlação próxima de 1 ou -1 em um eixo e de 0 no outro), e os itens de Extroversão (como E2 e E4) carregam no Fator 2. A interpretação se torna direta.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rotação fatorial em R</span>"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-4-rotação-oblíqua-promax",
    "href": "src/exemplos/af_big_five.html#passo-4-rotação-oblíqua-promax",
    "title": "11  Rotação fatorial em R",
    "section": "11.4 Passo 4: Rotação Oblíqua (Promax)",
    "text": "11.4 Passo 4: Rotação Oblíqua (Promax)\nA rotação Promax é mais flexível, pois permite que os fatores sejam correlacionados. Isso costuma ser mais realista em psicologia.\n\n\nCódigo\n# Análise Fatorial com rotação Promax (oblíqua)\nfa_promax &lt;- fa(bfi_complete, \n                nfactors = 5, \n                rotate = \"promax\", \n                fm = \"pa\")\n\n\nLoading required namespace: GPArotation\n\n\nCódigo\ncat(\"Cargas Fatoriais (Pattern Matrix) - Rotação Promax:\\n\")\n\n\nCargas Fatoriais (Pattern Matrix) - Rotação Promax:\n\n\nCódigo\nprint(fa_promax$loadings, cutoff = 0.3, sort = TRUE)\n\n\n\nLoadings:\n   PA2    PA1    PA3    PA5    PA4   \nN1  0.835                            \nN2  0.791                            \nN3  0.741                            \nN4  0.533 -0.311                     \nN5  0.529                            \nE1        -0.636                     \nE2        -0.711                     \nE3         0.545                     \nE4         0.660                     \nC1                0.567              \nC2                0.697              \nC3                0.597              \nC4               -0.652              \nC5               -0.561              \nA2                       0.611       \nA3                       0.620       \nO3                              0.576\nO5                             -0.543\nA1                      -0.463       \nA4                       0.411       \nA5         0.332         0.489       \nE5         0.498                     \nO1                              0.491\nO2                             -0.484\nO4                              0.370\n\n                 PA2   PA1   PA3   PA5   PA4\nSS loadings    2.704 2.486 2.050 1.638 1.461\nProportion Var 0.108 0.099 0.082 0.066 0.058\nCumulative Var 0.108 0.208 0.290 0.355 0.414\n\n\nA matriz de cargas é similar à da Varimax, mas a grande vantagem é poder examinar a matriz de correlação entre os fatores.\n\n\nCódigo\n# Matriz de correlação entre os fatores\nfactor_correlations &lt;- fa_promax$Phi\n\ncat(\"Matriz de Correlação entre os Fatores (Promax):\\n\")\n\n\nMatriz de Correlação entre os Fatores (Promax):\n\n\nCódigo\nprint(round(factor_correlations, 2))\n\n\n      PA2   PA1   PA3   PA5  PA4\nPA2  1.00 -0.26 -0.22 -0.01 0.04\nPA1 -0.26  1.00  0.40  0.35 0.14\nPA3 -0.22  0.40  1.00  0.24 0.19\nPA5 -0.01  0.35  0.24  1.00 0.16\nPA4  0.04  0.14  0.19  0.16 1.00\n\n\nCódigo\n# Visualização da matriz de correlação\ncorrplot(factor_correlations, method = \"color\", type = \"upper\", \n         addCoef.col = \"black\", tl.col = \"black\", tl.srt = 45, diag = FALSE)\n\n\n\n\n\n\n\n\nFigura 11.4: Correlações entre os fatores da solução Promax.\n\n\n\n\n\nA matriz de correlação (Figura 11.4) mostra que Neuroticismo (ML1) se correlaciona negativamente com Conscienciosidade (ML3) (-0.33) e Extroversão (ML2) (-0.24). Essas relações são teoricamente plausíveis e seriam perdidas em uma rotação ortogonal.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rotação fatorial em R</span>"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#conclusão-qual-rotação-escolher",
    "href": "src/exemplos/af_big_five.html#conclusão-qual-rotação-escolher",
    "title": "11  Rotação fatorial em R",
    "section": "11.5 Conclusão: Qual Rotação Escolher?",
    "text": "11.5 Conclusão: Qual Rotação Escolher?\n\nSolução Não Rotacionada: É apenas um ponto de partida matemático. Dificilmente é possível tirar interpretações úteis dela.\nRotação Ortogonal (Varimax): É a melhor escolha quando há fortes razões teóricas para acreditar que os fatores são independentes. Oferece uma solução mais simples (parcimoniosa).\nRotação Oblíqua (Promax): É uma escolha mais realista nas ciências sociais. A decisão final deve ser baseada na matriz de correlação dos fatores. Se as correlações forem substanciais, a solução oblíqua é superior.\n\nNeste exemplo, a solução Promax (oblíqua) é a mais apropriada. Ela não apenas recupera a estrutura dos Big Five, mas também fornece insights sobre como esses traços se relacionam, oferecendo uma visão mais rica e fiel da realidade psicológica.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rotação fatorial em R</span>"
    ]
  },
  {
    "objectID": "src/exemplos/agrupamentos.html",
    "href": "src/exemplos/agrupamentos.html",
    "title": "12  Análise de agrupamentos",
    "section": "",
    "text": "12.1 Preparação dos Dados\nNeste exemplo, aplicaremos o procedimento de análise de agrupamentos proposto na Seção 9.3.2. O objetivo é ilustrar como a combinação de métodos hierárquicos e não hierárquicos pode levar a uma solução de agrupamento robusta e interpretável.\nUtilizaremos o conjunto de dados USArrests, que contém estatísticas de crimes para cada um dos 50 estados dos EUA em 1973. As variáveis são:\nNosso objetivo é agrupar os estados com base em seus perfis de criminalidade e urbanização.\nO primeiro passo em qualquer análise de agrupamento baseada em distância é a padronização dos dados. As variáveis no nosso conjunto de dados têm escalas muito diferentes (Assault varia na casa das centenas, enquanto Murder varia na casa das dezenas). Se não padronizarmos, a variável Assault dominará o cálculo da distância, e o agrupamento será baseado quase inteiramente nela.\nPadronizamos as variáveis para que tenham média 0 e desvio padrão 1.\nVisualização dos 5 primeiros estados com dados padronizados:\n              Murder   Assault  UrbanPop      Rape\nrownames                                          \nAlabama     1.255179  0.790787 -0.526195 -0.003451\nAlaska      0.513019  1.118060 -1.224067  2.509424\nArizona     0.072361  1.493817  1.009122  1.053466\nArkansas    0.234708  0.233212 -1.084492 -0.186794\nCalifornia  0.281093  1.275635  1.776781  2.088814\n\n\n\nFigura 12.1",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Análise de agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/exemplos/agrupamentos.html#preparação-dos-dados",
    "href": "src/exemplos/agrupamentos.html#preparação-dos-dados",
    "title": "12  Análise de agrupamentos",
    "section": "",
    "text": "Código\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.cluster.hierarchy import fcluster\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom tabulate import tabulate\n\n# Carregando o conjunto de dados\ndata = sm.datasets.get_rdataset(\"USArrests\", \"datasets\").data\n\n# Separando os dados e os nomes dos estados\nX = data.values\nstates = data.index\n\n# Padronizando os dados\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Criando um DataFrame com os dados padronizados para facilitar a manipulação\nX_scaled_df = pd.DataFrame(X_scaled, index=states, columns=data.columns)\n\nprint(\"Visualização dos 5 primeiros estados com dados padronizados:\")\nprint(X_scaled_df.head())\n\n\n\n12.1.1 Análise Descritiva\nAntes de iniciar o agrupamento, é sempre útil explorar a distribuição das variáveis. A figura abaixo mostra os histogramas para cada uma das quatro variáveis do conjunto de dados.\n\n\nCódigo\nfig, axes = plt.subplots(2, 2, figsize=(7, 5))\n\nsns.histplot(data['Murder'], ax=axes[0, 0], kde=True)\naxes[0, 0].set_title('Assassinatos')\n\nsns.histplot(data['Assault'], ax=axes[0, 1], kde=True)\naxes[0, 1].set_title('Agressões')\n\nsns.histplot(data['UrbanPop'], ax=axes[1, 0], kde=True)\naxes[1, 0].set_title('População Urbana (%)')\n\nsns.histplot(data['Rape'], ax=axes[1, 1], kde=True)\naxes[1, 1].set_title('Estupros')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nFigura 12.2: Distribuição das variáveis do dataset USArrests.\n\n\n\n\n\nObservamos que as variáveis de crime (Murder, Assault, Rape) parecem ter uma leve assimetria à direita, com a maioria dos estados concentrados em valores mais baixos. A variável UrbanPop tem uma distribuição mais simétrica, quase uniforme, indicando uma boa variedade nos níveis de urbanização entre os estados.\nAlém dos histogramas, podemos visualizar a matriz de correlação entre as variáveis para entender suas relações lineares.\n\n\nCódigo\ncorr_matrix = data.corr()\nplt.figure(figsize=(7, 5))\nsns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt='.2f')\nplt.show()\n\n\n\n\n\n\n\n\nFigura 12.3: Matriz de Correlação entre as variáveis.\n\n\n\n\n\nA matriz de correlação na Figura 12.3 mostra, como esperado, uma forte correlação positiva entre as três variáveis de crime (Murder, Assault, Rape). UrbanPop tem uma correlação positiva mais fraca com as outras variáveis, sugerindo que o efeito da urbanização no aumento da criminalidade geral é moderado.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Análise de agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/exemplos/agrupamentos.html#agrupamento-hierárquico-e-escolha-de-k",
    "href": "src/exemplos/agrupamentos.html#agrupamento-hierárquico-e-escolha-de-k",
    "title": "12  Análise de agrupamentos",
    "section": "12.2 Agrupamento Hierárquico e Escolha de K",
    "text": "12.2 Agrupamento Hierárquico e Escolha de K\nAgora, aplicamos o agrupamento hierárquico aglomerativo usando o método de Ward, que busca minimizar a variância dentro dos grupos a cada fusão. Em seguida, plotamos o dendrograma para nos ajudar a decidir o número ideal de grupos, \\(K\\).\n\n\nCódigo\n# Realizando o agrupamento hierárquico com o método de Ward\nlinked = linkage(X_scaled, method='ward')\n\n# Plotando o dendrograma\nplt.figure(figsize=(7, 5))\ndendrogram(linked,\n           orientation='top',\n           labels=states,\n           distance_sort='descending',\n           show_leaf_counts=True,\n           color_threshold=5.2)\nplt.xlabel('Estados')\nplt.ylabel('Distância de Ward')\nplt.axhline(y=5.2, color='r', linestyle='-.', label=\"Corte 1 (K=4)\")\nplt.axhline(y=10.5, color='grey', linestyle='--', label=\"Corte 2 (K=2)\")\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\n\n\n\n\nFigura 12.4: Dendrograma para o conjunto de dados USArrests usando o método de Ward.\n\n\n\n\n\nAnalisando o Figura 12.4, procuramos por um corte que cruze o maior espaço vertical possível. Vemos duas opções razoáveis, indicadas pelas linhas tracejadas. O “Corte 2” (cinza) sugere uma partição em \\(K=2\\) grupos, separando os estados em dois grandes blocos. O “Corte 1” (vermelho), mais abaixo, sugere uma partição mais granular de \\(K=4\\) grupos. Uma solução com 4 grupos nos dará um entendimento mais detalhado dos perfis dos estados. Portanto, iniciaremos a análise com \\(K=4\\) e, ao final deste exemplo, exploraremos a solução mais simples com \\(K=2\\) para fins de comparação.\nPara verificar a robustez dessa escolha, podemos comparar o resultado com o de outro método de ligação, como a ligação completa.\n\n\nCódigo\nlinked_complete = linkage(X_scaled, method='complete')\n\nplt.figure(figsize=(7, 5))\ndendrogram(linked_complete,\n           orientation='top',\n           labels=states,\n           distance_sort='descending',\n           show_leaf_counts=True)\nplt.xlabel('Estados')\nplt.ylabel('Distância')\nplt.show()\n\n\n\n\n\n\n\n\nFigura 12.5: Dendrograma para o conjunto de dados USArrests usando o método de Ligação Completa.\n\n\n\n\n\nO dendrograma de ligação completa também sugere uma partição de 2 ou 4 grupos como as mais sensatas.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Análise de agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/exemplos/agrupamentos.html#k-médias-com-centroides-hierárquicos",
    "href": "src/exemplos/agrupamentos.html#k-médias-com-centroides-hierárquicos",
    "title": "12  Análise de agrupamentos",
    "section": "12.3 K-Médias com Centroides Hierárquicos",
    "text": "12.3 K-Médias com Centroides Hierárquicos\nSeguindo o nosso procedimento, agora usaremos o resultado do agrupamento hierárquico para informar o algoritmo K-médias.\n\nObtemos as 4 partições (grupos) do método de Ward.\nCalculamos o centroide (média) de cada um desses 4 grupos.\nExecutamos o K-médias com \\(K=4\\), usando os centroides calculados como pontos de partida.\n\nIsso ajuda o K-médias a evitar mínimos locais e a convergir para uma solução mais estável e significativa.\n\n\nCódigo\n# 1. Obter os 4 grupos do modelo hierárquico\nk = 4\nhierarchical_grupos = fcluster(linked, k, criterion='maxclust')\n\n# Adicionar ao DataFrame para calcular os centroides\nX_scaled_df['hierarchical_grupo'] = hierarchical_grupos\n\n# 2. Calcular os centroides iniciais\ninitial_centroids = X_scaled_df.groupby('hierarchical_grupo').mean().values\n\n# 3. Executar o K-médias com os centroides iniciais\nkmeans = KMeans(n_clusters=k, init=initial_centroids, n_init=1, random_state=42)\nkmeans.fit(X_scaled_df.drop('hierarchical_grupo', axis=1))\n\n# Obter os grupos finais\nfinal_grupos = kmeans.labels_\n\n# Adicionar os grupos finais ao DataFrame original (não padronizado)\ndata['grupo'] = final_grupos",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Análise de agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/exemplos/agrupamentos.html#interpretação-e-visualização-dos-grupos",
    "href": "src/exemplos/agrupamentos.html#interpretação-e-visualização-dos-grupos",
    "title": "12  Análise de agrupamentos",
    "section": "12.4 Interpretação e Visualização dos Grupos",
    "text": "12.4 Interpretação e Visualização dos Grupos\nCom os grupos finais definidos, o passo mais importante é a interpretação. Calculamos a média de cada variável para cada grupo para criar um “perfil”.\n\nCódigo\n# Calcular as médias por grupo\ngrupo_profile = data.groupby('grupo').mean()\nprint(tabulate(grupo_profile, headers='keys', tablefmt='pipe'))\n\n\n\n\nTabela 12.1: Perfil dos grupos: médias das variáveis para cada grupo.\n\n\n\n|   grupo |   Murder |   Assault |   UrbanPop |    Rape |\n|--------:|---------:|----------:|-----------:|--------:|\n|       0 | 13.9375  |  243.625  |    53.75   | 21.4125 |\n|       1 | 10.9667  |  264      |    76.5    | 33.6083 |\n|       2 |  3.6     |   78.5385 |    52.0769 | 12.1769 |\n|       3 |  5.85294 |  141.176  |    73.6471 | 19.3353 |\n\n\n\n\nA Tabela 12.1 nos permite caracterizar cada grupo:\n\nGrupo 0 (Estados Perigosos): Este grupo tem os maiores índices de assassinatos e índices também altos de agressões e estupros. A população urbana é uma das mais baixas. Podemos nomeá-lo “Estados Violentos e Rurais”.\nGrupo 1 (Estados Urbanizados e Perigosos): Este grupo tem alta urbanização e níveis de criminalidade também muito altos, especialmente quanto a estupros e agressões. Um bom nome seria “Grandes Centros Urbanos Perigosos”.\nGrupo 2 (Estados Seguros e Rurais): Este grupo se destaca bastante dos anteriores. Apresenta os menores índices em todas as categorias de crime. A população urbana também é a mais baixa. Inclui estados como Dakota do Norte, Vermont e Iowa. Poderíamos chamá-lo de “Estados Seguros e Rurais”.\nGrupo 3 (Estados Intermediários): Este grupo é formado por estados urbanizados com menores índices de criminalidade, quando comparados aos grupos 0 e 1. Nele, nenhum extremo se destaca. Podemos chamá-lo de “Estados na Média”.\n\nPara visualizar a separação, usamos a Análise de Componentes Principais (ACP) para reduzir a dimensionalidade dos dados para 2D e plotamos os estados, colorindo-os por grupo.\n\n12.4.1 Interpretando os Componentes Principais\nAntes de visualizar o gráfico, é crucial entender o que os eixos (os componentes principais) representam. Eles são combinações lineares das variáveis originais. Podemos inspecionar os pesos (loadings) de cada variável para interpretar o significado de cada componente.\n\nCódigo\n# Reduzindo a dimensionalidade com ACP\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Variância explicada\nexplained_variance = pca.explained_variance_ratio_\n\n# Loadings\nloadings = pca.components_.T\nloadings_df = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=data.columns[:-1])\nprint(tabulate(loadings_df, headers='keys', tablefmt='pipe'))\n\n\n\n\nTabela 12.2: Cargas (loadings) dos componentes principais.\n\n\n\n|          |      PC1 |       PC2 |\n|:---------|---------:|----------:|\n| Murder   | 0.535899 | -0.418181 |\n| Assault  | 0.583184 | -0.187986 |\n| UrbanPop | 0.278191 |  0.872806 |\n| Rape     | 0.543432 |  0.167319 |\n\n\n\n\nA tabela Tabela 12.2 mostra as cargas das variáveis nos dois primeiros fatores.\nO primeiro componente (PC1) explica aproximadamente 62% da variância total, enquanto o segundo (PC2) explica cerca de 25%. Juntos, eles capturam 87% da informação original, o que é excelente para uma visualização 2D.\n\nComponente Principal 1 (PC1): Todas as quatro variáveis têm cargas positivas, com destaque para as variáveis associadas à criminalidade. Isso significa que ele representa uma medida geral de “Criminalidade”.\nComponente Principal 2 (PC2): Este componente mostra um contraste. Ele tem uma carga positiva forte para UrbanPop e uma carga negativa para Murder. Isso significa que PC2 separa estados urbanizados com menor índice de assassinatos (scores altos) de estados rurais com mais assassinatos (scores baixos).\n\nCom essa interpretação, podemos agora visualizar os grupos de forma mais informativa.\n\n\nCódigo\n# Criando um DataFrame para o plot\npca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\npca_df['grupo'] = final_grupos\npca_df['state'] = states\n\n# Plotando\nplt.figure(figsize=(7, 5))\nsns.scatterplot(x='PC1', y='PC2', hue='grupo', data=pca_df, palette='viridis', s=100)\n\n# Adicionando os nomes dos estados ao gráfico\nfor i in range(pca_df.shape[0]):\n    plt.text(x=pca_df.PC1[i]+0.05, y=pca_df.PC2[i], s=pca_df.state[i],\n             fontdict=dict(color='black',size=8))\n\nplt.title('Grupos de Estados dos EUA (Visualização com ACP)')\nplt.xlabel(f'Componente Principal 1 ({explained_variance[0]:.1%})')\nplt.ylabel(f'Componente Principal 2 ({explained_variance[1]:.1%})')\nplt.legend(title='Grupo')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigura 12.6: Visualização dos grupos no espaço dos dois primeiros componentes principais.\n\n\n\n\n\nO gráfico na Figura 12.6 mostra uma separação clara dos grupos. O primeiro componente principal (PC1, eixo horizontal) efetivamente separa os estados com base na “Criminalidade Geral”, com os grupos mais violentos (0 e 1) à direita e os mais seguros (2 e 3) à esquerda. O segundo componente principal (PC2, eixo vertical) está relacionado à “Urbanização”, posicionando os grupos mais urbanizados (1 e 3) na parte superior e os mais rurais (0 e 2) na parte inferior. A análise combinada forneceu uma partição clara e interpretável dos estados dos EUA com base em seus dados sociais de 1973.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Análise de agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/exemplos/agrupamentos.html#comparação-com-k2-grupos",
    "href": "src/exemplos/agrupamentos.html#comparação-com-k2-grupos",
    "title": "12  Análise de agrupamentos",
    "section": "12.5 Comparação com K=2 Grupos",
    "text": "12.5 Comparação com K=2 Grupos\nComo vimos no dendrograma, uma solução com \\(K=2\\) também é uma escolha justificável e representa a divisão de mais alto nível nos dados. Vamos repetir a etapa final do nosso procedimento para \\(K=2\\) e analisar o resultado.\n\nCódigo\n# 1. Obter os 2 grupos do modelo hierárquico\nk = 2\nhierarchical_grupos_k2 = fcluster(linked, k, criterion='maxclust')\n\n# Adicionar ao DataFrame para calcular os centroides\nX_scaled_df['hierarchical_grupo_k2'] = hierarchical_grupos_k2\ninitial_centroids_k2 = X_scaled_df.drop(columns=\"hierarchical_grupo\").groupby('hierarchical_grupo_k2').mean().values\n\n# 2. Executar K-médias\nkmeans_k2 = KMeans(n_clusters=k, init=initial_centroids_k2, n_init=1, random_state=42)\nkmeans_k2.fit(X_scaled_df.drop(['hierarchical_grupo', 'hierarchical_grupo_k2'], axis=1))\nfinal_grupos_k2 = kmeans_k2.labels_\n\n# 3. Calcular e exibir o perfil\ndata['grupo_k2'] = final_grupos_k2\ngrupo_profile_k2 = data.groupby('grupo_k2').mean().drop('grupo', axis=1)\nprint(tabulate(grupo_profile_k2, headers='keys', tablefmt='pipe'))\n\n\n\n\nTabela 12.3: Perfil dos grupos para a solução com K=2.\n\n\n\n|   grupo_k2 |   Murder |   Assault |   UrbanPop |    Rape |\n|-----------:|---------:|----------:|-----------:|--------:|\n|          0 |   12.165 |   255.25  |    68.4    | 29.165  |\n|          1 |    4.87  |   114.433 |    63.6333 | 15.9433 |\n\n\n\n\nCom \\(K=2\\), a partição é bem mais simples:\n\nGrupo 0: Agrega os estados que possuem níveis de criminalidade e urbanização mais elevados.\nGrupo 1: Engloba os estados mais seguros e rurais.\n\nEssa divisão é útil para uma visão macro, mas perde a granularidade que a solução com 4 grupos nos proporcionou, como a distinção entre os estados “intermediários” e os “grandes centros urbanos”. A visualização no espaço dos componentes principais ilustra isso claramente.\n\n\nCódigo\npca_df['grupo_k2'] = final_grupos_k2\n\nplt.figure(figsize=(7, 5))\nsns.scatterplot(x='PC1', y='PC2', hue='grupo_k2', data=pca_df, palette='viridis', s=100)\n\n# Adicionando os nomes dos estados ao gráfico\nfor i in range(pca_df.shape[0]):\n    plt.text(x=pca_df.PC1[i]+0.05, y=pca_df.PC2[i], s=pca_df.state[i],\n             fontdict=dict(color='black',size=8))\n\nplt.title('Grupos de Estados dos EUA (K=2, Visualização com ACP)')\nplt.xlabel(f'Componente Principal 1 ({explained_variance[0]:.1%})')\nplt.ylabel(f'Componente Principal 2 ({explained_variance[1]:.1%})')\nplt.legend(title='Grupo')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigura 12.7: Visualização dos grupos (K=2) no espaço dos componentes principais.",
    "crumbs": [
      "Exemplos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Análise de agrupamentos</span>"
    ]
  }
]