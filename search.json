[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Multivariada 2",
    "section": "",
    "text": "Prefácio\nSeja Bem-vindo! Este livro aborda uma variedade de técnicas de análise multivariada, essenciais para a compreensão de dados complexos em diversas áreas do conhecimento.\nEste material foi elaborado especialmente para estudantes tendo um primeiro contato com técnicas estatísticas multivariadas. São cobertos os métodos a seguir:\n\nAnálise de Componentes Principais\nAnálise Fatorial\nAnálise de Agrupamento\nAnálise Discriminante\nAnálise de Correlação Canônica\nAnálise de Correspondência\n\nO objetivo é fornecer uma base sólida e prática para a aplicação dessas técnicas. antes, temos uma breve introdução dos conceitos fundamentais que norteiam a análise multivariada e algumas definições e resultados vetores e matrizes que são importantes para o acompanhamento do livro.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html",
    "href": "src/01_intro/01_introducao.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Por que usar Análise Multivariada?\nA análise multivariada é o campo da estatística dedicado a compreender conjuntos de dados com múltiplas variáveis inter-relacionadas. Em vez de analisar cada variável isoladamente, seu foco é examinar simultaneamente as relações entre três ou mais variáveis para extrair padrões e estruturas que de outra forma permaneceriam ocultos.\nCada observação em um estudo — seja um paciente descrito por indicadores de saúde, um consumidor por hábitos de compra, ou uma empresa por métricas financeiras — pode ser representada como um vetor de observações. A análise multivariada nos fornece as ferramentas para entender a estrutura de dependência e interdependência dentro desses vetores.\nA análise multivariada é motivada pela necessidade de extrair informações significativas de conjuntos de dados complexos. Ao invés de analisar variáveis de forma isolada, essas técnicas permitem uma compreensão mais profunda e realista dos dados. Os principais objetivos são:\nNos próximos capítulos, construiremos a base teórica para atingir esses objetivos, começando pelo conceito de vetor aleatório e seus parâmetros, para depois explorarmos como as amostras de dados nos permitem estimar e analisar essas estruturas.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html#por-que-usar-análise-multivariada",
    "href": "src/01_intro/01_introducao.html#por-que-usar-análise-multivariada",
    "title": "1  Introdução",
    "section": "",
    "text": "Simplificação Estrutural: Reduzir a dimensionalidade dos dados, identificando as principais fontes de variação e eliminando redundâncias. Isso facilita a visualização e a interpretação de dados complexos, revelando a estrutura subjacente de forma mais clara.\nAgrupamento e Classificação: Organizar as observações em grupos homogêneos (agrupamento) ou atribuir observações a categorias predefinidas (classificação). O objetivo é identificar padrões que permitam segmentar os dados de maneira significativa.\nInvestigação de Estruturas de Dependência: Explorar e quantificar as relações entre variáveis. Isso inclui desde a análise de correlações simples até a modelagem de interações complexas entre múltiplos conjuntos de variáveis.\nPredição: Construir modelos para prever o valor de uma ou mais variáveis com base em outras.\nInferência: Realizar testes de hipóteses e inferências estatísticas sobre as relações em um contexto multivariado.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/01_introducao.html#visão-geral-das-técnicas-multivariadas",
    "href": "src/01_intro/01_introducao.html#visão-geral-das-técnicas-multivariadas",
    "title": "1  Introdução",
    "section": "1.2 Visão Geral das Técnicas Multivariadas",
    "text": "1.2 Visão Geral das Técnicas Multivariadas\nAs técnicas de análise multivariada podem ser classificadas com base em seus objetivos e na natureza das relações entre as variáveis. Uma distinção fundamental é entre técnicas de dependência, que analisam a relação entre variáveis dependentes e independentes, e técnicas de interdependência, que exploram as relações em um único conjunto de variáveis.\n\nTécnicas de Dependência: Analisam a relação entre uma ou mais variáveis dependentes e um conjunto de variáveis independentes. O objetivo é prever ou explicar o valor das variáveis dependentes.\nTécnicas de Interdependência: Exploram as relações entre todas as variáveis de um conjunto, sem fazer distinção entre dependentes e independentes. O foco é entender a estrutura geral dos dados.\n\nAlém disso a escolha de uma determinada técnica depende também dos tipos de variáveis em questão.\n\nVariáveis Categóricas (Qualitativas): Representam categorias ou grupos (e.g., gênero, tipo de produto).\nVariáveis Métricas (Quantitativas): Representam quantidades numéricas (e.g., idade, altura, renda, temperatura).\n\nCom o objetivo de classificar os métodos a serem apresentados nesse livro e posteriormente auxiliar na escolha da técnica mais adequada para o tratamento de um conjunto de dados, apresentamos a seguir uma tabela com algumas características de cada método e na sequência um fluxograma de decisão.\n\n\n\nTabela 1.1: Principais técnicas abordadas neste livro.\n\n\n\n\n\n\n\n\n\n\n\nTécnica\nObjetivo Principal\nTipo de Variável\nTipo de Análise\n\n\n\n\nComponentes Principais (PCA)\nRedução de dimensionalidade\nQuantitativas\nInterdependência\n\n\nAnálise Fatorial (FA)\nIdentificação de fatores latentes\nQuantitativas\nInterdependência\n\n\nAnálise de Agrupamento\nFormação de grupos homogêneos\nQuantitativas/Qualitativas\nInterdependência\n\n\nAnálise Discriminante\nClassificação de observações\nMista (Quali/Quanti)\nDependência\n\n\nCorrelação Canônica\nRelação entre conjuntos de variáveis\nQuantitativas\nDependência\n\n\nAnálise de Correspondência\nRelação entre variáveis categóricas\nQualitativas\nInterdependência\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\ncluster_blue_leaves\n\n\n\n\ncanonical_corr\n\nCorrelação Canônica\n\n\n\ndiscr_analysis\n\nAnálise Discriminante\n\n\n\nfactor_analysis\n\nAnálise Fatorial\n\n\n\npca\n\nComponentes Principais\n\n\n\ncluster_analysis\n\nAnálise de Agrupamento\n\n\n\ncorrespondence_analysis\n\nAnálise de Correspondência\n\n\n\nmulti_regression\n\nRegressão Múltipla\n\n\n\nmanova\n\nMANOVA\n\n\n\nlogistic_regression\n\nRegressão Logística\n\n\n\nrelation_type\n\nTipo de relação estudada\n\n\n\ndep_vs_indep\n\nDependência\n\n\n\nrelation_type-&gt;dep_vs_indep\n\n\n\n\n\ninterdep\n\nInterdependência\n\n\n\nrelation_type-&gt;interdep\n\n\n\n\n\ndep_var_type\n\nVariável Dependente?\n\n\n\ndep_vs_indep-&gt;dep_var_type\n\n\n\n\n\ngoal_interdep\n\nObjetivo?\n\n\n\ninterdep-&gt;goal_interdep\n\n\n\n\n\nnum_dependents\n\nQuantas Dependentes?\n\n\n\ndep_var_type-&gt;num_dependents\n\n\nMétrica\n\n\n\ngoal_cat_dep\n\nObjetivo?\n\n\n\ndep_var_type-&gt;goal_cat_dep\n\n\nCategórica\n\n\n\nnum_dependents-&gt;multi_regression\n\n\nUma\n\n\n\ngoal_multi_dep\n\nObjetivo?\n\n\n\nnum_dependents-&gt;goal_multi_dep\n\n\nMúltiplas\n\n\n\ngoal_multi_dep-&gt;canonical_corr\n\n\nRelacionar Conjuntos\n\n\n\ngoal_multi_dep-&gt;manova\n\n\nComparar Grupos\n\n\n\ngoal_cat_dep-&gt;discr_analysis\n\n\nClassificar\n\n\n\ngoal_cat_dep-&gt;logistic_regression\n\n\nPrever Probabilidade\n\n\n\ngoal_interdep-&gt;cluster_analysis\n\n\nAgrupar\n\n\n\nlatent_factors\n\nFatores Latentes?\n\n\n\ngoal_interdep-&gt;latent_factors\n\n\nReduzir Dimensão\n\n\n\ncategorical_vars\n\nVariáveis Categóricas?\n\n\n\ngoal_interdep-&gt;categorical_vars\n\n\nAssociar\n\n\n\nlatent_factors-&gt;factor_analysis\n\n\nSim\n\n\n\nlatent_factors-&gt;pca\n\n\nNão\n\n\n\ncategorical_vars-&gt;correspondence_analysis\n\n\nSim\n\n\n\n\n\n\nFigura 1.1: Diagrama de decisão para escolha de técnica de Análise Multivariada. Nós enfatizados fundo azul escuro indicam as técnicas de análise multivariada abordadas neste livro. Importante: Este diagrama é um guia simplificado para auxiliar na escolha da técnica mais adequada com base nas características dos dados e nos objetivos da análise. Ele não é exaustivo e serve apenas para posicionar as técnicas discutidas neste livro. A escolha final da técnica deve sempre considerar o contexto específico do problema e as características detalhadas dos dados.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html",
    "href": "src/01_intro/02_vetores_aleatorios.html",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "",
    "text": "2.1 O Vetor Aleatório\nNo capítulo anterior, estabelecemos a motivação para a análise multivariada: a necessidade de entender sistemas complexos onde múltiplas variáveis interagem. Para fazer isso de maneira formal e rigorosa, precisamos primeiro definir o objeto matemático central de nosso estudo. Em vez de começar com uma tabela de dados, começamos com o conceito que gera esses dados: o vetor aleatório.\nImagine que, para uma população de interesse (e.g., todos os estudantes de uma universidade), associamos a cada membro um conjunto de \\(p\\) características que nos interessam (e.g., nota em matemática, nota em história, horas de estudo). Antes de observarmos um membro específico, os valores dessas características são incertos. Podemos modelar essa incerteza tratando cada característica como uma variável aleatória.\nEste vetor é a representação matemática de uma “observação multivariada” em nível populacional. Toda a teoria da análise multivariada se baseia na compreensão das propriedades e da estrutura de distribuição deste vetor.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#o-vetor-aleatório",
    "href": "src/01_intro/02_vetores_aleatorios.html#o-vetor-aleatório",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "",
    "text": "Definição 2.1 Um vetor aleatório \\(\\mathbf{x}\\) é um vetor-coluna cujos componentes são \\(p\\) variáveis aleatórias, \\(X_1, X_2, \\ldots, X_p\\).\n\\[\n\\mathbf{x} = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\n\\vdots \\\\\nX_p\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\nCuidado com a Notação\n\n\n\n\nUma letra minúscula em negrito (e.g., \\(\\mathbf{x}\\)) denota um vetor aleatório.\nUma letra maiúscula comum (e.g., \\(X_j\\)) denota uma variável aleatória escalar, o \\(j\\)-ésimo componente do vetor.\nMais adiante, uma letra maiúscula em negrito (e.g., \\(\\mathbf{X}\\)) será usada para a matriz de dados (amostral).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#parâmetros-populacionais",
    "href": "src/01_intro/02_vetores_aleatorios.html#parâmetros-populacionais",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "2.2 Parâmetros Populacionais",
    "text": "2.2 Parâmetros Populacionais\nAssim como variáveis aleatórias escalares são caracterizadas por parâmetros como a média (expectativa) e a variância, os vetores aleatórios também o são. Esses parâmetros descrevem a tendência central, a dispersão e as inter-relações das variáveis que compõem o vetor.\n\nDefinição 2.2 O vetor de médias populacional, denotado por \\(\\boldsymbol{\\mu}\\), é o vetor das expectativas de cada uma de suas variáveis componentes.\n\\[\n\\boldsymbol{\\mu} = E[\\mathbf{x}] = \\begin{pmatrix}\nE[X_1] \\\\\nE[X_2] \\\\\n\\vdots \\\\\nE[X_p]\n\\end{pmatrix} = \\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{pmatrix}\n\\]\nGeometricamente, \\(\\boldsymbol{\\mu}\\) representa o centróide (centro de massa) da distribuição de probabilidade no espaço \\(p\\)-dimensional.\n\n\nDefinição 2.3 A matriz de covariâncias populacional, denotada por \\(\\boldsymbol{\\Sigma}\\), é uma matriz simétrica \\(p \\times p\\) cujo elemento \\((j, k)\\) é a covariância entre a \\(j\\)-ésima e a \\(k\\)-ésima variável aleatória, \\(\\sigma_{jk} = \\text{Cov}(X_j, X_k) = E[(X_j - \\mu_j)(X_k - \\mu_k)]\\).\n\\[\n\\boldsymbol{\\Sigma} = \\text{Cov}[\\mathbf{x}] = E[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})'] = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp}\n\\end{pmatrix}\n\\]\n\nDiagonal (\\(\\sigma_{jj}\\)): As variâncias, \\(\\text{Var}(X_j)\\), medem a dispersão de cada variável.\nFora da Diagonal (\\(\\sigma_{jk}\\)): As covariâncias, medem a tendência de associação linear entre as variáveis \\(X_j\\) e \\(X_k\\).\nSimetria: A matriz é simétrica, pois \\(\\text{Cov}(X_j, X_k) = \\text{Cov}(X_k, X_j)\\), o que implica \\(\\sigma_{jk} = \\sigma_{kj}\\).\n\n\n\nDefinição 2.4 A matriz de correlações populacional, denotada por \\(\\mathbf{P}\\), é uma versão reescalada da matriz de covariâncias, com elementos \\(\\rho_{jk} = \\frac{\\sigma_{jk}}{\\sqrt{\\sigma_{jj}}\\sqrt{\\sigma_{kk}}}\\).\n\\[\n\\mathbf{P} = \\begin{pmatrix}\n1 & \\rho_{12} & \\cdots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\cdots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nSeus elementos \\(\\rho_{jk}\\) variam de -1 a 1, fornecendo uma medida de associação linear livre de escala.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/02_vetores_aleatorios.html#propriedades-de-boldsymbolmu-e-boldsymbolsigma",
    "href": "src/01_intro/02_vetores_aleatorios.html#propriedades-de-boldsymbolmu-e-boldsymbolsigma",
    "title": "2  Vetores Aleatórios e Suas Características",
    "section": "2.3 Propriedades de \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)",
    "text": "2.3 Propriedades de \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)\nAs propriedades de combinações lineares são generalizações diretas dos resultados univariados. Seja \\(\\mathbf{x}\\) um vetor aleatório \\(p\\)-dimensional com média \\(\\boldsymbol{\\mu}\\) e covariância \\(\\boldsymbol{\\Sigma}\\). Sejam \\(\\mathbf{c}\\) um vetor de constantes \\(p \\times 1\\) e \\(\\mathbf{A}\\) uma matriz de constantes \\(q \\times p\\).\n\nEsperança de uma Combinação Linear: \\[\nE[\\mathbf{A}\\mathbf{x} + \\mathbf{c}] = \\mathbf{A}E[\\mathbf{x}] + \\mathbf{c} = \\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{c}\n\\]\nCovariância de uma Combinação Linear: \\[\n\\text{Cov}[\\mathbf{A}\\mathbf{x} + \\mathbf{c}] = \\mathbf{A} \\text{Cov}[\\mathbf{x}] \\mathbf{A}' = \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}'\n\\]\n\nNo próximo capítulo, veremos como, na prática, não temos acesso a esses parâmetros populacionais (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\Sigma}\\), \\(\\mathbf{P}\\)), mas podemos usar dados observados para obter estimativas confiáveis deles.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vetores Aleatórios e Suas Características</span>"
    ]
  },
  {
    "objectID": "src/01_intro/03_amostragem_e_estimacao.html",
    "href": "src/01_intro/03_amostragem_e_estimacao.html",
    "title": "3  Amostra e Estimação de Parâmetros",
    "section": "",
    "text": "3.1 Da População à Amostra\nNo capítulo anterior, introduzimos os conceitos teóricos que descrevem uma população multivariada: o vetor de médias \\(\\boldsymbol{\\mu}\\) e a matriz de covariâncias \\(\\boldsymbol{\\Sigma}\\). Esses parâmetros são construções ideais que existem no nível populacional. Na prática, quase nunca temos acesso a toda a população para calculá-los diretamente.\nO nosso trabalho como estatísticos e analistas de dados é fazer inferências sobre esses parâmetros desconhecidos com base em um conjunto limitado de dados. Fazemos isso através da amostragem.\nAssumimos que coletamos uma amostra aleatória de \\(n\\) observações da população. Cada observação, \\(\\mathbf{x}_i\\) (com \\(i=1, \\ldots, n\\)), é uma realização independente do vetor aleatório \\(\\mathbf{x}\\) que definimos no capítulo anterior.\nA coleção de todas essas observações forma o nosso conjunto de dados. É aqui que, finalmente, introduzimos a matriz de dados, \\(\\mathbf{X}\\), uma estrutura central em toda a análise multivariada aplicada.\nA matriz \\(\\mathbf{X}\\) é uma matriz de dimensão \\(n \\times p\\), onde cada linha é uma observação multivariada e cada coluna representa uma variável.\n\\[\n\\mathbf{X} = \\begin{pmatrix}\n\\mathbf{x}_1' \\\\\n\\mathbf{x}_2' \\\\\n\\vdots \\\\\n\\mathbf{x}_n'\n\\end{pmatrix} = \\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{pmatrix}\n\\tag{3.1}\\]\nO elemento \\(x_{ij}\\) representa o valor da \\(j\\)-ésima variável para a \\(i\\)-ésima observação. Com esta matriz em mãos, nosso objetivo é calcular quantidades que sirvam como boas estimativas para os parâmetros populacionais \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Amostra e Estimação de Parâmetros</span>"
    ]
  },
  {
    "objectID": "src/01_intro/03_amostragem_e_estimacao.html#estimadores-amostrais",
    "href": "src/01_intro/03_amostragem_e_estimacao.html#estimadores-amostrais",
    "title": "3  Amostra e Estimação de Parâmetros",
    "section": "3.2 Estimadores Amostrais",
    "text": "3.2 Estimadores Amostrais\nAs quantidades que calculamos a partir da amostra são chamadas de estatísticas amostrais ou estimadores, e são as contrapartes amostrais dos parâmetros populacionais.\n\nDefinição 3.1 O estimador de \\(\\boldsymbol{\\mu}\\) é o vetor de médias amostral, \\(\\bar{\\mathbf{x}}\\), cujos componentes \\(\\bar{x}_j\\) são a média das observações para a \\(j\\)-ésima variável.\n\\[\n\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}, \\quad \\text{resultando em} \\quad \\bar{\\mathbf{x}} = \\begin{pmatrix}\n\\bar{x}_1 \\\\\n\\vdots \\\\\n\\bar{x}_p\n\\end{pmatrix}\n\\]\n\n\nDefinição 3.2 O estimador de \\(\\boldsymbol{\\Sigma}\\) é a matriz de covariâncias amostral, \\(\\mathbf{S}\\). Seus elementos são a variância amostral (\\(s_{jj}\\)) e a covariância amostral (\\(s_{jk}\\)).\n\\(s_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)\\)\nA matriz resultante é:\n\\[\n\\mathbf{S} = \\begin{pmatrix}\ns_{11} & s_{12} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\cdots & s_{pp}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nPor que dividir por \\(n-1\\)?\n\n\n\nA divisão por \\(n-1\\) (graus de liberdade) em vez de \\(n\\) é feita para garantir que \\(s_{jk}\\) seja um estimador não-viesado de \\(\\sigma_{jk}\\), ou seja, \\(E[s_{jk}] = \\sigma_{jk}\\).\n\n\n\n\nDefinição 3.3 O estimador de \\(\\mathbf{P}\\) é a matriz de correlações amostral, \\(\\mathbf{R}\\), cujos elementos \\(r_{jk}\\) são obtidos padronizando a covariância amostral.\n\\(r_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}} = \\frac{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum_{i=1}^n (x_{ik} - \\bar{x}_k)^2}}\\)\nA matriz resultante é:\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{pmatrix}\n\\]\nA matriz \\(\\mathbf{R}\\) é uma matriz simétrica com 1s na diagonal.\n\n\nEm resumo: Neste capítulo, fizemos a ponte crucial entre a teoria e a prática. - No nível populacional, temos parâmetros teóricos e não observáveis (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\Sigma}\\)). - No nível amostral, temos dados observáveis na matriz \\(\\mathbf{X}\\), a partir da qual calculamos estatísticas (\\(\\bar{\\mathbf{x}}\\), \\(\\mathbf{S}\\)) que estimam esses parâmetros.\nA maior parte das técnicas que veremos neste livro opera sobre as matrizes \\(\\mathbf{S}\\) ou \\(\\mathbf{R}\\) para fazer inferências sobre a estrutura da população.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Amostra e Estimação de Parâmetros</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html",
    "href": "src/01_intro/04_normal_multivariada.html",
    "title": "4  A Distribuição Normal Multivariada",
    "section": "",
    "text": "4.1 A Função de Densidade\nAté agora, discutimos os parâmetros de um vetor aleatório (\\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\)) sem assumir uma forma específica para sua distribuição de probabilidade. No entanto, para desenvolvermos uma teoria de inferência estatística robusta e compreendermos o funcionamento de muitas técnicas clássicas, precisamos de um modelo de distribuição de referência. Na análise multivariada, esse papel é desempenhado pela distribuição Normal Multivariada (NMV).\nA NMV é uma generalização da distribuição normal (Gaussiana) para o caso de \\(p\\) variáveis. Ela é, de longe, a distribuição mais importante da análise multivariada, por várias razões: 1. Muitos fenômenos naturais podem ser aproximados pela NMV. 2. O Teorema do Limite Central, em sua forma multivariada, garante que a média de vetores aleatórios de (quase) qualquer distribuição tende a se comportar como uma NMV para amostras grandes. 3. Suas propriedades matemáticas são extremamente convenientes e bem compreendidas, o que facilita muito a derivação de resultados teóricos.\nUm vetor aleatório \\(\\mathbf{x}\\) de dimensão \\(p\\) segue uma distribuição Normal Multivariada com vetor de médias \\(\\boldsymbol{\\mu}\\) e matriz de covariâncias \\(\\boldsymbol{\\Sigma}\\) (positiva definida), denotado por \\(\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), se sua função de densidade de probabilidade (FDP) for dada por:\n\\[\nf(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right)\n\\]\nOnde: - \\(|\\boldsymbol{\\Sigma}|\\) é o determinante da matriz de covariâncias. - \\(\\boldsymbol{\\Sigma}^{-1}\\) é a inversa da matriz de covariâncias.\nApesar de parecer intimidante, a estrutura da FDP é bastante lógica. O termo no expoente, \\((\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\), é uma forma quadrática que mede a “distância” do ponto \\(\\mathbf{x}\\) ao centro \\(\\boldsymbol{\\mu}\\), ponderada pela estrutura de covariância \\(\\boldsymbol{\\Sigma}\\). Essa distância é chamada de distância de Mahalanobis. Quanto maior essa distância, menor o valor da função de densidade, o que faz todo o sentido.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Distribuição Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html#propriedades-da-distribuição-normal-multivariada",
    "href": "src/01_intro/04_normal_multivariada.html#propriedades-da-distribuição-normal-multivariada",
    "title": "4  A Distribuição Normal Multivariada",
    "section": "4.2 Propriedades da Distribuição Normal Multivariada",
    "text": "4.2 Propriedades da Distribuição Normal Multivariada\nA popularidade da NMV vem de suas propriedades elegantes:\n\nCombinações Lineares: Se \\(\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), então qualquer combinação linear de suas componentes, \\(\\mathbf{a}'\\mathbf{x} = a_1X_1 + \\dots + a_pX_p\\), segue uma distribuição normal univariada. Mais geralmente, se \\(\\mathbf{A}\\) é uma matriz de constantes, então \\(\\mathbf{Ax}\\) segue uma distribuição Normal Multivariada.\nDistribuições Marginais: Qualquer subconjunto de variáveis de um vetor Normal Multivariado também segue uma distribuição Normal Multivariada. Por exemplo, se \\(\\mathbf{x} = [X_1, X_2, X_3]'\\) é NMV, então o vetor \\([X_1, X_3]'\\) também é NMV.\nIndependência e Covariância Zero: Para a maioria das distribuições, covariância zero não implica independência. No entanto, para a NMV, essa implicação é verdadeira. Se um subconjunto de variáveis em um vetor NMV tem covariância zero com outro subconjunto, então esses dois subconjuntos são independentes. Esta é uma propriedade extremamente poderosa.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Distribuição Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/04_normal_multivariada.html#visualizando-a-normal-bivariada",
    "href": "src/01_intro/04_normal_multivariada.html#visualizando-a-normal-bivariada",
    "title": "4  A Distribuição Normal Multivariada",
    "section": "4.3 Visualizando a Normal Bivariada",
    "text": "4.3 Visualizando a Normal Bivariada\nPara ganhar intuição, é útil visualizar o caso bivariado (\\(p=2\\)). A função de densidade forma uma superfície em forma de sino no espaço 3D. Os contornos de densidade constante, quando projetados no plano \\((x_1, x_2)\\), formam elipses.\n\\[\n(\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) = c^2\n\\]\nA forma e a orientação dessas elipses são inteiramente determinadas pela matriz de covariâncias \\(\\boldsymbol{\\Sigma}\\).\n\nSe \\(\\sigma_{12} = 0\\) e \\(\\sigma_{11} = \\sigma_{22}\\): As variáveis são não correlacionadas (e, portanto, independentes) e têm a mesma variância. Os contornos são círculos.\nSe \\(\\sigma_{12} = 0\\) e \\(\\sigma_{11} \\neq \\sigma_{22}\\): As variáveis são não correlacionadas, mas com variâncias diferentes. Os contornos são elipses alinhadas com os eixos de coordenadas.\nSe \\(\\sigma_{12} \\neq 0\\): As variáveis são correlacionadas. Os contornos são elipses rotacionadas. A direção do eixo principal da elipse é determinada pelos autovetores de \\(\\boldsymbol{\\Sigma}\\), e o comprimento dos eixos é determinado pelos autovalores.\n\n\n\n\n\n\n\n\n\nG\n\ncluster_A\n\nσ₁₂ = 0, σ₁₁ = σ₂₂\n\n\ncluster_B\n\nσ₁₂ = 0, σ₁₁ &gt; σ₂₂\n\n\ncluster_C\n\nσ₁₂ &gt; 0\n\n\n\nA\n\n\n\n\nB\n\n\n\n\n\nC\n\nrotacionada\n\n\n\n\n\n\n\nFigura 4.1: Contornos de densidade para uma distribuição Normal Bivariada, ilustrando o efeito da matriz de covariância.\n\n\n\n\n\nEssa conexão entre a álgebra da matriz \\(\\boldsymbol{\\Sigma}\\) e a geometria da distribuição de dados é um dos temas mais importantes da análise multivariada e será a base para a técnica de Componentes Principais, que exploraremos mais adiante.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Distribuição Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html",
    "href": "src/01_intro/05_algebra_matricial.html",
    "title": "5  Fundamentos Matemáticos: Álgebra Matricial",
    "section": "",
    "text": "5.1 Formas Quadráticas\nCom os conceitos estatísticos fundamentais estabelecidos, voltamos nossa atenção para as ferramentas matemáticas necessárias para manipular esses objetos. A linguagem da análise multivariada é a álgebra linear.\nNeste capítulo, revisaremos conceitos-chave — formas quadráticas, matrizes positiva-definidas e a decomposição espectral — que são a base para muitas das técnicas que veremos, como a Análise de Componentes Principais (PCA).\nUma forma quadrática é uma função polinomial de várias variáveis que contém apenas termos de grau dois. Para um vetor \\(\\mathbf{x}\\) de dimensão \\(p \\times 1\\) e uma matriz simétrica \\(\\mathbf{A}\\) de dimensão \\(p \\times p\\), a forma quadrática é expressa como:\n\\[\nQ(\\mathbf{x}) = \\mathbf{x}' \\mathbf{A} \\mathbf{x} = \\sum_{i=1}^p \\sum_{j=1}^p a_{ij} x_i x_j\n\\]\nUm exemplo fundamental que já encontramos é a distância de Mahalanobis ao quadrado, \\((\\mathbf{x} - \\boldsymbol{\\mu})' \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\), que aparece no expoente da distribuição normal multivariada. Esta forma quadrática define as elipses de contorno de densidade constante da distribuição.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentos Matemáticos: Álgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html#matrizes-positiva-definidas",
    "href": "src/01_intro/05_algebra_matricial.html#matrizes-positiva-definidas",
    "title": "5  Fundamentos Matemáticos: Álgebra Matricial",
    "section": "5.2 Matrizes positiva-Definidas",
    "text": "5.2 Matrizes positiva-Definidas\nO conceito de positividade para um número escalar é estendido para matrizes através das formas quadráticas. Uma matriz simétrica \\(\\mathbf{A}\\) é dita:\n\npositiva-definida se \\(\\mathbf{x}' \\mathbf{A} \\mathbf{x} &gt; 0\\) para todos os vetores não-nulos \\(\\mathbf{x}\\).\npositiva-semidefinida se \\(\\mathbf{x}' \\mathbf{A} \\mathbf{x} \\geq 0\\) para todos os vetores não-nulos \\(\\mathbf{x}\\).\n\nPropriedades de uma matriz positiva-definida: - Todos os seus autovalores são estritamente positivos (\\(\\lambda_i &gt; 0\\)). - A matriz é invertível (não-singular). - Seu determinante é positivo.\nMatrizes de covariância (\\(\\boldsymbol{\\Sigma}\\)) e correlação (\\(\\mathbf{R}\\)) são, por natureza, positiva-semidefinidas. Para que a função de densidade da normal multivariada seja bem definida e a matriz \\(\\boldsymbol{\\Sigma}\\) seja invertível, exigimos que ela seja positiva-definida. Isso implica que nenhuma variável no vetor aleatório é uma combinação linear perfeita de outras (ou seja, não há redundância linear total nos dados).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentos Matemáticos: Álgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/05_algebra_matricial.html#sec-espectral",
    "href": "src/01_intro/05_algebra_matricial.html#sec-espectral",
    "title": "5  Fundamentos Matemáticos: Álgebra Matricial",
    "section": "5.3 Decomposição Espectral",
    "text": "5.3 Decomposição Espectral\nA decomposição espectral (ou de autovalores) é uma fatoração de uma matriz simétrica em seus autovalores e autovetores. Ela revela a estrutura fundamental da transformação linear representada pela matriz.\nToda matriz simétrica \\(\\mathbf{A}\\) de dimensão \\(p \\times p\\) pode ser reescrita como:\n\\[\n\\mathbf{A} = \\mathbf{E}\\Lambda\\mathbf{E}'\n\\]\nOnde:\n\n\\(\\lambda_1, \\dots, \\lambda_p\\) são os autovalores de \\(\\mathbf{A}\\).\n\\(\\mathbf{e}_1, \\dots, \\mathbf{e}_p\\) são os autovetores ortonormais correspondentes.\n\\(\\Lambda\\) é a matriz diagonal com os autovalores \\(\\lambda_i\\) na diagonal.\n\\(\\mathbf{E}\\) é a matriz ortogonal cujas colunas são os autovetores \\(\\mathbf{e}_i\\).\n\n\nExemplo 5.1 Vamos decompor a seguinte matriz de covariâncias \\(\\mathbf{S}\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2\n\\end{pmatrix}\n\\]\n\nAutovalores: Resolvendo a equação característica \\(\\det(\\mathbf{S} - \\lambda\\mathbf{I}) = 0\\), encontramos \\(\\lambda_1 = 3\\) e \\(\\lambda_2 = 1\\).\nAutovetores:\n\nPara \\(\\lambda_1 = 3\\): O autovetor correspondente é \\(\\mathbf{e}_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2}\n\\end{pmatrix}\\).\nPara \\(\\lambda_2 = 1\\): O autovetor correspondente é \\(\\mathbf{e}_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2}\n\\end{pmatrix}\\).\n\n\nA decomposição é \\(\\mathbf{S} = \\mathbf{E}\\Lambda\\mathbf{E}'\\), com: \\[\n\\mathbf{E} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2}\n\\end{pmatrix}, \\quad\n\\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1\n\\end{pmatrix}\n\\]\nIsso nos diz que a maior variância dos dados (igual a 3) está na direção do vetor \\((1, 1)\\), enquanto a variância na direção ortogonal \\((1, -1)\\) é menor (igual a 1).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentos Matemáticos: Álgebra Matricial</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html",
    "href": "src/01_intro/06_distancias.html",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "",
    "text": "6.1 Distâncias vs. Dissimilaridades\nUm conceito fundamental que permeia quase todas as técnicas de análise multivariada é a medição da “proximidade” ou “distância” entre observações. Seja para agrupar dados semelhantes, classificar uma nova observação ou entender a estrutura de um conjunto de dados, essas medidas determinam uma forma quantitativa para expressar o quão perto ou longe duas observações estão uma da outra no espaço p-dimensional.\nFormalmente, uma função \\(d(\\cdot, \\cdot)\\) é considerada uma métrica de distância se satisfaz as seguintes propriedades para quaisquer pontos \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}\\):\nNo entanto, em muitos contextos práticos, utilizamos medidas que não satisfazem todas essas propriedades, mas que ainda são extremamente úteis para quantificar o quão diferentes dois objetos são. Usamos o termo mais geral medida de dissimilaridade para nos referirmos a qualquer função que indique o grau de diferença entre dois pontos, onde valores pequenos indicam semelhança e valores grandes indicam diferença.\nUm exemplo clássico de uma medida de dissimilaridade que não é uma métrica de distância estrita é a distância Euclidiana quadrática, \\(d^2(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} - \\mathbf{y})'(\\mathbf{x} - \\mathbf{y})\\). Ela viola a propriedade da desigualdade triangular, mas pode ser usada em algoritmos como o K-médias e o método de Ward por suas convenientes propriedades computacionais (evitar o cálculo da raiz quadrada economiza tempo).\nNas seções a seguir, apresentamos algumas das medidas de dissimilaridade e distância mais populares. A escolha da medida ideal é um campo vasto e depende fundamentalmente da natureza dos dados e do objetivo da análise.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#distâncias-vs.-dissimilaridades",
    "href": "src/01_intro/06_distancias.html#distâncias-vs.-dissimilaridades",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "",
    "text": "Não-negatividade: \\(d(\\mathbf{x}, \\mathbf{y}) \\ge 0\\)\nIdentidade: \\(d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}\\)\nSimetria: \\(d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})\\)\nDesigualdade Triangular: \\(d(\\mathbf{x}, \\mathbf{z}) \\le d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z})\\)",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#medidas-para-dados-contínuos",
    "href": "src/01_intro/06_distancias.html#medidas-para-dados-contínuos",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "6.2 Medidas para Dados Contínuos",
    "text": "6.2 Medidas para Dados Contínuos\n\nDefinição 6.1 A Distância Euclidiana é a métrica de distância mais comum e corresponde à noção intuitiva de distância em linha reta entre dois pontos.\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2} = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_j)'(\\mathbf{x}_i - \\mathbf{x}_j)}\n\\]\n\n\nDefinição 6.2 A Distância de Manhattan (ou City-Block) calcula a distância como a soma das diferenças absolutas entre as coordenadas dos pontos. É como se deslocar entre dois pontos em uma cidade, movendo-se apenas ao longo das ruas (horizontais e verticais).\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sum_{k=1}^{p} |x_{ik} - x_{jk}|\n\\]\nEsta medida é, em geral, mais robusta a outliers do que a distância Euclidiana.\n\n\nDefinição 6.3 A Distância de Minkowski é uma generalização tanto da Euclidiana quanto da de Manhattan.\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\left( \\sum_{k=1}^{p} |x_{ik} - x_{jk}|^m \\right)^{1/m}\n\\]\n\nSe \\(m=2\\), temos a distância Euclidiana.\nSe \\(m=1\\), temos a distância de Manhattan.\n\nQuanto maior o valor de \\(m\\), mais peso é dado às maiores diferenças entre as coordenadas.\n\n\n\n\n\n\n\nLimitação das Distâncias Comuns\n\n\n\nAs distâncias Euclidiana, de Manhattan e de Minkowski são sensíveis às escalas das variáveis. Se uma variável tiver uma magnitude muito maior que as outras, ela dominará o cálculo da distância. Por isso, é prática comum padronizar as variáveis (subtrair a média e dividir pelo desvio padrão) antes de calcular a matriz de distâncias.\n\n\n\nA Distância de Mahalanobis é uma medida de distância estatística que leva em conta a correlação entre as variáveis e é invariante à escala.\n\\[\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_j)' \\mathbf{S}^{-1} (\\mathbf{x}_i - \\mathbf{x}_j)}\n\\]\nOnde \\(\\mathbf{S}^{-1}\\) é a inversa da matriz de covariâncias amostral. Ela mede a distância entre os pontos em unidades de desvio padrão, ajustando a contribuição de cada variável pela estrutura de covariância dos dados. Já encontramos essa forma quadrática no expoente da distribuição Normal Multivariada (Capítulo 4).",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#medidas-para-variáveis-binárias",
    "href": "src/01_intro/06_distancias.html#medidas-para-variáveis-binárias",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "6.3 Medidas para Variáveis Binárias",
    "text": "6.3 Medidas para Variáveis Binárias\nQuando os dados são binários (0 ou 1), a interpretação da distância muda. A distância Euclidiana quadrática, por exemplo, simplesmente conta o número de posições em que os dois vetores discordam.\n\\[\n(x_{ij} - x_{kj})^2 =\n\\begin{cases}\n0, & \\text{se } x_{ij} = x_{kj} \\\\\n1, & \\text{se } x_{ij} \\neq x_{kj}\n\\end{cases}\n\\]\nO problema é que essa abordagem dá o mesmo peso para uma concordância de 1-1 e uma concordância de 0-0. Em muitos contextos, a ausência conjunta de uma característica (concordância 0-0) é menos informativa do que a presença conjunta (concordância 1-1).\nPara lidar com isso, podemos construir uma tabela de contingência para duas observações \\(\\mathbf{x}_i\\) e \\(\\mathbf{x}_j\\):\n\n\n\n\nObservação j = 1\nObservação j = 0\nTotal\n\n\n\n\nObs i = 1\na\nb\na+b\n\n\nObs i = 0\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\np\n\n\n\nOnde: - a: número de variáveis onde \\(x_{ik}=1\\) e \\(x_{jk}=1\\). - d: número de variáveis onde \\(x_{ik}=0\\) e \\(x_{jk}=0\\). - b e c: número de variáveis onde há discordância.\nA distância Euclidiana quadrática corresponde a \\(b+c\\).\n\nDefinição 6.4 O Coeficiente de Jaccard é uma medida de similaridade para dados binários que ignora as concordâncias 0-0.\n\\[\nJ(\\mathbf{x}_i, \\mathbf{x}_j) = \\frac{a}{a+b+c}\n\\]\nA Distância de Jaccard é a sua contraparte de dissimilaridade, definida como \\(1 - J(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\n\nDefinição 6.5 O Coeficiente de Correspondência Simples (Simple Matching Coefficient, SMC) considera tanto as presenças (1-1) quanto as ausências (0-0) como concordâncias. É útil quando a ausência de uma característica é tão informativa quanto a sua presença.\n\\[\nSMC = \\frac{a+d}{a+b+c+d}\n\\]\n\n\nDefinição 6.6 O Coeficiente de Dice (ou Sørensen-Dice) é outra medida de similaridade que, assim como Jaccard, ignora as concordâncias 0-0. No entanto, ele dá um peso maior às concordâncias 1-1.\n\\[\nDice = \\frac{2a}{2a+b+c}\n\\]\n\n\nDefinição 6.7 O Coeficiente de Russell-Rao é uma medida mais simples que calcula a proporção de presenças conjuntas em relação ao total de variáveis.\n\\[\nRR = \\frac{a}{a+b+c+d}\n\\]",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/01_intro/06_distancias.html#matriz-de-distâncias",
    "href": "src/01_intro/06_distancias.html#matriz-de-distâncias",
    "title": "6  Medidas de Distância e Similaridade",
    "section": "6.4 Matriz de Distâncias",
    "text": "6.4 Matriz de Distâncias\nUma vez escolhida a medida de dissimilaridade, é comum pré-calcular todas as distâncias entre os pares de observações e organizá-las em uma matriz de distâncias \\(\\mathbf{D}\\), de dimensão \\(n \\times n\\).\n\\[\n\\mathbf{D} =\n\\begin{pmatrix}\n0 & d(\\mathbf{x}_1, \\mathbf{x}_2) & \\cdots & d(\\mathbf{x}_1, \\mathbf{x}_n) \\\\\nd(\\mathbf{x}_2, \\mathbf{x}_1) & 0 & \\cdots & d(\\mathbf{x}_2, \\mathbf{x}_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nd(\\mathbf{x}_n, \\mathbf{x}_1) & d(\\mathbf{x}_n, \\mathbf{x}_2) & \\cdots & 0\n\\end{pmatrix}\n\\]\nEsta matriz é simétrica, ou seja \\(d(\\mathbf{x}_i, \\mathbf{x}_j) = d(\\mathbf{x}_j, \\mathbf{x}_i)\\), e possui zeros na diagonal principal. Ela serve como a entrada para muitos algoritmos de agrupamento, especialmente os hierárquicos.",
    "crumbs": [
      "Parte I: Introdução",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas de Distância e Similaridade</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html",
    "href": "src/02_tec_mult/06_acp.html",
    "title": "7  Análise de Componentes Principais",
    "section": "",
    "text": "7.1 Variância como Medida de Informação\nA Análise de Componentes Principais (ACP ou PCA do acrônimo em inglês) é uma técnica estatística multivariada que transforma um conjunto de variáveis possivelmente correlacionadas em um novo conjunto de variáveis não correlacionadas, chamadas de componentes principais. O objetivo primário da ACP é a redução de dimensionalidade: representar a variabilidade presente nos dados originais com um número menor de variáveis, minimizando a perda de informação.\nCada componente principal é uma combinação linear das variáveis originais. O primeiro componente principal é construído para capturar a maior variabilidade possível nos dados. O segundo componente principal, ortogonal ao primeiro, captura a maior parte da variabilidade restante, e assim por diante. Ao final, o número de componentes principais é igual ao número de variáveis originais, mas a expectativa é que os primeiros componentes concentrem a maior parte da informação relevante.\nGeometricamente, a ACP é uma projeção do espaço original de variáveis para um outro espaço com características mais interessantes: A variância dos dados é concentrada em direções especificas (os componentes principais) e não existem correlações entre os novos eixos.\nPara construir a intuição geométrica, vamos começar com um exemplo simples, em duas dimensões.\nA essa altura, você deve estar se perguntando: por que a direção do “maior alongamento” é a mais importante? Em estatística, a variância é frequentemente usada como uma medida de informação. Uma variável com alta variância indica que seus valores são bem espalhados, o que nos ajuda a diferenciar as observações. Se a variância fosse zero, todos os pontos seriam idênticos, não nos fornecendo nenhuma informação sobre suas diferenças.\nA ACP utiliza essa ideia para encontrar os eixos mais informativos. Ao rotacionar o sistema de coordenadas, ela não altera a variabilidade total dos dados, mas a redistribui de forma inteligente.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#variância-como-medida-de-informação",
    "href": "src/02_tec_mult/06_acp.html#variância-como-medida-de-informação",
    "title": "7  Análise de Componentes Principais",
    "section": "",
    "text": "Definição 7.1 A variância total de um conjunto de dados com \\(p\\) variáveis é a soma das variâncias de cada variável individual. Matematicamente, se \\(\\mathbf{x} = (X_1, \\dots, X_p)'\\) é o vetor de variáveis aleatórias com matriz de covariâncias \\(\\mathbf{\\Sigma}\\), a variância total é definida como:\n\\[\n\\text{Variância Total} = \\sum_{j=1}^{p} \\text{Var}(X_j) = \\sum_{j=1}^{p} \\sigma_{jj} = tr(\\mathbf{\\Sigma})\n\\]\nOnde \\(\\sigma_{jj}\\) é a variância da \\(j\\)-ésima variável e \\(tr(\\mathbf{\\Sigma})\\) é o traço da matriz de covariâncias (a soma dos elementos da diagonal principal). Essa medida representa a dispersão total na nuvem de pontos, somando a variabilidade em cada uma das direções dos eixos originais.\n\n\nExemplo 7.2 Voltando ao exemplo Exemplo 7.1, as variâncias das variáveis originais são:\n\nVariância do Peso: 59.88\nVariância da Altura: 66.71\nVariância Total Original: 126.59\n\nApós a rotação, as variâncias ao longo dos novos eixos (os componentes principais) são:\n\nVariância de \\(CP_1\\): 123.55\nVariância de \\(CP_2\\): 3.04\nVariância Total dos Componentes: 126.59\n\nDois fatos cruciais se destacam:\n\nA variância total é conservada. A soma das variâncias é a mesma nos dois sistemas de eixos. Nenhuma informação foi perdida; o ponto de vista foi apenas alterado.\nA variância foi eficientemente redistribuída. O primeiro componente, \\(CP_1\\), agora concentra 97.60% da variância total. Isso significa que, se quiséssemos reduzir nossos dados de 2D para 1D, poderíamos manter apenas o \\(CP_1\\) e ainda reter a maior parte da informação original. Essa é a essência da redução de dimensionalidade com ACP.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#a-formalização-matemática",
    "href": "src/02_tec_mult/06_acp.html#a-formalização-matemática",
    "title": "7  Análise de Componentes Principais",
    "section": "7.2 A Formalização Matemática",
    "text": "7.2 A Formalização Matemática\nCom a intuição geométrica estabelecida, podemos formalizar a Análise de Componentes Principais. O objetivo é transformar um conjunto de variáveis correlacionadas \\(\\mathbf{x} = (X_1, \\dots, X_p)'\\) em um novo conjunto de variáveis não correlacionadas, os componentes principais \\(\\mathbf{y} = (Y_1, \\dots, Y_p)'\\). Cada componente é uma combinação linear das variáveis originais:\n\\[\n\\begin{aligned}\nY_1 &= e_{11}X_1 + e_{12}X_2 + \\dots + e_{1p}X_p = \\mathbf{e}_1' \\mathbf{x} \\\\\nY_2 &= e_{21}X_1 + e_{22}X_2 + \\dots + e_{2p}X_p = \\mathbf{e}_2' \\mathbf{x} \\\\\n&\\vdots \\\\\nY_p &= e_{p1}X_1 + e_{p2}X_2 + \\dots + e_{pp}X_p = \\mathbf{e}_p' \\mathbf{x}\n\\end{aligned}\n\\]\nEm notação matricial, a transformação pode ser escrita de forma compacta:\n\\[\n\\mathbf{Y} = \\mathbf{E}' \\mathbf{X}\n\\tag{7.1}\\]\nOnde \\(\\mathbf{Y}\\) é o vetor \\(p \\times 1\\) de autovalores e \\(\\mathbf{E}\\) é a matriz \\(p \\times p\\) cujas colunas são os vetores de coeficientes \\(\\mathbf{e}_k\\).\nEsses componentes são construídos para satisfazer duas condições fundamentais:\n\nVariâncias Ordenadas: A variância do primeiro componente é a maior possível, a do segundo é a maior possível entre as direções não correlacionadas com o primeiro, e assim por diante. Ou seja, \\(\\text{Var}(Y_1) \\ge \\text{Var}(Y_2) \\ge \\dots \\ge \\text{Var}(Y_p)\\).\nNão Correlacionados: Os componentes são ortogonais entre si, o que significa que \\(\\text{Cov}[Y_i, Y_k] = 0\\) para todo \\(i \\neq k\\).\n\n\n7.2.1 O Problema de Maximização\nO primeiro componente principal, \\(Y_1 = \\mathbf{e}_1'\\mathbf{x}\\), é a combinação linear com variância máxima. A variância de \\(Y_1\\) é dada por:\n\\[\n\\text{Var}(Y_1) = \\text{Var}(\\mathbf{e}_1'\\mathbf{x}) = \\mathbf{e}_1' \\text{Var}(\\mathbf{x}) \\mathbf{e}_1 = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1\n\\]\nOnde \\(\\mathbf{\\Sigma}\\) é a matriz de covariâncias de \\(\\mathbf{x}\\). Para evitar que a variância seja aumentada simplesmente inflando os coeficientes em \\(\\mathbf{e}_1\\), impomos a restrição de que seu comprimento seja unitário, \\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\). Formalmente, o problema de maximização para o primeiro componente principal se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_1} \\quad & \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 \\\\\n    \\text{sujeito a} \\quad & \\mathbf{e}_1' \\mathbf{e}_1 = 1\n\\end{aligned}\n\\]\nPara maximizar a variância sujeita à restrição, utilizamos o método dos multiplicadores de Lagrange. A função a ser maximizada é:\n\\[\nL(\\mathbf{e}_1, \\lambda_1) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 - \\lambda_1 (\\mathbf{e}_1' \\mathbf{e}_1 - 1)\n\\]\nDerivando em relação a \\(\\mathbf{e}_1\\) e igualando a zero, obtemos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_1} = 2 \\mathbf{\\Sigma} \\mathbf{e}_1 - 2 \\lambda_1 \\mathbf{e}_1 = 0\n\\]\nO que nos leva à equação fundamental de autovalores e autovetores:\n\\[\n\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\n\\]\nEsta equação mostra que o vetor de coeficientes \\(\\mathbf{e}_1\\) deve ser um autovetor da matriz de covariâncias \\(\\mathbf{\\Sigma}\\). Para encontrar a variância, pré-multiplicamos a equação por \\(\\mathbf{e}_1'\\):\n\\[\n\\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1' \\mathbf{e}_1\n\\]\nComo \\(\\text{Var}(Y_1) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_1\\) e a restrição é \\(\\mathbf{e}_1' \\mathbf{e}_1 = 1\\), temos:\n\\[\n\\text{Var}(Y_1) = \\lambda_1\n\\]\nPara maximizar a variância de \\(Y_1\\), devemos escolher o maior autovalor possível. Portanto, \\(\\lambda_1\\) é o maior autovalor de \\(\\mathbf{\\Sigma}\\), e \\(\\mathbf{e}_1\\) é o autovetor correspondente.\n\n\n\n\n\n\nNota\n\n\n\nA matriz de covariâncias \\(\\mathbf{\\Sigma}\\) é, por construção, uma matriz simétrica e positiva semi-definida. Conforme discutido em Seção 5.3, o Teorema Espectral garante que os autovalores de tal matriz são reais e não-negativos, e que seus autovetores correspondentes a autovalores distintos são ortogonais. Esta propriedade é fundamental para a existência e unicidade dos componentes principais.\n\n\n\n\n\n\n\n\nNota\n\n\n\nA demonstração acima, utilizando multiplicadores de Lagrange, é uma maneira moderna e elegante de conduzir a derivação do problema de máximização. Uma abordagem clássica restringe a norma de \\(e\\) através do quociente,\n\\[\n\\text{Var}(Y_1) = \\max_{e_1} \\frac{\\mathbf{e}_1' \\Sigma \\mathbf{e}_1}{\\mathbf{e}_1' \\mathbf{e_1}}\n\\]\nEste é um problema clássico na álgebra linear. Um teorema fundamental afirma que para qualquer matriz simétrica \\(A\\), o máximo da forma quadrática \\(\\mathbf{x}' A \\mathbf{x}\\), sujeito à restrição \\(\\mathbf{x}' \\mathbf{x} = 1\\), é o maior autovalor de \\(A\\). O vetor \\(\\mathbf{x}\\) que atinge esse máximo é o autovetor correspondente. Como a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) é simétrica, este teorema se aplica diretamente ao nosso problema.\n\n\n\n\n7.2.2 Componentes Subsequentes\nUma vez encontrada a primeira direção de máxima variância, o segundo componente principal, \\(Y_2 = \\mathbf{e}_2'\\mathbf{x}\\), busca capturar o máximo da variabilidade restante, sob a condição de ser não correlacionado com \\(Y_1\\). A condição de componentes não correlacionados garante que a informação presente no segundo componente principal não é redundante com relação aquela já presente no primeiro. Formalmente, temos:\n\\[\nCov(Y_1, Y_2) = Cov(\\mathbf{e}'_1 \\mathbf{x}, \\mathbf{e}'_2 \\mathbf{x}) = \\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_2\n\\]\nComo \\(\\mathbf{e}_1\\) é o primeiro autovetor, temos \\(\\mathbf{\\Sigma} \\mathbf{e}_1 = \\lambda_1 \\mathbf{e}_1\\), Assim:\n\\[\n\\mathbf{e}_1' \\mathbf{\\Sigma} \\mathbf{e}_2 = (\\lambda_1 \\mathbf{e}_1)' \\mathbf{e}_2 = \\lambda_1 \\mathbf{e}_1' \\mathbf{e}_2\n\\]\nLogo:\n\\[\nCov(Y_1, Y_2) = 0 \\iff \\mathbf{e}_1' \\mathbf{e}_2 = 0\n\\]\nCom essa condição bem definida, o problema para o segundo componente se torna:\n\\[\n\\begin{aligned}\n    \\max_{\\mathbf{e}_2} \\quad & \\mathbf{e}_2' \\mathbf{\\Sigma} \\mathbf{e}_2 \\\\\n    \\text{sujeito a} \\quad & \\begin{cases}\n        \\mathbf{e}_2' \\mathbf{e}_2 = 1 \\\\\n        \\mathbf{e}_1' \\mathbf{e}_2 = 0\n    \\end{cases}\n\\end{aligned}\n\\]\nA função Lagrangiana agora inclui dois multiplicadores, \\(\\lambda_2\\) e \\(\\phi\\):\n\\[\nL(\\mathbf{e}_2, \\lambda_2, \\phi) = \\mathbf{e}_2' \\mathbf{\\Sigma} \\mathbf{e}_2 - \\lambda_2(\\mathbf{e}_2' \\mathbf{e}_2 - 1) - \\phi(\\mathbf{e}_1' \\mathbf{e}_2 - 0)\n\\]\nDerivando em relação a \\(\\mathbf{e}_2\\) e igualando a zero, temos:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{e}_2} = 2\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_2 - \\phi\\mathbf{e}_1 = \\mathbf{0}\n\\]\nPré-multiplicando por \\(\\mathbf{e}_1'\\):\n\\[\n2\\mathbf{e}_1'\\mathbf{\\Sigma}\\mathbf{e}_2 - 2\\lambda_2\\mathbf{e}_1'\\mathbf{e}_2 - \\phi\\mathbf{e}_1'\\mathbf{e}_1 = 0\n\\]\nSabendo que:\n\n\\(\\mathbf{e}_1'\\mathbf{\\Sigma} = \\lambda_1\\mathbf{e}_1'\\)\n\\(\\mathbf{e}_1'\\mathbf{e}_2 = 0\\)\n\\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\)\n\nA equação se simplifica a \\(\\phi = 0\\). Substituindo \\(\\phi=0\\) de volta na derivada, a equação se torna:\n\\[\n\\mathbf{\\Sigma}\\mathbf{e}_2 = \\lambda_2\\mathbf{e}_2\n\\]\nAssim, \\(\\mathbf{e}_2\\) é o autovetor de \\(\\mathbf{\\Sigma}\\) correspondente ao autovalor \\(\\lambda_2\\). Como \\(\\lambda_1\\) foi o maior autovalor, para maximizar a variância de \\(Y_2\\), \\(\\lambda_2\\) deve ser o segundo maior autovalor. Este processo se generaliza para os componentes subsequentes.\nEste processo continua: o \\(k\\)-ésimo componente principal (\\(Y_k\\)) é definido pelo autovetor \\(\\mathbf{e}_k\\) associado ao \\(k\\)-ésimo maior autovalor \\(\\lambda_k\\), garantindo que \\(\\text{Var}(Y_k) = \\lambda_k\\) e que todos os componentes sejam mutuamente não correlacionados.\nÉ neste ponto que a conexão com a Seção 5.3 se torna explícita. A matriz \\(\\mathbf{P}\\) (Equação 7.1), cujas colunas são os autovetores da matriz de covariâncias \\(\\mathbf{\\Sigma}\\), é exatamente a mesma matriz \\(\\mathbf{P}\\) da decomposição espectral \\(\\mathbf{\\Sigma} = \\mathbf{P}\\Lambda\\mathbf{P}'\\). Além disso, \\(\\Lambda\\) é uma matriz diagonal contendo a variância de cada componente principal. Logo, podemos obter todos os componentes principais de maneira prática e simultânea através da decomposição espectral.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#a-importância-do-pré-processamento-dos-dados",
    "href": "src/02_tec_mult/06_acp.html#a-importância-do-pré-processamento-dos-dados",
    "title": "7  Análise de Componentes Principais",
    "section": "7.3 A Importância do Pré-processamento dos Dados",
    "text": "7.3 A Importância do Pré-processamento dos Dados\nA Análise de Componentes Principais é, em sua essência, uma análise de variabilidade. A forma como medimos essa variabilidade impacta diretamente o resultado. Dois pré-processamentos são cruciais: a centralização e o escalonamento.\n\n7.3.1 Centralização\nNa Análise de Componentes Principais (ACP), a centralização dos dados — ou seja, a subtração da média de cada variável — é uma etapa fundamental não apenas para o cálculo da matriz de covariâncias, mas também para a projeção dos dados nos componentes principais.\nAo projetar os dados em um componente \\(\\mathbf{e}_i\\), é imprescindível que a projeção seja feita a partir dos dados centralizados, ou seja:\n\\[\nY_i = \\mathbf{e}_i^𝑇(\\mathbf{x} − \\bar{\\mathbf{{x}}})\n\\]\nEsse detalhe é essencial porque os autovetores da ACP são obtidos com base na matriz de covariâncias, a qual descreve a dispersão dos dados em torno da média, e não em torno da origem. Se aplicarmos a projeção diretamente sobre \\(\\mathbf{x}\\), sem subtrair a média, os componentes resultantes não representarão adequadamente as direções de maior variabilidade — e sim uma combinação da dispersão com a posição média dos dados.\nPortanto, para que os componentes principais preservem a interpretação correta como combinações lineares que explicam a variância dos dados em torno do centro da nuvem de pontos, é indispensável que tanto o cálculo da matriz de covariâncias quanto a projeção dos dados utilizem os dados centralizados.\n\n\n\n\n\n\nImportante\n\n\n\nNo contexto de ACP, é comum e prático denotar por \\(\\mathbf{x}\\) o vetor de variáveis já centralizado. Utilizamos esse abuso de notação durante esse capítulo para simplificação do texto sem perda de generalidade.\n\n\n\n\n7.3.2 Por que Escalonar? O Dilema da Covariância vs. Correlação\nA Análise de Componentes Principais (ACP) é sensível à escala das variáveis. Se uma variável tiver uma variância numericamente muito maior que as outras — mesmo que apenas por causa da sua unidade de medida — ela poderá dominar os primeiros componentes principais.\nImagine incluir uma terceira variável no conjunto Altura/Peso: a renda mensal, medida em Reais. As variâncias poderiam ser aproximadamente:\n\nAltura: 80 cm²\n\nPeso: 60 kg²\n\nRenda: 4.000.000 (RS)²\n\nNesse cenário, a variância da Renda é milhares de vezes maior que a das outras variáveis. Se aplicarmos a ACP diretamente na matriz de covariâncias, o primeiro componente principal será fortemente direcionado pela Renda, mesmo que sua correlação com as demais variáveis seja baixa. Isso ocorre porque a ACP estará apenas “seguindo” a direção da variável com maior variância — não necessariamente a mais informativa.\nPara evitar esse viés, escalonamos as variáveis: cada uma é dividida por seu desvio padrão. Isso padroniza todas para variância igual a 1. Ao fazer isso, estamos na prática realizando a ACP sobre a matriz de correlação \\((\\mathbf{R})\\) em vez da matriz de covariâncias \\((\\mathbf{\\Sigma})\\).\nVantagem do escalonamento:\nUsar a matriz de correlação “democratiza” a análise. Todas as variáveis começam com a mesma importância inicial (variância 1), e a ACP passa a capturar a estrutura de correlações, ao invés de ser enviesada pelas diferenças de escala.\nQuando usar a matriz de covariâncias?\nSomente quando todas as variáveis estão na mesma unidade de medida e possuem uma interpretação comparável. Por exemplo, comparar a temperatura em Celsius em diferentes regiões pode fazer sentido sem escalonamento. Fora isso, a matriz de correlação é geralmente a escolha mais robusta e segura.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "href": "src/02_tec_mult/06_acp.html#componentes-principais-populacionais-vs.-amostrais",
    "title": "7  Análise de Componentes Principais",
    "section": "7.4 Componentes Principais Populacionais vs. Amostrais",
    "text": "7.4 Componentes Principais Populacionais vs. Amostrais\nAté este ponto, discutimos os componentes principais em um contexto populacional, onde a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) (ou correlação \\(\\mathbf{R}\\)) e seus autovalores \\(\\lambda_k\\) e autovetores \\(\\mathbf{e}_k\\) são conhecidos. Na prática, quase sempre trabalhamos com uma amostra de dados. Nesse caso, não conhecemos os verdadeiros parâmetros populacionais e devemos estimá-los.\nOs componentes principais amostrais são obtidos da mesma maneira, mas usando a matriz de covariâncias amostral \\(\\mathbf{S}\\) (ou a matriz de correlação amostral \\(\\mathbf{R}\\)). As quantidades resultantes são estimativas dos seus análogos populacionais:\n\nO \\(k\\)-ésimo autovalor amostral, \\(\\hat{\\lambda}_k\\), é uma estimativa de \\(\\lambda_k\\).\nO \\(k\\)-ésimo autovetor amostral, \\(\\hat{\\mathbf{e}}_k\\), é uma estimativa de \\(\\mathbf{e}_k\\).\nO \\(k\\)-ésimo componente principal amostral, \\(\\hat{Y}_k = \\hat{\\mathbf{e}}_k' \\mathbf{x}\\), é uma estimativa de \\(Y_k\\).\n\nA teoria e a interpretação permanecem as mesmas. Para simplificar a notação, ao longo deste capítulo, omitimos o acento circunflexo (\\(\\hat{\\phantom{a}}\\)), mas é importante lembrar que, na aplicação prática, estamos sempre lidando com estimativas amostrais.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#escolhendo-o-número-de-componentes",
    "href": "src/02_tec_mult/06_acp.html#escolhendo-o-número-de-componentes",
    "title": "7  Análise de Componentes Principais",
    "section": "7.5 Escolhendo o Número de Componentes",
    "text": "7.5 Escolhendo o Número de Componentes\nA principal vantagem da ACP é a redução de dimensionalidade. Mas como decidimos quantos componentes (\\(q &lt; p\\)) reter? A escolha de \\(q\\) envolve um trade-off entre a simplicidade (poucos componentes) e a fidelidade aos dados originais (muitos componentes). Não existe uma regra única, mas sim um conjunto de critérios que devem ser avaliados em conjunto.\n\n7.5.1 Critério da Variância Explicada Acumulada\nEste é o critério mais comum. Calculamos a proporção da variância total explicada por cada componente e acumulamos essa proporção.\n\\[\n\\text{Proporção da Variância por } CP_k = \\frac{\\lambda_k}{\\sum_{j=1}^{p} \\lambda_j}\n\\]\nEm seguida, escolhemos o menor número de componentes \\(q\\) cuja variância explicada acumulada atinja um limiar satisfatório, geralmente entre 70% e 90%. A escolha do limiar depende do contexto da análise.\n\n\n7.5.2 Critério do Autovalor (Critério de Kaiser)\nProposto por Henry Kaiser, este critério sugere reter apenas os componentes cujos autovalores (\\(\\lambda_k\\)) são maiores que 1. A intuição por trás dessa regra é mais clara quando a ACP é aplicada sobre a matriz de correlação. Nesse caso, as variáveis originais são padronizadas para ter variância 1. Um componente com autovalor (variância) menor que 1 está, portanto, explicando menos variabilidade do que uma única variável original. Reter tal componente não traria uma “economia” de informação”, tornando-o um candidato à exclusão.\n\n\n7.5.3 Scree Plot (Gráfico de Cotovelo)\nO Scree Plot, proposto por Raymond Cattell, é uma ferramenta visual que nos ajuda a identificar o número ideal de componentes. Ele é um gráfico de linha dos autovalores (variâncias dos componentes) em ordem decrescente.\nTipicamente, o gráfico mostra uma queda acentuada nos primeiros autovalores, seguida por um nivelamento gradual para os autovalores restantes. O ponto onde a curva “dobra” ou forma um “cotovelo” (elbow) é considerado o ponto de corte. A ideia é reter os componentes que aparecem antes do cotovelo, pois eles são os que contribuem mais significativamente para a variância total. Os componentes após o cotovelo formam o “cascalho” (scree) na base de uma montanha e são considerados “ruído”.\n\n\n\n\n\n\n\n\nFigura 7.3: Exemplo de um Scree Plot. O ‘cotovelo’ em k=3 sugere a retenção de 3 componentes.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/06_acp.html#interpretando-os-componentes-principais",
    "href": "src/02_tec_mult/06_acp.html#interpretando-os-componentes-principais",
    "title": "7  Análise de Componentes Principais",
    "section": "7.6 Interpretando os Componentes Principais",
    "text": "7.6 Interpretando os Componentes Principais\nUma vez que selecionamos o número de componentes a reter, o passo final é a interpretação. O que esses novos eixos, que são combinações de nossas variáveis originais, realmente significam?\nOs coeficientes \\(e_{kj}\\) do autovetor \\(\\mathbf{e}_k\\) são chamados de cargas (loadings) e representam o peso da variável original \\(X_j\\) na formação do componente \\(Y_k\\). Embora as cargas sejam importantes, a sua interpretação pode ser complicada, pois sua magnitude depende das unidades das variáveis originais.\nUma medida mais interpretável é a correlação entre os componentes principais e as variáveis originais, \\(Cor(Y_k, X_j)\\). Ela nos diz o quão “alinhado” um componente está com cada variável original, numa escala padronizada de -1 a 1. A fórmula para essa correlação é:\n\\[\nCor(Y_k, X_j) = \\frac{e_{kj} \\sqrt{\\lambda_k}}{\\sqrt{s_{jj}}}\n\\]\nOnde:\n\n\\(e_{kj}\\) é a carga da variável \\(j\\) no componente \\(k\\).\n\\(\\lambda_k\\) é o autovalor (variância) do componente \\(k\\).\n\\(s_{jj}\\) é a variância da variável original \\(j\\).\n\nQuando a ACP é realizada sobre a matriz de correlação (ou seja, com dados padronizados), as variâncias \\(s_{jj}\\) são todas iguais a 1. Nesse caso, a fórmula simplifica para \\(Cor(Y_k, X_j) = e_{kj} \\sqrt{\\lambda_k}\\). As correlações se tornam proporcionais às cargas, facilitando a interpretação.\nAlém disso, quando a ACP é realizada sobre a matriz de correlações, as variáveis são padronizadas. Nesse caso, uma opção comum e direta é avaliar os próprios loadings (os autovetores da matriz de correlação) para entender a contribuição de cada variável. Um loading alto (próximo de 1 ou -1) indica que a variável tem uma forte influência na construção daquele componente.\nA etapa mais crucial da ACP é transformar os eixos matemáticos (os componentes) em descobertas práticos. A ferramenta visual mais adequada para essa tarefa é o biplot. O termo “biplot” significa “dois plots” (plot duplo), pois ele sobrepõe duas informações em um único gráfico:\n\nOs scores: As coordenadas das observações no novo espaço dos componentes principais.\nOs loadings: As contribuições das variáveis originais para a criação desses componentes.\n\nO resultado é um mapa rico que mostra não apenas como as observações se agrupam, mas por que elas se agrupam daquela maneira. A interpretação de um biplot segue uma lógica visual. Vamos quebrar em partes:\n\nEixos (Componentes Principais): O eixo horizontal é o CP1 e o vertical é o CP2. Eles são as “réguas” do nosso novo mapa e representam as direções de maior variabilidade nos dados. A porcentagem de variância que cada um explica é mostrada nos seus rótulos.\nPontos (Observações): Cada ponto no gráfico é uma observação.\n\nProximidade: Pontos próximos uns dos outros representam observações com perfis semelhantes (conforme capturado pelos dois primeiros CPs).\nAgrupamentos: Grupos de pontos (clusters) indicam subpopulações nos dados.\n\nVetores (Variáveis Originais): Cada seta (vetor) representa uma das variáveis originais.\n\nDireção: A direção da seta indica como a variável contribui para os dois componentes. Uma seta que aponta para a direita indica uma forte contribuição positiva para o CP1. Uma que aponta para cima, uma forte contribuição positiva para o CP2.\nComprimento: O comprimento da seta é proporcional a quão bem a variável é representada no espaço 2D do biplot. Setas mais longas significam que a variável tem uma forte influência nos componentes mostrados e é bem representada no gráfico. Setas curtas são menos importantes para os dois primeiros CPs ou sua variabilidade está melhor explicada em outros componentes (CP3, CP4, etc.).\nRelações entre Variáveis: O ângulo entre os vetores nos informa sobre a correlação entre as variáveis originais.\n\nÂngulo pequeno (&lt; 90°): As variáveis são positivamente correlacionadas.\nÂngulo de ~90°: As variáveis não são correlacionadas.\nÂngulo obtuso (&gt; 90°): As variáveis são negativamente correlacionadas.\n\n\nRelação entre Pontos e Vetores: Para entender o perfil de um ponto (ou grupo de pontos), projete-o ortogonalmente sobre os vetores das variáveis. Se a projeção de um ponto cai na direção de um vetor, aquela observação tem um valor alto para aquela variável. Se cai na direção oposta, tem um valor baixo.\n\nCom essas regras em mente, vamos analisar um biplot genérico.\n\n\n\n\n\n\n\n\nFigura 7.4: Exemplo de um biplot genérico para ilustrar a interpretação dos seus elementos. Os pontos representam as observações e as setas, as variáveis originais.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise de Componentes Principais</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html",
    "href": "src/02_tec_mult/07_af.html",
    "title": "8  Análise Fatorial",
    "section": "",
    "text": "8.1 O Modelo Fatorial Ortogonal\nA Análise Fatorial é uma técnica estatística utilizada para descrever a estrutura de covariância entre um conjunto de variáveis observadas. A hipótese central é que essa estrutura é gerada por um número menor de variáveis latentes não observáveis, denominadas fatores comuns.\nComeçamos com uma intuição. Suponha que temos as seguintes variáveis de gastos para diferentes famílias:\nÉ razoável supor que essas variáveis sejam correlacionadas. Mais do que isso, pode existir um fator latente, como a renda familiar (\\(F_1\\)), que influencia todos esses gastos. A Análise Fatorial busca formalizar e quantificar essa relação.\nO modelo supõe que cada variável observada é linearmente dependente de um conjunto de fatores comuns, somado a um termo de variância individual, ou específico.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#o-modelo-fatorial-ortogonal",
    "href": "src/02_tec_mult/07_af.html#o-modelo-fatorial-ortogonal",
    "title": "8  Análise Fatorial",
    "section": "",
    "text": "Definição 8.1 Seja \\(\\mathbf{x}\\) um vetor aleatório de p variáveis observadas com vetor de médias \\(\\boldsymbol{\\mu}\\) e matriz de covariâncias \\(\\mathbf{\\Sigma}\\). O modelo fatorial com m fatores comuns (\\(m &lt; p\\)) postula que \\(\\mathbf{x}\\) é linearmente dependente de m fatores comuns \\(F_1, F_2, \\dots, F_m\\) e p termos de erro \\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p\\). Em notação matricial, o modelo é:\n\\[\n\\mathbf{x}_{(p \\times 1)} - \\boldsymbol{\\mu}_{(p \\times 1)} = \\mathbf{L}_{(p \\times m)}\\mathbf{F}_{(m \\times 1)} + \\boldsymbol{\\epsilon}_{(p \\times 1)}\n\\tag{8.1}\\]\nOnde:\n\n\\(\\mathbf{L}\\) é a matriz de cargas fatoriais: Uma matriz de pesos responsável por quantificar as relações entre as p variáveis e os m fatores.\n\\(\\mathbf{F}\\) é o vetor de fatores comuns, ou seja \\(\\mathbf{F} = [F_1, F_2, \\dots, F_m]'\\).\n\\(\\boldsymbol{\\epsilon}\\) é o vetor de erros, ou variâncias específicas, ou seja, \\(\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p]'\\)\n\nPara que o ajuste desse modelo seja factível, as seguintes suposições são feitas para o modelo ortogonal:\n\n\\(E[\\mathbf{F}] = \\mathbf{0}\\) e \\(Cov(\\mathbf{F}) = E[\\mathbf{F}\\mathbf{F}'] = \\mathbf{I}\\).\n\\(E[\\boldsymbol{\\epsilon}] = \\mathbf{0}\\) e \\(Cov(\\boldsymbol{\\epsilon}) = E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] = \\mathbf{\\Psi}\\), onde \\(\\mathbf{\\Psi}\\) é uma matriz diagonal.\n\\(Cov(\\mathbf{F}, \\boldsymbol{\\epsilon}) = E[\\mathbf{F}\\boldsymbol{\\epsilon}'] = \\mathbf{0}\\).",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#a-estrutura-de-covariância-implícita",
    "href": "src/02_tec_mult/07_af.html#a-estrutura-de-covariância-implícita",
    "title": "8  Análise Fatorial",
    "section": "8.2 A Estrutura de Covariância Implícita",
    "text": "8.2 A Estrutura de Covariância Implícita\nAs suposições do modelo implicam uma estrutura específica para a matriz de covariâncias \\(\\mathbf{\\Sigma}\\).\n\nTeorema 8.1 Sob as premissas do modelo fatorial ortogonal (Definição 8.1), a matriz de covariâncias \\(\\mathbf{\\Sigma}\\) do vetor \\(\\mathbf{x}\\) é dada por:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\n\\tag{8.2}\\]\n\n\nComprovação. A partir do modelo fatorial em Equação 8.1, temos que \\(\\mathbf{x} - \\boldsymbol{\\mu} = \\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon}\\). A matriz de covariâncias de \\(\\mathbf{x}\\) é, por definição, \\(\\mathbf{\\Sigma} = E[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})']\\). Substituindo a expressão do modelo, obtemos:\n\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= E[(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})'] \\\\\n&= E[(\\mathbf{L}\\mathbf{F} + \\boldsymbol{\\epsilon})(\\mathbf{F}'\\mathbf{L}' + \\boldsymbol{\\epsilon}')] \\\\\n&= E[\\mathbf{L}\\mathbf{F}\\mathbf{F}'\\mathbf{L}' + \\mathbf{L}\\mathbf{F}\\boldsymbol{\\epsilon}' + \\boldsymbol{\\epsilon}\\mathbf{F}'\\mathbf{L}' + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] \\\\\n&= \\mathbf{L}E[\\mathbf{F}\\mathbf{F}']\\mathbf{L}' + \\mathbf{L}E[\\mathbf{F}\\boldsymbol{\\epsilon}'] + E[\\boldsymbol{\\epsilon}\\mathbf{F}']\\mathbf{L}' + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}']\n\\end{aligned}\n\\]\nPelas suposições do modelo ortogonal (Definição 8.1):\n\n\\(E[\\mathbf{F}\\mathbf{F}'] = \\text{Cov}(\\mathbf{F}) = \\mathbf{I}\\) (os fatores são não correlacionados e têm variância unitária).\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}'] = \\text{Cov}(\\boldsymbol{\\epsilon}) = \\mathbf{\\Psi}\\) (os erros são não correlacionados entre si).\n\\(E[\\mathbf{F}\\boldsymbol{\\epsilon}'] = \\text{Cov}(\\mathbf{F}, \\boldsymbol{\\epsilon}) = \\mathbf{0}\\) (os fatores e os erros são não correlacionados).\n\nSubstituindo essas esperanças na equação de \\(\\mathbf{\\Sigma}\\), temos:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{I}\\mathbf{L}' + \\mathbf{L}\\mathbf{0} + \\mathbf{0}\\mathbf{L}' + \\mathbf{\\Psi} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\n\\]\nIsso completa a prova.\n\nEsta equação decompõe a variância de cada variável \\(X_i\\) em:\n\nComunalidade (\\(h_i^2\\)): A porção da variância de \\(X_i\\) explicada pelos m fatores comuns (\\(h_i^2 = \\sum_{j=1}^{m} l_{ij}^2\\)).\nVariância Específica (\\(\\psi_i\\)): A porção da variância de \\(X_i\\) não explicada pelos fatores comuns (\\(Var(X_i) = \\sigma_{ii} = h_i^2 + \\psi_i\\)).\n\n\nExemplo 8.1 Suponha que a matriz de covariâncias de um vetor aleatório \\(\\mathbf{x}\\) com \\(p=4\\) variáveis seja:\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n19 & 30 & 2 & 12 \\\\\n30 & 57 & 5 & 23 \\\\\n2 & 5 & 37 & 47 \\\\\n12 & 23 & 47 & 68\n\\end{pmatrix}\n\\]\nÉ possível mostrar que um modelo fatorial com \\(m=2\\) fatores comuns pode gerar essa estrutura de covariância. Uma solução possível para \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) é dada por:\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n4 & 1 \\\\\n7 & 2 \\\\\n-1 & 6 \\\\\n1 & 8\n\\end{pmatrix}\n,\n\\quad\n\\mathbf{\\Psi} =\n\\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n0 & 4 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{pmatrix}\n\\]\nO leitor pode verificar que \\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\). O modelo decompõe a variância de cada variável.\n\nPara \\(X_1\\), a comunalidade é \\(h_1^2 = 4^2 + 1^2 = 17\\), e sua variância total é \\(Var(X_1) = \\sigma_{11} = h_1^2 + \\psi_1 = 17 + 2 = 19\\).\nPara \\(X_2\\), a comunalidade é \\(h_2^2 = 7^2 + 2^2 = 53\\), e sua variância total é \\(Var(X_2) = \\sigma_{22} = h_2^2 + \\psi_2 = 53 + 4 = 57\\).",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#problemas-no-modelo-fatorial",
    "href": "src/02_tec_mult/07_af.html#problemas-no-modelo-fatorial",
    "title": "8  Análise Fatorial",
    "section": "8.3 Problemas no Modelo Fatorial",
    "text": "8.3 Problemas no Modelo Fatorial\n\nExistência da Solução: Nem sempre existe uma solução factível para o modelo fatorial com m fatores. A estimação dos parâmetros, especialmente com um número inadequado de fatores, pode levar a soluções impróprias, como uma variância específica negativa (\\(\\hat{\\psi}_i &lt; 0\\)), conhecida como caso de Heywood. Isso viola a premissa de que \\(\\psi_i\\) é uma variância e, portanto, deve ser não-negativa. Geralmente, uma solução imprópria indica que o modelo é inadequado para os dados.\n\n\nExemplo 8.2 Considere um modelo de um fator (\\(m=1\\)) para \\(p=3\\) variáveis, com a seguinte matriz de correlação populacional:\n\\[\n\\mathbf{R} =\n\\begin{pmatrix}\n1.0 & 0.4 & 0.9 \\\\\n0.4 & 1.0 & 0.7 \\\\\n0.9 & 0.7 & 1.0\n\\end{pmatrix}\n\\]\nO modelo fatorial para a matriz de correlação é \\(\\mathbf{P} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\). Para \\(m=1\\), as cargas são um vetor \\(\\mathbf{L} = [l_{11}, l_{21}, l_{31}]'\\). As covariâncias (correlações) são dadas por \\(\\rho_{ij} = l_{i1}l_{j1}\\). Temos o sistema:\n\n\\(\\rho_{12} = l_{11}l_{21} = 0.4\\)\n\\(\\rho_{13} = l_{11}l_{31} = 0.9\\)\n\\(\\rho_{23} = l_{21}l_{31} = 0.7\\)\n\nMultiplicando as três equações, obtemos \\((l_{11}l_{21}l_{31})^2 = 0.4 \\times 0.9 \\times 0.7 = 0.252\\). Isso nos permite resolver para as cargas:\n\n\\(l_{11}^2 = (l_{11}l_{21})(l_{11}l_{31}) / (l_{21}l_{31}) = (0.4 \\times 0.9) / 0.7 \\approx 0.514\\)\n\\(l_{21}^2 = (l_{11}l_{21})(l_{21}l_{31}) / (l_{11}l_{31}) = (0.4 \\times 0.7) / 0.9 \\approx 0.311\\)\n\\(l_{31}^2 = (l_{11}l_{31})(l_{21}l_{31}) / (l_{11}l_{21}) = (0.9 \\times 0.7) / 0.4 = 1.575\\)\n\nA comunalidade da terceira variável é \\(h_3^2 = l_{31}^2 = 1.575\\). Como estamos modelando uma matriz de correlação, a variância total de cada variável é 1. A variância específica seria \\(\\psi_3 = 1 - h_3^2 = 1 - 1.575 = -0.575\\). Uma variância negativa é impossível, indicando que o modelo de um fator não é apropriado para descrever a estrutura de correlação dada.\n\n\nIndeterminação da Solução (Rotação Fatorial): A solução para a matriz de cargas \\(\\mathbf{L}\\) não é única. Para qualquer matriz ortogonal \\(\\mathbf{T}\\) de dimensão \\(m \\times m\\) (ou seja, uma matriz tal que \\(\\mathbf{T}\\mathbf{T}' = \\mathbf{T}'\\mathbf{T} = \\mathbf{I}\\)), podemos definir uma nova matriz de cargas \\(\\mathbf{L}^* = \\mathbf{L}\\mathbf{T}\\) que resulta na mesma matriz de covariâncias.\n\nIsso ocorre porque a parte da covariância explicada pelos fatores, \\(\\mathbf{L}\\mathbf{L}'\\), permanece inalterada:\n\\[\n\\mathbf{L}^*(\\mathbf{L}^*)' = (\\mathbf{L}\\mathbf{T})(\\mathbf{L}\\mathbf{T})' = \\mathbf{L}\\mathbf{T}\\mathbf{T}'\\mathbf{L}' = \\mathbf{L}(\\mathbf{T}\\mathbf{T}')\\mathbf{L}' = \\mathbf{L}\\mathbf{I}\\mathbf{L}' = \\mathbf{L}\\mathbf{L}'\n\\]\nPortanto, o modelo \\(\\mathbf{\\Sigma} = \\mathbf{L}^*(\\mathbf{L}^*)' + \\mathbf{\\Psi}\\) é equivalente ao modelo original. Essa propriedade é a base para a rotação fatorial, um procedimento que busca a solução \\(\\mathbf{L}^*\\) mais simples e interpretável, sem alterar o ajuste do modelo.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#adequabilidade-do-modelo-fatorial",
    "href": "src/02_tec_mult/07_af.html#adequabilidade-do-modelo-fatorial",
    "title": "8  Análise Fatorial",
    "section": "8.4 Adequabilidade do Modelo Fatorial",
    "text": "8.4 Adequabilidade do Modelo Fatorial\nAntes de aplicar os métodos de estimação, pode-se avaliar se os dados são adequados para a Análise Fatorial. A lema fundamental da AF é que as variáveis observadas são correlacionadas e que essa correlação pode ser explicada por fatores latentes. Se as variáveis são ortogonais ou se a correlação entre elas é espúria, o modelo fatorial não é apropriado.\nDois dos principais diagnósticos para verificar a adequabilidade dos dados são o Teste de Esfericidade de Bartlett e a medida de adequação da amostra de Kaiser-Meyer-Olkin (KMO).\n\n8.4.1 Teste de Esfericidade de Bartlett\nO Teste de Esfericidade de Bartlett avalia a hipótese nula (\\(H_0\\)) de que a matriz de correlação populacional \\(\\mathbf{P}\\) é uma matriz identidade (\\(H_0: \\mathbf{P} = \\mathbf{I}\\)). Se essa hipótese for verdadeira, as variáveis são não correlacionadas, e não há estrutura latente para ser extraída.\nA estatística de teste é baseada no determinante da matriz de correlação amostral \\(\\mathbf{R}\\) e, sob \\(H_0\\), segue aproximadamente uma distribuição Qui-quadrado. Para uma amostra de tamanho n e p variáveis, a estatística é:\n\\[\n\\chi^2 = -\\left[(n - 1) - \\frac{2p + 5}{6}\\right] \\ln(|\\mathbf{R}|)\n\\]\nEsta estatística tem, aproximadamente, uma distribuição \\(\\chi^2\\) com \\(p(p-1)/2\\) graus de liberdade. Um p-valor baixo (e.g., &lt; 0.05) leva à rejeição de \\(H_0\\), indicando que existe correlação suficiente entre as variáveis para justificar a aplicação da Análise Fatorial.\n\n\n8.4.2 Medida de Adequação da Amostra (KMO)\nEnquanto o teste de Bartlett avalia se a matriz de correlação como um todo se desvia significativamente da identidade, a medida de Kaiser-Meyer-Olkin (KMO) quantifica o quão adequados os dados são para a fatorização. O KMO compara a magnitude dos coeficientes de correlação observados com a magnitude dos coeficientes de correlação parcial.\nA lógica é que, se as variáveis compartilham fatores comuns, as correlações parciais entre pares de variáveis (controlando pelas outras variáveis) devem ser pequenas. A estatística KMO é calculada como:\n\\[\n\\text{KMO} = \\frac{\\sum_{i \\neq j} r_{ij}^2}{\\sum_{i \\neq j} r_{ij}^2 + \\sum_{i \\neq j} a_{ij}^2}\n\\]\nOnde \\(r_{ij}\\) é o coeficiente de correlação simples entre as variáveis \\(X_i\\) e \\(X_j\\), e \\(a_{ij}\\) é o coeficiente de correlação parcial.\nO valor do KMO varia de 0 a 1. Valores mais altos indicam que a Análise Fatorial é mais apropriada. Uma regra prática para a interpretação do KMO é:\n\n&gt; 0.9: Maravilhoso\n0.8 - 0.9: Meritório\n0.7 - 0.8: Razoável\n0.6 - 0.7: Medíocre\n0.5 - 0.6: Ruim\n&lt; 0.5: Inaceitável\n\nValores abaixo de 0.5 sugerem que a Análise Fatorial pode não ser uma boa ideia.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#métodos-de-estimação",
    "href": "src/02_tec_mult/07_af.html#métodos-de-estimação",
    "title": "8  Análise Fatorial",
    "section": "8.5 Métodos de Estimação",
    "text": "8.5 Métodos de Estimação\nAssumindo uma amostra aleatória \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\) de uma população com matriz de covariâncias \\(\\mathbf{\\Sigma}\\), o desafio é estimar \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) usando a matriz de covariâncias amostral \\(\\mathbf{S}\\) ou a matriz de correlação amostral \\(\\mathbf{R}\\).\nExistem diversos métodos para estimar os parâmetros do modelo fatorial, cada um com suas próprias premissas e propriedades. Alguns dos mais conhecidos incluem:\n\nMétodo de Componentes Principais (MCP)\nMétodo da Máxima Verossimilhança (MMV)\nMétodo dos Fatores Principais (Principal Axis Factoring)\nMínimos Quadrados Ponderados\nMínimos Quadrados Generalizados\n\nNeste capítulo, focaremos nos dois métodos mais amplamente utilizados na prática: o Método de Componentes Principais, por sua simplicidade computacional, e o Método da Máxima Verossimilhança, por sua fundamentação estatística robusta.\n\n8.5.1 A Solução por Componentes Principais\nO método de componentes principais (MCP) provê uma solução para \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) a partir da decomposição espectral da matriz de covariâncias amostral \\(\\mathbf{S}\\) (ou da matriz de correlações \\(\\mathbf{R}\\)).\nA ideia é que a matriz \\(\\mathbf{S}\\) pode ser decomposta em termos de seus pares de autovalor-autovetor \\((\\hat{\\lambda}_i, \\hat{\\mathbf{e}}_i)\\):\n\\[\n\\mathbf{S} = \\hat{\\lambda}_1\\hat{\\mathbf{e}}_1\\hat{\\mathbf{e}}_1' + \\hat{\\lambda}_2\\hat{\\mathbf{e}}_2\\hat{\\mathbf{e}}_2' + \\dots + \\hat{\\lambda}_p\\hat{\\mathbf{e}}_p\\hat{\\mathbf{e}}_p'\n\\]\nA estrutura do modelo fatorial é \\(\\mathbf{S} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\). O MCP busca uma aproximação para \\(\\mathbf{S}\\) retendo apenas os m primeiros componentes, que explicam a maior parte da variabilidade total. A matriz \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\) é construída para igualar a contribuição desses componentes:\n\\[\n\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' = \\hat{\\lambda}_1\\hat{\\mathbf{e}}_1\\hat{\\mathbf{e}}_1' + \\hat{\\lambda}_2\\hat{\\mathbf{e}}_2\\hat{\\mathbf{e}}_2' + \\dots + \\hat{\\lambda}_m\\hat{\\mathbf{e}}_m\\hat{\\mathbf{e}}_m'\n\\]\nUma solução explícita para \\(\\hat{\\mathbf{L}}\\) que satisfaz essa equação é uma matriz \\(p \\times m\\) cujas colunas são os autovetores reescalados pelos respectivos autovalores. A matriz \\(\\hat{\\mathbf{\\Psi}}\\) é então definida para garantir que as variâncias do modelo (\\(\\text{diag}(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}})\\)) sejam iguais às variâncias amostrais (\\(\\text{diag}(\\mathbf{S})\\)).\nIsso nos leva à seguinte definição formal.\n\nDefinição 8.2 Seja \\(\\mathbf{S}\\) a matriz de covariância amostral com pares de autovalor-autovetor \\((\\hat{\\lambda}_i, \\hat{\\mathbf{e}}_i)\\). A solução de componentes principais com m fatores é definida por:\n\nMatriz de Cargas Estimada (\\(\\hat{\\mathbf{L}}\\)): \\[\n\\hat{\\mathbf{L}} = [\\sqrt{\\hat{\\lambda}_1}\\hat{\\mathbf{e}}_1 | \\sqrt{\\hat{\\lambda}_2}\\hat{\\mathbf{e}}_2 | \\dots | \\sqrt{\\hat{\\lambda}_m}\\hat{\\mathbf{e}}_m]\n\\]\nMatriz de Variâncias Específicas Estimada (\\(\\hat{\\mathbf{\\Psi}}\\)): \\[\n\\hat{\\mathbf{\\Psi}} = \\text{diag}(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}')\n\\]\n\nonde \\(\\hat{\\psi}_i = s_{ii} - \\sum_{j=1}^{m} \\hat{l}_{ij}^2\\).\nSe a matriz de correlações \\(\\mathbf{R}\\) for utilizada, as cargas \\(\\hat{\\mathbf{L}}\\) são calculadas a partir dos autovalores e autovetores de \\(\\mathbf{R}\\), e as variâncias específicas são \\(\\hat{\\psi}_i = 1 - \\sum_{j=1}^{m} \\hat{l}_{ij}^2\\).\n\nPor construção, este método força a diagonal da matriz de covariâncias do modelo, \\(\\hat{\\mathbf{\\Sigma}} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\), a ser idêntica à diagonal de \\(\\mathbf{S}\\). O ajuste do modelo é então avaliado pela magnitude dos resíduos fora da diagonal. A matriz de resíduos é:\n\\[\n\\mathbf{S} - \\hat{\\mathbf{\\Sigma}} = \\mathbf{S} - (\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}})\n\\]\nComo \\(\\hat{\\mathbf{\\Psi}} = \\text{diag}(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}')\\), os elementos da diagonal da matriz de resíduos são zero. Os resíduos fora da diagonal são os elementos de \\(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\). Pode-se demonstrar que a soma dos quadrados de todos os elementos da matriz \\(\\mathbf{S} - \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}'\\) (incluindo a diagonal) é:\n\\[\n\\sum_{i=1}^p \\sum_{j=1}^p (s_{ij} - \\sum_{k=1}^m \\hat{l}_{ik}\\hat{l}_{jk})^2 = \\sum_{k=m+1}^p \\hat{\\lambda}_k^2\n\\]\nIsso mostra que, para que o ajuste seja bom, a soma dos autovalores descartados (\\(\\hat{\\lambda}_{m+1}, \\dots, \\hat{\\lambda}_p\\)) deve ser pequena.\n\n\n8.5.2 Método da Máxima Verossimilhança (MMV)\nO método da máxima verossimilhança (MMV) é uma abordagem mais rigorosa para a estimação, baseada em suposições sobre a distribuição dos dados.\nSuposições Adicionais:\n\nO vetor de fatores comuns \\(\\mathbf{F}\\) e o vetor de erros \\(\\boldsymbol{\\epsilon}\\) seguem uma distribuição normal multivariada:\n\n\\(\\mathbf{F} \\sim N_m(\\mathbf{0}, \\mathbf{I})\\)\n\\(\\boldsymbol{\\epsilon} \\sim N_p(\\mathbf{0}, \\mathbf{\\Psi})\\)\n\n\\(\\mathbf{F}\\) e \\(\\boldsymbol{\\epsilon}\\) são independentes.\n\nSob essas condições, o vetor de variáveis observáveis \\(\\mathbf{x}\\) segue uma distribuição normal multivariada \\(N_p(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})\\), onde \\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}' + \\mathbf{\\Psi}\\).\nDada uma amostra aleatória \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\), a função de log-verossimilhança (ignorando constantes) para os parâmetros \\(\\mathbf{L}\\) e \\(\\mathbf{\\Psi}\\) é:\n\\[\n\\log L(\\mathbf{L}, \\mathbf{\\Psi}) = -\\frac{n}{2} \\ln |\\mathbf{\\Sigma}| - \\frac{n}{2} \\text{tr}(\\mathbf{\\Sigma}^{-1}\\mathbf{S})\n\\]\nonde \\(\\mathbf{S}\\) é a matriz de covariâncias amostral (versão ML, com divisor n). O objetivo é encontrar as estimativas \\(\\hat{\\mathbf{L}}\\) e \\(\\hat{\\mathbf{\\Psi}}\\) que maximizam essa função, sujeito à restrição de que \\(\\hat{\\mathbf{L}}'\\hat{\\mathbf{\\Psi}}^{-1}\\hat{\\mathbf{L}}\\) seja uma matriz diagonal para garantir a unicidade da solução.\nA maximização é realizada por meio de algoritmos numéricos (como o de Newton-Raphson), pois não há uma solução analítica fechada. As estimativas resultantes, \\(\\hat{\\mathbf{L}}\\) e \\(\\hat{\\mathbf{\\Psi}}\\), satisfazem um conjunto complexo de equações.\nA principal vantagem do MMV é que ele permite um teste de hipóteses para a adequação do número de fatores m, comparando a matriz de covariâncias do modelo, \\(\\hat{\\mathbf{\\Sigma}} = \\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\), com a matriz amostral \\(\\mathbf{S}\\). Isso é fundamental na Análise Fatorial Confirmatória (AFC)",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#a-escolha-do-número-de-fatores-m",
    "href": "src/02_tec_mult/07_af.html#a-escolha-do-número-de-fatores-m",
    "title": "8  Análise Fatorial",
    "section": "8.6 A Escolha do Número de Fatores (m)",
    "text": "8.6 A Escolha do Número de Fatores (m)\nA determinação do número de fatores, m, é uma das decisões mais importantes na Análise Fatorial. Um número muito baixo de fatores pode não capturar a estrutura de covariância subjacente, enquanto um número muito alto pode levar a um modelo superajustado e de difícil interpretação, violando o princípio da parcimônia.\nA escolha de m geralmente envolve uma combinação de critérios estatísticos e julgamento prático. Vários dos métodos utilizados são análogos aos empregados na Análise de Componentes Principais (Capítulo 7). Os mais comuns são:\n\nProporção da Variância Total Explicada: Um critério comum é reter fatores suficientes para explicar uma proporção substancial (e.g., 70-90%) da variância total. No contexto do método de componentes principais para AF, a proporção da variância explicada pelo fator j é \\(\\hat{\\lambda}_j / \\text{tr}(\\mathbf{S})\\).\nCritério de Kaiser (Autovalores &gt; 1): Ao trabalhar com a matriz de correlação \\(\\mathbf{R}\\), o critério de Kaiser sugere reter apenas os fatores correspondentes a autovalores maiores que 1. A lógica é que um fator deve explicar pelo menos a variância de uma variável original.\nGráfico de cotovelo (Scree Plot): Este é um gráfico dos autovalores ordenados (\\(\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots\\)). Procura-se por um “cotovelo” no gráfico, um ponto onde a magnitude dos autovalores começa a diminuir drasticamente. O número de fatores a reter seria o número de pontos antes do início do platô.\nTeste de Hipóteses (para MMV): Quando o método da máxima verossimilhança é utilizado, é possível realizar um teste de razão de verossimilhanças para testar a hipótese nula de que m fatores são suficientes para descrever a estrutura de covariância.\n\nNa prática, é recomendável utilizar uma combinação desses critérios. A interpretabilidade da solução fatorial resultante é, em última análise, o guia mais importante.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/07_af.html#rotação-fatorial",
    "href": "src/02_tec_mult/07_af.html#rotação-fatorial",
    "title": "8  Análise Fatorial",
    "section": "8.7 Rotação Fatorial",
    "text": "8.7 Rotação Fatorial\nComo visto anteriormente, a solução para a matriz de cargas fatoriais \\(\\hat{\\mathbf{L}}\\) não é única. Qualquer rotação ortogonal dos fatores resulta em uma nova matriz de cargas \\(\\hat{\\mathbf{L}}^* = \\hat{\\mathbf{L}}\\mathbf{T}\\) que explica a estrutura de covariâncias dos dados exatamente da mesma forma, pois \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' = \\hat{\\mathbf{L}}^* (\\hat{\\mathbf{L}}^*)'\\).\nEssa indeterminação, que a princípio parece um problema, é na verdade uma das ferramentas mais poderosas da Análise Fatorial. Ela nos permite girar a estrutura fatorial para uma posição que seja mais simples e interpretável, sem sacrificar o ajuste do modelo. O objetivo é alcançar o que o psicólogo Louis Thurstone chamou de estrutura simples.\nA estrutura simples ideal teria as seguintes propriedades:\n\nCada variável deve ter pelo menos uma carga fatorial próxima de zero.\nCada fator deve ter várias cargas próximas de zero e algumas cargas altas.\nPara cada par de fatores, deve haver variáveis com cargas altas em um fator, mas não no outro.\n\nEm suma, busca-se uma matriz de cargas onde cada variável esteja fortemente associada a apenas um ou poucos fatores, e cada fator represente claramente um subconjunto de variáveis. Os métodos de rotação são algoritmos que buscam, de forma objetiva, uma matriz \\(\\mathbf{T}\\) que aproxime a matriz de cargas rotacionada \\(\\hat{\\mathbf{L}}^*\\) a essa estrutura ideal.\nAs rotações dividem-se em duas categorias principais.\n\n8.7.1 Rotações Ortogonais\nNeste tipo de rotação, a matriz de transformação \\(\\mathbf{T}\\) é ortogonal, o que significa que os eixos dos fatores são girados, mas mantidos em um ângulo de 90 graus entre si. A consequência fundamental é que os fatores rotacionados permanecem não correlacionados.\nOs métodos mais comuns de rotação ortogonal são:\n\nVarimax: É o método de rotação ortogonal mais popular. O objetivo do Varimax é simplificar as colunas da matriz de cargas fatoriais. Para cada fator, ele busca maximizar a variância das cargas ao quadrado, efetivamente empurrando as cargas para perto de 0 ou \\(\\pm 1\\). Isso facilita a identificação de quais variáveis estão associadas a cada fator. O critério Varimax maximiza a seguinte função:\n\\[\nV = \\sum_{j=1}^{m} \\left[ \\frac{1}{p} \\sum_{i=1}^{p} (\\hat{l}_{ij}^* / h_i)^4 - \\left( \\frac{1}{p} \\sum_{i=1}^{p} (\\hat{l}_{ij}^* / h_i)^2 \\right)^2 \\right]\n\\]\nOnde \\(\\hat{l}_{ij}^*\\) são as cargas rotacionadas e \\(h_i^2\\) são as comunalidades (que permanecem invariantes sob rotação).\nQuartimax: Este método foca em simplificar as linhas da matriz de cargas. Ele tenta fazer com que cada variável tenha carga alta em apenas um fator. O Quartimax foi o primeiro método analítico proposto, mas tende a criar um fator geral com cargas altas para muitas variáveis, o que pode dificultar a interpretação.\nEquimax: É um meio termo entre o Varimax e o Quartimax. Ele tenta simplificar tanto as linhas quanto as colunas da matriz de cargas simultaneamente.\n\n\n\n8.7.2 Rotações Oblíquas\nEm muitos campos, especialmente nas ciências sociais, é teoricamente razoável esperar que os fatores latentes sejam correlacionados. Por exemplo, os fatores “habilidade verbal” e “habilidade matemática” são distintos, mas é provável que sejam positivamente correlacionados.\nAs rotações oblíquas permitem que os fatores se tornem correlacionados. A matriz de transformação \\(\\mathbf{T}\\) não é mais ortogonal, e os eixos dos fatores podem ter ângulos diferentes de 90 graus. A vantagem é a capacidade de encontrar uma estrutura mais simples e teoricamente mais realista, ao custo de uma complexidade maior na interpretação, pois é preciso analisar tanto a matriz de cargas quanto a matriz de correlação entre os fatores.\nOs métodos mais comuns incluem:\n\nPromax: É um método muito utilizado que funciona em duas etapas. Primeiro, ele realiza uma rotação ortogonal (geralmente Varimax). Em seguida, ele relaxa a restrição de ortogonalidade, permitindo que os fatores se correlacionem para buscar uma estrutura ainda mais simples (com mais cargas próximas de zero).\nOblimin Direto: É um método mais geral que busca minimizar a covariância das cargas ao quadrado para pares de fatores. Ele possui um parâmetro (delta) que controla o grau de correlação permitido entre os fatores.\n\nA escolha entre uma rotação ortogonal e oblíqua depende de considerações teóricas. Se não há uma razão forte para acreditar que os fatores são correlacionados, a rotação ortogonal (como a Varimax) é geralmente preferida por sua simplicidade. Se a correlação entre os fatores é esperada, uma rotação oblíqua pode fornecer uma representação mais fiel da realidade.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Análise Fatorial</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html",
    "href": "src/02_tec_mult/08_cluster.html",
    "title": "9  Análise de Agrupamentos",
    "section": "",
    "text": "9.1 Decomposição da Variabilidade\nA Análise de Agrupamentos, ou Análise de Clusters, é uma técnica exploratória multivariada cujo objetivo é particionar um conjunto de observações em subgrupos (os clusters). A partição é feita de tal forma que as observações dentro de um mesmo grupo sejam semelhantes entre si, enquanto observações em grupos diferentes sejam o mais distintas possível.\nDiferentemente de outras técnicas como a análise de regressão ou a análise discriminante, a análise de agrupamentos é um método de aprendizagem não supervisionada. Isso significa que não temos uma variável resposta ou rótulos pré-definidos para os grupos; o objetivo é descobrir a estrutura de agrupamentos inerente aos próprios dados.\nO princípio fundamental é a maximização da homogeneidade intra-grupo e, ao mesmo tempo, a maximização da heterogeneidade entre grupos.\nDiversas técnicas apresentadas nesse capítulo dependem da definição de uma medida para quantificar o quão semelhantes ou diferentes as observações são. Essa medida é formalizada como uma medida de dissimilaridade ou distância. Uma discussão detalhada sobre as diferentes métricas de distância pode ser encontrada na Capítulo 6.\nPodemos formalizar o critério de “boa separação” dos grupos através de uma decomposição da variabilidade total dos dados, análoga à Análise de Variância (ANOVA).\nO objetivo da análise de agrupamentos pode ser visto como encontrar a partição que minimiza a SQI (grupos coesos) e maximiza a SQE (grupos separados).\nA prova deste teorema mostra que a variabilidade total é conservada e apenas particionada, de forma que minimizar a SQI é equivalente a maximizar a SQE.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#decomposição-da-variabilidade",
    "href": "src/02_tec_mult/08_cluster.html#decomposição-da-variabilidade",
    "title": "9  Análise de Agrupamentos",
    "section": "",
    "text": "Definição 9.2 Dado um conjunto de \\(n\\) observações e uma partição em \\(K\\) clusters \\(C_1, \\dots, C_K\\):\n\nSoma de Quadrados Total (SQT): Mede a dispersão total dos dados em torno da média geral \\(\\bar{\\mathbf{x}}\\). \\[\nSQT = \\sum_{i=1}^{n} (\\mathbf{x}_i - \\bar{\\mathbf{x}})'(\\mathbf{x}_i - \\bar{\\mathbf{x}})\n\\]\nSoma de Quadrados Intra-grupos (SQI): Mede a dispersão dentro dos clusters. É a soma das dispersões de cada observação em relação ao centroide do seu próprio cluster, \\(\\bar{\\mathbf{x}}_k\\). Também é conhecida como Within-Cluster Sum of Squares (WCSS). \\[\nSQI = \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)\n\\]\nSoma de Quadrados Entre-grupos (SQE): Mede a dispersão entre os centroides dos clusters em relação à média geral. \\[\nSQE = \\sum_{k=1}^{K} n_k (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\] Onde \\(n_k\\) é o número de observações no cluster \\(C_k\\).\n\n\n\n\nTeorema 9.1 A soma de quadrados total pode ser decomposta como a soma da variabilidade dentro dos grupos e entre os grupos.\n\\[\nSQT = SQI + SQE\n\\]\n\nComprovação. A prova parte da decomposição do desvio de uma observação \\(\\mathbf{x}_i \\in C_k\\) em relação à média geral \\(\\bar{\\mathbf{x}}\\):\n\\[\n(\\mathbf{x}_i - \\bar{\\mathbf{x}}) = (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\]\nElevando ao quadrado (no sentido de produto vetorial), temos:\n\\[\n(\\mathbf{x}_i - \\bar{\\mathbf{x}})'(\\mathbf{x}_i - \\bar{\\mathbf{x}}) = [(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})]'[(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})]\n\\]\n\\[\n= (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}}) + 2(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\]\nAgora, somamos sobre todas as observações \\(i=1, \\dots, n\\). Para fazer isso, somamos primeiro dentro de cada cluster \\(k\\) e depois somamos os resultados sobre todos os clusters:\n\\[\nSQT = \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}})'(\\mathbf{x}_i - \\bar{\\mathbf{x}})\n\\]\n\\[\n= \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) + \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}}) + \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} 2(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\n\\]\nAnalisando cada termo:\n\nO primeiro termo é, por definição, a Soma de Quadrados Intra-grupos (SQI).\nNo segundo termo, a expressão \\((\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\\) é constante para todas as \\(n_k\\) observações no cluster \\(C_k\\). Portanto, a soma interna resulta em \\(n_k (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})'(\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}})\\). Somar sobre \\(k\\) nos dá a Soma de Quadrados Entre-grupos (SQE).\nPara o terceiro termo (o termo cruzado), podemos reescrevê-lo como: \\[\n2 \\sum_{k=1}^{K} \\left[ \\left( \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) \\right)' (\\bar{\\mathbf{x}}_k - \\bar{\\mathbf{x}}) \\right]\n\\] Pela definição do centroide \\(\\bar{\\mathbf{x}}_k\\), a soma dos desvios em torno dele dentro de um cluster é zero: \\(\\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k) = \\mathbf{0}\\). Portanto, todo o terceiro termo é igual a zero.\n\nJuntando os resultados, obtemos \\(SQT = SQI + SQE\\).",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#agrupamentos-hierárquicos",
    "href": "src/02_tec_mult/08_cluster.html#agrupamentos-hierárquicos",
    "title": "9  Análise de Agrupamentos",
    "section": "9.2 Agrupamentos Hierárquicos",
    "text": "9.2 Agrupamentos Hierárquicos\nOs métodos de agrupamento hierárquico criam uma sequência de partições aninhadas, que pode ser representada visualmente por uma árvore chamada dendrograma. Existem duas abordagens principais:\n\nAglomerativa (Bottom-Up): Começa com cada observação em seu próprio cluster e, a cada passo, funde os dois clusters mais próximos até que reste apenas um único cluster contendo todas as observações.\nDivisiva (Top-Down): Começa com todas as observações em um único cluster e, a cada passo, divide um cluster em dois até que cada observação esteja em seu próprio cluster.\n\nA abordagem aglomerativa é a mais comum. O ponto crucial do algoritmo é definir a “proximidade” entre dois clusters.\n\n9.2.1 Métodos de Ligação (Linkage)\nO método de ligação é a regra que define a distância entre dois clusters.\n\nLigação Simples (Single Linkage): A distância entre dois clusters é a distância mínima entre quaisquer dois pontos dos clusters. Tende a produzir clusters “alongados” e é sensível a ruído. \\[ d(A, B) = \\min_{\\mathbf{a} \\in A, \\mathbf{b} \\in B} d(\\mathbf{a}, \\mathbf{b}) \\]\nLigação Completa (Complete Linkage): A distância é o máximo da distância entre quaisquer dois pontos. Produz clusters mais compactos e esféricos. \\[ d(A, B) = \\max_{\\mathbf{a} \\in A, \\mathbf{b} \\in B} d(\\mathbf{a}, \\mathbf{b}) \\]\nLigação Média (Average Linkage): A distância é a média de todas as distâncias entre os pares de pontos dos dois clusters. É um meio-termo entre a simples e a completa. \\[ d(A, B) = \\frac{1}{n_A n_B} \\sum_{\\mathbf{a} \\in A} \\sum_{\\mathbf{b} \\in B} d(\\mathbf{a}, \\mathbf{b}) \\]\nMétodo de Ward: Este método se baseia em um critério de minimização da variância. A cada passo do algoritmo aglomerativo, ele funde o par de clusters que leva ao menor aumento possível na Soma de Quadrados Intra-grupos (SQI). O objetivo é encontrar, a cada passo, a fusão mais “econômica” em termos de perda de coesão interna.\nVamos formalizar esse aumento. Suponha que estejamos considerando fundir dois clusters, \\(C_i\\) e \\(C_j\\). O aumento na SQI, que denotamos por \\(\\Delta(C_i, C_j)\\), é a diferença entre a SQI do novo cluster fundido (\\(C_{ij}\\)) e a soma das SQIs dos clusters individuais antes da fusão.\n\\[\n\\Delta(C_i, C_j) = \\text{SQI}(C_{ij}) - (\\text{SQI}(C_i) + \\text{SQI}(C_j))\n\\]\nPode-se demonstrar que esse aumento é diretamente proporcional à distância euclidiana quadrada entre os centroides dos clusters que estão sendo fundidos.\n\nComprovação. A SQI para o novo cluster \\(C_{ij} = C_i \\cup C_j\\) é \\(\\sum_{\\mathbf{x} \\in C_{ij}} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})\\). Podemos reescrever essa soma como: \\[\n\\sum_{\\mathbf{x} \\in C_i} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij}) + \\sum_{\\mathbf{x} \\in C_j} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})\n\\] Usando a mesma lógica da decomposição da variância, podemos mostrar que \\(\\sum_{\\mathbf{x} \\in C_i} (\\mathbf{x} - \\bar{\\mathbf{x}}_{ij})'(\\mathbf{x} - \\bar{\\mathbf{x}}_{ij}) = \\text{SQI}(C_i) + n_i (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})\\).\nAplicando o resultado para ambos os termos, a SQI do novo cluster é: \\[\n\\text{SQI}(C_{ij}) = \\text{SQI}(C_i) + \\text{SQI}(C_j) + n_i (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij}) + n_j (\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})\n\\] Portanto, o aumento na SQI é: \\[\n\\Delta(C_i, C_j) = n_i (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_{ij}) + n_j (\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})'(\\bar{\\mathbf{x}}_j - \\bar{\\mathbf{x}}_{ij})\n\\] Substituindo \\(\\bar{\\mathbf{x}}_{ij} = \\frac{n_i\\bar{\\mathbf{x}}_i + n_j\\bar{\\mathbf{x}}_j}{n_i+n_j}\\) e simplificando a álgebra, chegamos a: \\[\n\\Delta(C_i, C_j) = \\frac{n_i n_j}{n_i + n_j} (\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_j)'(\\bar{\\mathbf{x}}_i - \\bar{\\mathbf{x}}_j)\n\\]\n\nA fórmula final nos dá uma maneira eficiente de calcular o critério de Ward. A cada passo, o algoritmo calcula \\(\\Delta(C_i, C_j)\\) para todos os pares de clusters e realiza a fusão para o par que tiver o menor valor. Como a fórmula depende da distância entre centroides, o método de Ward tende a produzir clusters de tamanho semelhante e formato esférico.\n\n\n\n9.2.2 O dendograma\nTBD",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/02_tec_mult/08_cluster.html#agrupamento-não-hierárquico-k-médias-k-means",
    "href": "src/02_tec_mult/08_cluster.html#agrupamento-não-hierárquico-k-médias-k-means",
    "title": "9  Análise de Agrupamentos",
    "section": "9.3 Agrupamento Não-Hierárquico: K-Médias (K-Means)",
    "text": "9.3 Agrupamento Não-Hierárquico: K-Médias (K-Means)\nDiferente dos métodos hierárquicos, os métodos particionais, como o K-Médias, dividem os dados em um número \\(K\\) de clusters pré-especificado. O K-Médias é um dos algoritmos de agrupamento mais populares e eficientes.\nO objetivo do K-Médias é particionar as \\(n\\) observações em \\(K\\) clusters de modo a minimizar a Soma de Quadrados Intra-grupos (SQI), também chamada de inércia ou Within-Cluster Sum of Squares (WCSS).\n\\[\n\\text{SQI} = \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)'(\\mathbf{x}_i - \\bar{\\mathbf{x}}_k)\n\\]\nOnde \\(\\bar{\\mathbf{x}}_k\\) é o centroide (média) do cluster \\(C_k\\).\n\n9.3.1 O Algoritmo K-Médias\nO algoritmo é iterativo e funciona da seguinte forma:\n\nInicialização: Escolha \\(K\\) centroides iniciais. Isso pode ser feito selecionando \\(K\\) observações aleatórias dos dados.\nAtribuição: Atribua cada observação ao cluster cujo centroide é o mais próximo (usando, por exemplo, a distância Euclidiana).\nAtualização: Recalcule o centroide de cada um dos \\(K\\) clusters como a média de todas as observações atribuídas a ele.\nRepetição: Repita os passos 2 e 3 até que as atribuições dos clusters não mudem mais ou um número máximo de iterações seja atingido.\n\nO algoritmo tem a garantia de convergir, mas pode chegar a um mínimo local, não necessariamente o mínimo global da WCSS. Por isso, é comum executar o algoritmo várias vezes com diferentes inicializações e escolher o resultado com a menor WCSS.\n\n\n9.3.2 Escolhendo o Número de Clusters K\nA escolha de \\(K\\) é um passo crítico. Dois métodos comuns são:\n\nMétodo do Cotovelo (Elbow Method): Plota-se a WCSS para diferentes valores de \\(K\\). O gráfico tipicamente se parece com um braço. O “cotovelo” nesse gráfico, o ponto onde a taxa de diminuição da WCSS se torna abruptamente menor, é um bom candidato para o número ideal de clusters.\nAnálise de Silhueta: O coeficiente de silhueta de uma observação mede o quão bem ela se encaixa em seu cluster em comparação com os clusters vizinhos. Ele varia de -1 a 1, onde valores altos indicam que a observação está bem agrupada. Pode-se calcular a média do coeficiente de silhueta para todos os pontos para um dado \\(K\\), e o \\(K\\) que maximiza essa média é considerado o ideal.",
    "crumbs": [
      "Parte II: Métodos multivariados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Análise de Agrupamentos</span>"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html",
    "href": "src/exemplos/acp_manual.html",
    "title": "Exemplo manual: ACP",
    "section": "",
    "text": "O Cenário\nNeste exemplo, vamos detalhar passo a passo a aplicação da Análise de Componentes Principais (ACP) em um pequeno conjunto de dados. O objetivo é demonstrar manualmente todos os cálculos, desde a preparação dos dados até a interpretação dos resultados, seguindo a metodologia apresentada no Capítulo 3.\nVamos expandir o exemplo da intuição geométrica, que usava Peso e Altura. Adicionaremos uma terceira variável, Renda (em milhares de R$), para um grupo de 5 indivíduos. A ideia é que Peso e Altura sejam correlacionados, mas a Renda não tenha uma correlação forte com eles.\nNosso conjunto de dados inicial é:",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#o-cenário",
    "href": "src/exemplos/acp_manual.html#o-cenário",
    "title": "Exemplo manual: ACP",
    "section": "",
    "text": "Indivíduo\nPeso (kg)\nAltura (cm)\nRenda (R$ 1000)\n\n\n\n\n1\n65\n170\n5.5\n\n\n2\n72\n182\n4.0\n\n\n3\n58\n165\n7.0\n\n\n4\n81\n190\n3.5\n\n\n5\n75\n178\n5.0",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-1-preparação-dos-dados",
    "href": "src/exemplos/acp_manual.html#passo-1-preparação-dos-dados",
    "title": "Exemplo manual: ACP",
    "section": "Passo 1: Preparação dos Dados",
    "text": "Passo 1: Preparação dos Dados\nConforme discutido no Capítulo 3, a ACP é sensível à escala das variáveis. Portanto, o primeiro passo é padronizar os dados. Isso envolve duas etapas: centralizar (subtrair a média) e escalonar (dividir pelo desvio padrão).\n\n1.1. Calcular a Média e o Desvio Padrão\nPrimeiro, calculamos a média e o desvio padrão para cada variável.\n\\[\n\\bar{x}_{peso} = \\frac{65+72+58+81+75}{5} = 70.2 \\, \\text{kg}\n\\] \\[\n\\bar{x}_{altura} = \\frac{170+182+165+190+178}{5} = 177.0 \\, \\text{cm}\n\\] \\[\n\\bar{x}_{renda} = \\frac{5.5+4.0+7.0+3.5+5.0}{5} = 5.0 \\, (RS 1000)\n\\]\nAgora, os desvios padrão (usando a fórmula com denominador \\(n-1\\)):\n\\[\ns_{peso} = \\sqrt{\\frac{(65-70.2)^2 + ... + (75-70.2)^2}{4}} = 8.64 \\, \\text{kg}\n\\] \\[\ns_{altura} = \\sqrt{\\frac{(170-177)^2 + ... + (178-177)^2}{4}} = 9.67 \\, \\text{cm}\n\\] \\[\ns_{renda} = \\sqrt{\\frac{(5.5-5.0)^2 + ... + (5.0-5.0)^2}{4}} = 1.35 \\, (RS 1000)\n\\]\n\n\n1.2. Padronizar os Dados\nCom as médias e desvios padrão, podemos padronizar cada observação \\(x_{ij}\\) usando a fórmula \\(z_{ij} = (x_{ij} - \\bar{x}_j) / s_j\\).\nPor exemplo, para o Indivíduo 1: \\[\nz_{1, peso} = \\frac{65 - 70.2}{8.64} = -0.60\n\\] \\[\nz_{1, altura} = \\frac{170 - 177}{9.67} = -0.72\n\\] \\[\nz_{1, renda} = \\frac{5.5 - 5.0}{1.35} = 0.37\n\\]\nAplicando isso a todos os dados, obtemos a matriz de dados padronizados \\(\\mathbf{Z}\\):\n\\[\n\\mathbf{Z} = \\begin{pmatrix}\n-0.60 & -0.72 & 0.37 \\\\\n0.21 & 0.52 & -0.74 \\\\\n-1.41 & -1.24 & 1.48 \\\\\n1.25 & 1.34 & -1.11 \\\\\n0.56 & 0.10 & 0.00\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-2-calcular-a-matriz-de-correlação",
    "href": "src/exemplos/acp_manual.html#passo-2-calcular-a-matriz-de-correlação",
    "title": "Exemplo manual: ACP",
    "section": "Passo 2: Calcular a Matriz de Correlação",
    "text": "Passo 2: Calcular a Matriz de Correlação\nComo estamos trabalhando com dados padronizados, a ACP será realizada sobre a matriz de correlação \\(\\mathbf{R}\\). A matriz de correlação pode ser calculada como:\n\\[\n\\mathbf{R} = \\frac{1}{n-1} \\mathbf{Z}' \\mathbf{Z}\n\\]\nCalculando \\(\\mathbf{Z}' \\mathbf{Z}\\): \\[\n\\mathbf{Z}' \\mathbf{Z} = \\begin{pmatrix}\n4.00 & 3.85 & -0.81 \\\\\n3.85 & 4.00 & -1.18 \\\\\n-0.81 & -1.18 & 4.00\n\\end{pmatrix}\n\\]\nDividindo por \\(n-1 = 4\\), obtemos a matriz de correlação \\(\\mathbf{R}\\):\n\\[\n\\mathbf{R} = \\begin{pmatrix}\n1.00 & 0.96 & -0.20 \\\\\n0.96 & 1.00 & -0.29 \\\\\n-0.20 & -0.29 & 1.00\n\\end{pmatrix}\n\\]\nComo esperado, a correlação entre Peso e Altura (0.96) é muito alta, enquanto a Renda tem uma correlação fraca e negativa com as outras duas variáveis.",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-3-decomposição-espectral-da-matriz-de-correlação",
    "href": "src/exemplos/acp_manual.html#passo-3-decomposição-espectral-da-matriz-de-correlação",
    "title": "Exemplo manual: ACP",
    "section": "Passo 3: Decomposição Espectral da Matriz de Correlação",
    "text": "Passo 3: Decomposição Espectral da Matriz de Correlação\nO próximo passo é encontrar os autovalores (\\(\\lambda\\)) e autovetores (\\(\\mathbf{e}\\)) da matriz de correlação \\(\\mathbf{R}\\). Eles são a solução da equação \\(\\mathbf{R}\\mathbf{e} = \\lambda\\mathbf{e}\\), que é equivalente a resolver \\((\\mathbf{R} - \\lambda\\mathbf{I})\\mathbf{e} = \\mathbf{0}\\).\nIsso requer encontrar as raízes do polinômio característico \\(det(\\mathbf{R} - \\lambda\\mathbf{I}) = 0\\).\n\\[\ndet \\begin{pmatrix}\n1.00 - \\lambda & 0.96 & -0.20 \\\\\n0.96 & 1.00 - \\lambda & -0.29 \\\\\n-0.20 & -0.29 & 1.00 - \\lambda\n\\end{pmatrix} = 0\n\\]\nResolver este determinante cúbico manualmente é trabalhoso. Usando uma calculadora ou software, encontramos os seguintes autovalores:\n\\[\n\\lambda_1 = 1.98 \\quad \\lambda_2 = 1.00 \\quad \\lambda_3 = 0.02\n\\]\n\nInterpretação dos Autovalores\nA variância total no sistema é a soma dos autovalores (que é igual ao traço da matriz \\(\\mathbf{R}\\), ou seja, 3). - Variância Total = \\(1.98 + 1.00 + 0.02 = 3.00\\)\nA proporção da variância explicada por cada componente é: - CP1: \\(\\frac{1.98}{3.00} = 66.0\\%\\) - CP2: \\(\\frac{1.00}{3.00} = 33.3\\%\\) - CP3: \\(\\frac{0.02}{3.00} = 0.7\\%\\)\nOs dois primeiros componentes juntos explicam \\(66.0\\% + 33.3\\% = 99.3\\%\\) da variância total. Isso indica que podemos reduzir a dimensionalidade de 3 para 2 com uma perda mínima de informação.",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-4-calcular-os-autovetores",
    "href": "src/exemplos/acp_manual.html#passo-4-calcular-os-autovetores",
    "title": "Exemplo manual: ACP",
    "section": "Passo 4: Calcular os Autovetores",
    "text": "Passo 4: Calcular os Autovetores\nAgora, para cada autovalor, resolvemos o sistema \\((\\mathbf{R} - \\lambda_i\\mathbf{I})\\mathbf{e}_i = \\mathbf{0}\\) para encontrar o autovetor correspondente \\(\\mathbf{e}_i\\).\n\nPara \\(\\lambda_1 = 1.98\\): \\[\n\\begin{pmatrix}\n-0.98 & 0.96 & -0.20 \\\\\n0.96 & -0.98 & -0.29 \\\\\n-0.20 & -0.29 & -0.98\n\\end{pmatrix}\n\\begin{pmatrix} e_{11} \\\\ e_{12} \\\\ e_{13} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solução, após normalização (para que \\(\\mathbf{e}_1'\\mathbf{e}_1 = 1\\)), é: \\[\n\\mathbf{e}_1 = \\begin{pmatrix} 0.69 \\\\ 0.71 \\\\ -0.15 \\end{pmatrix}\n\\]\nPara \\(\\lambda_2 = 1.00\\): \\[\n\\begin{pmatrix}\n0.00 & 0.96 & -0.20 \\\\\n0.96 & 0.00 & -0.29 \\\\\n-0.20 & -0.29 & 0.00\n\\end{pmatrix}\n\\begin{pmatrix} e_{21} \\\\ e_{22} \\\\ e_{23} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solução normalizada é: \\[\n\\mathbf{e}_2 = \\begin{pmatrix} 0.18 \\\\ -0.22 \\\\ -0.96 \\end{pmatrix}\n\\]\nPara \\(\\lambda_3 = 0.02\\): \\[\n\\begin{pmatrix}\n0.98 & 0.96 & -0.20 \\\\\n0.96 & 0.98 & -0.29 \\\\\n-0.20 & -0.29 & 0.98\n\\end{pmatrix}\n\\begin{pmatrix} e_{31} \\\\ e_{32} \\\\ e_{33} \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] A solução normalizada é: \\[\n\\mathbf{e}_3 = \\begin{pmatrix} -0.70 \\\\ 0.67 \\\\ -0.24 \\end{pmatrix}\n\\]",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-5-interpretação-dos-componentes",
    "href": "src/exemplos/acp_manual.html#passo-5-interpretação-dos-componentes",
    "title": "Exemplo manual: ACP",
    "section": "Passo 5: Interpretação dos Componentes",
    "text": "Passo 5: Interpretação dos Componentes\nOs autovetores (ou loadings) nos dizem como as variáveis originais se combinam para formar cada componente.\n\nComponente Principal 1 (\\(CP_1\\)): \\[\nY_1 = 0.69 \\cdot Z_{peso} + 0.71 \\cdot Z_{altura} - 0.15 \\cdot Z_{renda}\n\\] Este componente é basicamente uma média ponderada de Peso e Altura, com uma pequena contribuição negativa da Renda. Podemos interpretá-lo como um índice de “Tamanho Corporal”. As cargas altas e positivas para Peso e Altura confirmam a alta correlação entre essas variáveis.\nComponente Principal 2 (\\(CP_2\\)): \\[\nY_2 = 0.18 \\cdot Z_{peso} - 0.22 \\cdot Z_{altura} - 0.96 \\cdot Z_{renda}\n\\] Este componente é dominado pela Renda, com uma carga muito alta e negativa. As cargas para Peso e Altura são pequenas. Podemos interpretar o \\(CP_2\\) como um índice de “Status Socioeconômico Inverso”, já que ele é quase que inteiramente uma representação da Renda (com sinal trocado).",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#passo-6-calcular-os-scores-dos-componentes",
    "href": "src/exemplos/acp_manual.html#passo-6-calcular-os-scores-dos-componentes",
    "title": "Exemplo manual: ACP",
    "section": "Passo 6: Calcular os Scores dos Componentes",
    "text": "Passo 6: Calcular os Scores dos Componentes\nFinalmente, podemos calcular os valores (scores) dos componentes principais para cada indivíduo. Usamos a fórmula \\(\\mathbf{Y} = \\mathbf{Z} \\mathbf{P}\\), onde \\(\\mathbf{P}\\) é a matriz cujas colunas são os autovetores.\n\\[\n\\mathbf{P} = \\begin{pmatrix}\n0.69 & 0.18 & -0.70 \\\\\n0.71 & -0.22 & 0.67 \\\\\n-0.15 & -0.96 & -0.24\n\\end{pmatrix}\n\\]\nPara o Indivíduo 1, com dados padronizados \\((-0.60, -0.72, 0.37)\\): \\[\ny_{11} = (-0.60)(0.69) + (-0.72)(0.71) + (0.37)(-0.15) = -0.98\n\\] \\[\ny_{12} = (-0.60)(0.18) + (-0.72)(-0.22) + (0.37)(-0.96) = -0.31\n\\]\nCalculando para todos os indivíduos, obtemos a matriz de scores \\(\\mathbf{Y}\\):\n\n\n\nIndivíduo\nCP1 (Tamanho)\nCP2 (Renda Inversa)\n\n\n\n\n1\n-0.98\n-0.31\n\n\n2\n0.57\n0.85\n\n\n3\n-2.19\n-1.18\n\n\n4\n2.09\n1.35\n\n\n5\n0.46\n0.08",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/acp_manual.html#conclusão",
    "href": "src/exemplos/acp_manual.html#conclusão",
    "title": "Exemplo manual: ACP",
    "section": "Conclusão",
    "text": "Conclusão\nEste exemplo demonstra o poder da ACP. Começamos com três variáveis e, através de uma derivação passo a passo, conseguimos: 1. Reduzir a dimensionalidade: Mostramos que 99.3% da informação está contida em dois componentes. 2. Criar variáveis não correlacionadas: O \\(CP_1\\) e o \\(CP_2\\) são, por construção, ortogonais. 3. Interpretar a estrutura latente: Identificamos que a principal fonte de variação nos dados é o “Tamanho Corporal” (uma combinação de Peso e Altura), seguida pelo “Status Socioeconômico” (representado pela Renda).\nA análise manual, embora trabalhosa, revela a mecânica exata da técnica, solidificando a compreensão teórica apresentada no capítulo principal.",
    "crumbs": [
      "Exemplos",
      "Exemplo manual: ACP"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html",
    "href": "src/exemplos/af_big_five.html",
    "title": "Rotação fatorial em R",
    "section": "",
    "text": "Passo 1: Análise Descritiva e Adequação dos Dados\nA Análise Fatorial (AF) é uma técnica estatística poderosa usada para identificar estruturas latentes (fatores) subjacentes a um conjunto de variáveis observadas. No entanto, a solução matemática inicial de uma AF raramente é interpretável. É aqui que a rotação fatorial se torna a etapa mais crítica do processo. A rotação transforma a matriz de cargas fatoriais inicial em uma solução mais simples e teoricamente mais significativa, sem alterar as propriedades matemáticas fundamentais da solução.\nEste documento oferece um exemplo prático e didático, totalmente focado em demonstrar o impacto das diferentes estratégias de rotação. Usaremos o software R e o clássico conjunto de dados bfi (Big Five Inventory) do pacote psych.\nObjetivos:\nPrimeiro, carregamos os pacotes necessários e o conjunto de dados bfi. Este dataset contém respostas de 2800 indivíduos a 25 itens de personalidade.\nCódigo\n# Carregar pacotes\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(corrplot)\n\n# Carregar os dados do Big Five Inventory\ndata(bfi, package = \"psych\")\n\n# Selecionar apenas as 25 variáveis de itens de personalidade\nbfi_items &lt;- bfi[, 1:25]\n\n# Remover linhas com dados ausentes para simplificar\nbfi_complete &lt;- na.omit(bfi_items)\n\nknitr::kable(head(bfi_complete))\n\n\n\n\nTabela 1: Exemplo de respostas no banco de dados Big Five\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1\nA2\nA3\nA4\nA5\nC1\nC2\nC3\nC4\nC5\nE1\nE2\nE3\nE4\nE5\nN1\nN2\nN3\nN4\nN5\nO1\nO2\nO3\nO4\nO5\n\n\n\n\n61617\n2\n4\n3\n4\n4\n2\n3\n3\n4\n4\n3\n3\n3\n4\n4\n3\n4\n2\n2\n3\n3\n6\n3\n4\n3\n\n\n61618\n2\n4\n5\n2\n5\n5\n4\n4\n3\n4\n1\n1\n6\n4\n3\n3\n3\n3\n5\n5\n4\n2\n4\n3\n3\n\n\n61620\n5\n4\n5\n4\n4\n4\n5\n4\n2\n5\n2\n4\n4\n4\n5\n4\n5\n4\n2\n3\n4\n2\n5\n5\n2\n\n\n61621\n4\n4\n6\n5\n5\n4\n4\n3\n5\n5\n5\n3\n4\n4\n4\n2\n5\n2\n4\n1\n3\n3\n4\n3\n5\n\n\n61622\n2\n3\n3\n4\n5\n4\n4\n5\n3\n2\n2\n2\n5\n4\n5\n2\n3\n4\n4\n3\n3\n3\n4\n3\n3\n\n\n61623\n6\n6\n5\n6\n5\n6\n6\n6\n1\n3\n2\n1\n6\n5\n6\n3\n5\n2\n2\n3\n4\n3\n5\n6\n1\nAs 25 variáveis correspondem a 5 itens para cada um dos traços do “Big Five”:\nA hipótese teórica é que os 5 itens que medem o mesmo traço (e.g., N1 a N5) estarão altamente correlacionados entre si e se agruparão em um único fator latente (Neuroticismo).\nUma boa prática é verificar se os dados são fatorizáveis. Para isso, podemos usar o teste de Bartlett e a medida KMO.\nCódigo\n# Teste de Bartlett\nbartlett_test &lt;- cortest.bartlett(bfi_complete)\n\n\nR was not square, finding R from data\n\n\nCódigo\n# Teste KMO\nkmo_test &lt;- KMO(bfi_complete)\n\n# Exibindo os resultados de forma concisa\ncat(\"Teste de Bartlett: p-valor =\", bartlett_test$p.value, \"\\n\")\n\n\nTeste de Bartlett: p-valor = 0 \n\n\nCódigo\ncat(\"Medida KMO Geral (Overall MSA):\", round(kmo_test$MSA, 2), \"\\n\")\n\n\nMedida KMO Geral (Overall MSA): 0.85\nO p-valor de Bartlett próximo de zero e o KMO de 0.85 (“meritório”) sugerem que os dados têm correlações suficientes para justificar uma análise fatorial.",
    "crumbs": [
      "Exemplos",
      "Rotação fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-1-análise-descritiva-e-adequação-dos-dados",
    "href": "src/exemplos/af_big_five.html#passo-1-análise-descritiva-e-adequação-dos-dados",
    "title": "Rotação fatorial em R",
    "section": "",
    "text": "A1-A5: Amabilidade (Agreeableness)\nC1-C5: Conscienciosidade (Conscientiousness)\nE1-E5: Extroversão (Extraversion)\nN1-N5: Neuroticismo (Neuroticism)\nO1-O5: Abertura à Experiência (Openness)",
    "crumbs": [
      "Exemplos",
      "Rotação fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-2-extração-inicial-dos-fatores-e-escolha-do-número-de-fatores",
    "href": "src/exemplos/af_big_five.html#passo-2-extração-inicial-dos-fatores-e-escolha-do-número-de-fatores",
    "title": "Rotação fatorial em R",
    "section": "Passo 2: Extração Inicial dos Fatores e Escolha do Número de Fatores",
    "text": "Passo 2: Extração Inicial dos Fatores e Escolha do Número de Fatores\nAntes de rotacionar, precisamos extrair os fatores. Dois métodos comuns são a Fatoração do Eixo Principal (ou “componentes principais” para o modelo fatorial) e a Máxima Verossimilhança. Vamos extrair 5 fatores usando ambos os métodos (sem rotação) para ver a solução inicial.\n\n\nCódigo\n# Extração via Fatoração do Eixo Principal (Principal Axis Factoring)\nfa_pa &lt;- fa(bfi_complete, nfactors = 5, rotate = \"none\", fm = \"pa\")\ncat(\"Cargas - Fatoração do Eixo Principal (PA):\\n\")\n\n\nCargas - Fatoração do Eixo Principal (PA):\n\n\nCódigo\nprint(fa_pa$loadings, cutoff = 0.3)\n\n\n\nLoadings:\n   PA1    PA2    PA3    PA4    PA5   \nA1                             -0.371\nA2  0.467                       0.340\nA3  0.534  0.302                     \nA4  0.417                            \nA5  0.581                            \nC1  0.343         0.446              \nC2  0.336         0.477              \nC3  0.319         0.351  0.310       \nC4 -0.465        -0.452              \nC5 -0.493                            \nE1 -0.408                            \nE2 -0.619                       0.323\nE3  0.527  0.328                     \nE4  0.599        -0.329              \nE5  0.513                            \nN1 -0.441  0.636                     \nN2 -0.423  0.616                     \nN3 -0.407  0.611                     \nN4 -0.528  0.416                     \nN5 -0.345  0.413                     \nO1  0.328               -0.360       \nO2                       0.370       \nO3  0.407               -0.446       \nO4                                   \nO5                       0.412       \n\n                 PA1   PA2   PA3   PA4   PA5\nSS loadings    4.600 2.268 1.549 1.218 0.956\nProportion Var 0.184 0.091 0.062 0.049 0.038\nCumulative Var 0.184 0.275 0.337 0.385 0.424\n\n\nCódigo\n# Extração via Máxima Verossimilhança (Maximum Likelihood)\nfa_ml &lt;- fa(bfi_complete, nfactors = 5, rotate = \"none\", fm = \"ml\")\ncat(\"\\nCargas - Máxima Verossimilhança (ML):\\n\")\n\n\n\nCargas - Máxima Verossimilhança (ML):\n\n\nCódigo\nprint(fa_ml$loadings, cutoff = 0.3)\n\n\n\nLoadings:\n   ML1    ML2    ML3    ML4    ML5   \nA1                             -0.322\nA2 -0.396  0.354                0.334\nA3 -0.462  0.401                0.321\nA4 -0.386                            \nA5 -0.546                            \nC1                0.465              \nC2                0.511              \nC3                0.404              \nC4  0.441        -0.512              \nC5  0.485        -0.358              \nE1  0.355 -0.309                     \nE2  0.585                       0.336\nE3 -0.446  0.436                     \nE4 -0.552  0.333                     \nE5 -0.409  0.429                     \nN1  0.609  0.566                     \nN2  0.587  0.543                     \nN3  0.533  0.479                     \nN4  0.591                            \nN5  0.421                            \nO1                      -0.409       \nO2                       0.388       \nO3 -0.329  0.349        -0.491       \nO4                      -0.307  0.311\nO5                       0.433       \n\n                 ML1   ML2   ML3   ML4   ML5\nSS loadings    4.451 2.379 1.546 1.221 0.977\nProportion Var 0.178 0.095 0.062 0.049 0.039\nCumulative Var 0.178 0.273 0.335 0.384 0.423\n\n\nAs duas soluções iniciais são numericamente diferentes, mas conceitualmente iguais: são ininterpretáveis. Um primeiro fator geral domina, e as variáveis se distribuem de forma confusa nos demais. Isso reforça a necessidade da rotação.\nPara determinar o número de fatores a extrair de forma mais objetiva, usamos a Análise Paralela.\n\n\nCódigo\nfa.parallel(bfi_complete, fa = \"fa\", fm = \"pa\")\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  NA \n\n\n\n\n\n\n\n\nFigura 1: Análise Paralela sugerindo a extração de 6 fatores.\n\n\n\n\n\nA Análise Paralela (Figura 1) sugere 6 fatores. No entanto, como nosso objetivo é testar a teoria dos Big Five, prosseguiremos com a extração de 5 fatores, uma decisão comum quando a teoria é forte.",
    "crumbs": [
      "Exemplos",
      "Rotação fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-3-rotação-ortogonal-varimax",
    "href": "src/exemplos/af_big_five.html#passo-3-rotação-ortogonal-varimax",
    "title": "Rotação fatorial em R",
    "section": "Passo 3: Rotação Ortogonal (Varimax)",
    "text": "Passo 3: Rotação Ortogonal (Varimax)\nA rotação Varimax “limpa” a estrutura sob a suposição de que os fatores não são correlacionados entre si.\n\n\nCódigo\n# Análise Fatorial com rotação Varimax\nfa_varimax &lt;- factanal(bfi_complete, \n                       factors = 5, \n                       rotation = \"varimax\")\n\ncat(\"Cargas Fatoriais (AF) - Rotação Varimax:\\n\")\n\n\nCargas Fatoriais (AF) - Rotação Varimax:\n\n\nCódigo\nprint(fa_varimax$loadings, cutoff = 0.3, sort = TRUE)\n\n\n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5\nN1  0.816                                 \nN2  0.787                                 \nN3  0.714                                 \nN4  0.562  -0.367                         \nN5  0.518                                 \nE1         -0.587                         \nE2         -0.674                         \nE4          0.613           0.363         \nC1                  0.533                 \nC2                  0.624                 \nC3                  0.554                 \nC4                 -0.653                 \nC5                 -0.573                 \nA2                          0.601         \nA3                          0.662         \nA5          0.351           0.580         \nO1                                  0.524 \nO3                                  0.614 \nO5                                 -0.512 \nA1                         -0.393         \nA4                          0.454         \nE3          0.490           0.315   0.313 \nE5          0.491   0.310                 \nO2                                 -0.454 \nO4                                  0.368 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      2.687   2.320   2.034   1.978   1.557\nProportion Var   0.107   0.093   0.081   0.079   0.062\nCumulative Var   0.107   0.200   0.282   0.361   0.423\n\n\nA estrutura agora é muito mais “simples” e alinhada com a teoria. Podemos visualizar essa transformação de forma clara comparando o círculo de correlações antes e depois da rotação.\nPrimeiro, a solução não rotacionada (Figura 2). Note como as variáveis (vetores) se espalham pelo espaço fatorial sem um padrão claro. É difícil traçar os eixos (fatores) de forma que representem grupos distintos de variáveis.\n\n\nCódigo\nlibrary(ggrepel)\n\n# Extrair cargas da solução NÃO ROTACIONADA (ml) para um dataframe\nloadings_unrotated_df &lt;- as.data.frame(unclass(fa_ml$loadings))\nloadings_unrotated_df$Variable &lt;- rownames(loadings_unrotated_df)\n\n# Selecionar algumas variáveis para anotar e evitar poluição\nvars_to_label &lt;- c(\"N1\", \"N3\", \"E2\", \"E4\", \"A1\", \"C1\", \"O1\")\nannotations_df_unrotated &lt;- loadings_unrotated_df %&gt;% filter(Variable %in% vars_to_label)\n\n# Criar dados para o círculo unitário\ncircle &lt;- data.frame(\n  angle = seq(-pi, pi, length = 100),\n  x = sin(seq(-pi, pi, length = 100)),\n  y = cos(seq(-pi, pi, length = 100))\n)\n\n# Gerar o gráfico com ggplot2\nggplot(data = loadings_unrotated_df, aes(x = ML1, y = ML2)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_path(data = circle, aes(x = x, y = y), inherit.aes = FALSE, color = \"gray60\") +\n  geom_segment(aes(x = 0, y = 0, xend = ML1, yend = ML2),\n               arrow = arrow(length = unit(0.1, \"inches\")),\n               color = \"steelblue\") +\n  geom_text_repel(data = annotations_df_unrotated, aes(label = Variable), min.segment.length = 0) +\n  coord_fixed(ratio = 1, xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  labs(title = \"Círculo de Correlações - Solução Não Rotacionada\",\n       x = \"Fator 1\",\n       y = \"Fator 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 2: Círculo de correlações da solução não rotacionada. Os vetores não estão alinhados com os eixos.\n\n\n\n\n\nAgora, veja o resultado após a rotação Varimax (Figura 3). A rotação funcionou como um ajuste dos eixos, alinhando-os com os agrupamentos de variáveis.\n\n\nCódigo\n# Extrair cargas da solução ROTACIONADA (Varimax) para um dataframe\nloadings_rotated_df &lt;- as.data.frame(unclass(fa_varimax$loadings))\nloadings_rotated_df$Variable &lt;- rownames(loadings_rotated_df)\n\n# Selecionar as mesmas variáveis para anotar\nannotations_df_rotated &lt;- loadings_rotated_df %&gt;% filter(Variable %in% vars_to_label)\n\n# Calcular a variância explicada para os eixos\nss_loadings &lt;- colSums(fa_varimax$loadings^2)\nprop_variance &lt;- ss_loadings / ncol(bfi_complete)\nxlab_text &lt;- sprintf(\"Fator 1 (%.2f%% da variância)\", prop_variance[1] * 100)\nylab_text &lt;- sprintf(\"Fator 2 (%.2f%% da variância)\", prop_variance[2] * 100)\n\n# Gerar o gráfico com ggplot2\nggplot(data = loadings_rotated_df, aes(x = Factor1, y = Factor2)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_path(data = circle, aes(x = x, y = y), inherit.aes = FALSE, color = \"gray60\") +\n  geom_segment(aes(x = 0, y = 0, xend = Factor1, yend = Factor2),\n               arrow = arrow(length = unit(0.1, \"inches\")),\n               color = \"steelblue\") +\n  geom_text_repel(data = annotations_df_rotated, aes(label = Variable), min.segment.length = 0) +\n  coord_fixed(ratio = 1, xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  labs(title = \"Círculo de Correlações - Rotação Varimax\",\n       x = xlab_text,\n       y = ylab_text) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigura 3: Círculo de correlações após a rotação Varimax. A rotação alinhou os vetores com os eixos, revelando uma estrutura simples.\n\n\n\n\n\nO resultado é uma “estrutura simples”, onde os itens de Neuroticismo (como N1 e N3) carregam quase exclusivamente no Fator 1 (correlação próxima de 1 ou -1 em um eixo e de 0 no outro), e os itens de Extroversão (como E2 e E4) carregam no Fator 2. A interpretação se torna direta.",
    "crumbs": [
      "Exemplos",
      "Rotação fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#passo-4-rotação-oblíqua-promax",
    "href": "src/exemplos/af_big_five.html#passo-4-rotação-oblíqua-promax",
    "title": "Rotação fatorial em R",
    "section": "Passo 4: Rotação Oblíqua (Promax)",
    "text": "Passo 4: Rotação Oblíqua (Promax)\nA rotação Promax é mais flexível, pois permite que os fatores sejam correlacionados. Isso costuma ser mais realista em psicologia.\n\n\nCódigo\n# Análise Fatorial com rotação Promax (oblíqua)\nfa_promax &lt;- fa(bfi_complete, \n                nfactors = 5, \n                rotate = \"promax\", \n                fm = \"pa\")\n\n\nLoading required namespace: GPArotation\n\n\nCódigo\ncat(\"Cargas Fatoriais (Pattern Matrix) - Rotação Promax:\\n\")\n\n\nCargas Fatoriais (Pattern Matrix) - Rotação Promax:\n\n\nCódigo\nprint(fa_promax$loadings, cutoff = 0.3, sort = TRUE)\n\n\n\nLoadings:\n   PA2    PA1    PA3    PA5    PA4   \nN1  0.835                            \nN2  0.791                            \nN3  0.741                            \nN4  0.533 -0.311                     \nN5  0.529                            \nE1        -0.636                     \nE2        -0.711                     \nE3         0.545                     \nE4         0.660                     \nC1                0.567              \nC2                0.697              \nC3                0.597              \nC4               -0.652              \nC5               -0.561              \nA2                       0.611       \nA3                       0.620       \nO3                              0.576\nO5                             -0.543\nA1                      -0.463       \nA4                       0.411       \nA5         0.332         0.489       \nE5         0.498                     \nO1                              0.491\nO2                             -0.484\nO4                              0.370\n\n                 PA2   PA1   PA3   PA5   PA4\nSS loadings    2.704 2.486 2.050 1.638 1.461\nProportion Var 0.108 0.099 0.082 0.066 0.058\nCumulative Var 0.108 0.208 0.290 0.355 0.414\n\n\nA matriz de cargas é similar à da Varimax, mas a grande vantagem é poder examinar a matriz de correlação entre os fatores.\n\n\nCódigo\n# Matriz de correlação entre os fatores\nfactor_correlations &lt;- fa_promax$Phi\n\ncat(\"Matriz de Correlação entre os Fatores (Promax):\\n\")\n\n\nMatriz de Correlação entre os Fatores (Promax):\n\n\nCódigo\nprint(round(factor_correlations, 2))\n\n\n      PA2   PA1   PA3   PA5  PA4\nPA2  1.00 -0.26 -0.22 -0.01 0.04\nPA1 -0.26  1.00  0.40  0.35 0.14\nPA3 -0.22  0.40  1.00  0.24 0.19\nPA5 -0.01  0.35  0.24  1.00 0.16\nPA4  0.04  0.14  0.19  0.16 1.00\n\n\nCódigo\n# Visualização da matriz de correlação\ncorrplot(factor_correlations, method = \"color\", type = \"upper\", \n         addCoef.col = \"black\", tl.col = \"black\", tl.srt = 45, diag = FALSE)\n\n\n\n\n\n\n\n\nFigura 4: Correlações entre os fatores da solução Promax.\n\n\n\n\n\nA matriz de correlação (Figura 4) mostra que Neuroticismo (ML1) se correlaciona negativamente com Conscienciosidade (ML3) (-0.33) e Extroversão (ML2) (-0.24). Essas relações são teoricamente plausíveis e seriam perdidas em uma rotação ortogonal.",
    "crumbs": [
      "Exemplos",
      "Rotação fatorial em R"
    ]
  },
  {
    "objectID": "src/exemplos/af_big_five.html#conclusão-qual-rotação-escolher",
    "href": "src/exemplos/af_big_five.html#conclusão-qual-rotação-escolher",
    "title": "Rotação fatorial em R",
    "section": "Conclusão: Qual Rotação Escolher?",
    "text": "Conclusão: Qual Rotação Escolher?\n\nSolução Não Rotacionada: É apenas um ponto de partida matemático. Dificilmente é possível tirar interpretações úteis dela.\nRotação Ortogonal (Varimax): É a melhor escolha quando há fortes razões teóricas para acreditar que os fatores são independentes. Oferece uma solução mais simples (parcimoniosa).\nRotação Oblíqua (Promax): É uma escolha mais realista nas ciências sociais. A decisão final deve ser baseada na matriz de correlação dos fatores. Se as correlações forem substanciais, a solução oblíqua é superior.\n\nNeste exemplo, a solução Promax (oblíqua) é a mais apropriada. Ela não apenas recupera a estrutura dos Big Five, mas também fornece insights sobre como esses traços se relacionam, oferecendo uma visão mais rica e fiel da realidade psicológica.",
    "crumbs": [
      "Exemplos",
      "Rotação fatorial em R"
    ]
  }
]